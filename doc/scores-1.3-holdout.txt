Scores against **complete holdout set** (realistic in term of positive/negative class distribution)


* CRF - trained on only positive paragraphs (no negative sampling) 
2021-05-05

===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            94.67        41.45        76.56        53.78        209    
<software>           76.94        29.18        58.49        38.93        648    
<url>                97.69        18.18        68.57        28.74        35     
<version>            95.79        51.85        84.85        64.37        231    

all (micro avg.)     91.27        34.58        67.59        45.75        1123   
all (macro avg.)     91.27        35.17        72.12        46.46        1123   

===== Instance-level results =====

Total expected instances:   33416
Correct instances:          32293
Instance-level recall:      96.64



* CRF - trained with random negative paragraphs (random oversampling) ratio 1

===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            95.48        53.16        76.56        62.75        209    
<software>           78.02        36.49        57.72        44.71        648    
<url>                98.31        28.57        68.57        40.34        35     
<version>            96.55        63.87        85.71        73.2         231    

all (micro avg.)     92.09        43.95        67.32        53.18        1123   
all (macro avg.)     92.09        45.52        72.14        55.25        1123   

===== Instance-level results =====

Total expected instances:   33416
Correct instances:          32664
Instance-level recall:      97.75



* CRF - trained with random negative paragraphs (random oversampling) ratio 5

===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            95.23        57.3         77.03        65.71        209    
<software>           80.99        48.54        56.48        52.21        648    
<url>                98.5         34.48        57.14        43.01        35     
<version>            96.74        70.86        85.28        77.41        231    

all (micro avg.)     92.86        54.27        66.25        59.66        1123   
all (macro avg.)     92.86        52.8         68.98        59.59        1123   

===== Instance-level results =====

Total expected instances:   33416
Correct instances:          32820
Instance-level recall:      98.22



* CRF - trained with random negative paragraphs (random oversampling) ratio 10

===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            95.89        67.67        75.12        71.2         209    
<software>           82.93        60.13        55.4         57.67        648    
<url>                98.35        35.71        57.14        43.96        35     
<version>            96.73        74.62        85.28        79.6         231    

all (micro avg.)     93.47        63.79        65.27        64.52        1123   
all (macro avg.)     93.47        59.54        68.24        63.11        1123   

===== Instance-level results =====

Total expected instances:   33416
Correct instances:          32968
Instance-level recall:      98.66


* CRF - trained with random negative paragraphs (random oversampling) ratio 15

===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            95.93        70.4         75.12        72.69        209    
<software>           83.7         66.92        53.7         59.59        648    
<url>                98.31        34.78        45.71        39.51        35     
<version>            97           79.75        83.55        81.61        231    

all (micro avg.)     93.73        69.25        63.58        66.3         1123   
all (macro avg.)     93.73        62.97        64.52        63.35        1123   

===== Instance-level results =====

Total expected instances:   33416
Correct instances:          33024
Instance-level recall:      98.83



* CRF - trained with model-selected negative paragraphs (active oversampling) 


===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            95.8         70.32        73.68        71.96        209    
<software>           83.9         68.95        52.78        59.79        648    
<url>                98.22        32.61        42.86        37.04        35     
<version>            97.03        80.93        82.68        81.8         231    

all (micro avg.)     93.74        70.41        62.51        66.23        1123   
all (macro avg.)     93.74        63.2         63           62.65        1123   

===== Instance-level results =====

Total expected instances:   33416
Correct instances:          33037
Instance-level recall:      98.87



* bidLSTM-CRF - trained on only positive paragraphs (no negative sampling) , eval complete holdout
2021-05-05

Evaluation:
    f1 (micro): 41.88
                  precision    recall  f1-score   support

       <creator>     0.4529    0.8278    0.5854       209
      <software>     0.2194    0.6852    0.3323       648
           <url>     0.1667    0.5714    0.2581        35
       <version>     0.5359    0.9048    0.6731       231

all (micro avg.)     0.2901    0.7533    0.4189      1123

Evaluation runtime: 350.445 seconds




* bidLSTM-CRF_FEATURES - trained on only positive paragraphs (no negative sampling) , eval complete holdout
2021-05-05

Evaluation:
        f1 (micro): 41.42
                  precision    recall  f1-score   support

       <creator>     0.4566    0.8565    0.5957       209
      <software>     0.2094    0.7454    0.3269       648
           <url>     0.1453    0.4857    0.2237        35
       <version>     0.5840    0.9177    0.7138       231

all (micro avg.)     0.2803    0.7934    0.4142      1123

Evaluation runtime: 389.84 seconds




* BERT-base-en - trained on only positive paragraphs (no negative sampling) , eval complete holdout
2021-05-05

Evaluation:
                  precision    recall  f1-score   support

       <creator>     0.4019    0.7943    0.5338       209
      <software>     0.1508    0.7423    0.2507       648
           <url>     0.0449    0.7143    0.0845        35
       <version>     0.4212    0.8788    0.5694       231

all (micro avg.)     0.1885    0.7792    0.3036      1123

Evaluation runtime: 1815.756 seconds 





* Bid-LSTM-CRF+ELMo - trained on only positive paragraphs (no negative sampling) 
2021-05-05


Evaluation:
    f1 (micro): 54.51
                  precision    recall  f1-score   support

       <creator>     0.7155    0.7943    0.7528       209
      <software>     0.3556    0.7485    0.4821       648
           <url>     0.1162    0.8000    0.2029        35
       <version>     0.7286    0.8831    0.7984       231

all (micro avg.)     0.4171    0.7863    0.5451      1123





* SciBERT-CRF - trained on only positive paragraphs (no negative sampling) 
2021-05-05

Evaluation:
number of alignment issues with test set: 995
                  precision    recall  f1-score   support

       <creator>     0.4414    0.8469    0.5803       209
      <software>     0.2573    0.8040    0.3898       648
           <url>     0.2778    0.7143    0.4000        35
       <version>     0.7172    0.9221    0.8068       231

all (micro avg.)     0.3327    0.8335    0.4756      1123

Evaluation runtime: 1794.844 seconds 


* SciBERT-CRF - random sampling ratio 15 (best ratio)
2021-05-05

Evaluation:
number of alignment issues with test set: 990
                  precision    recall  f1-score   support

       <creator>     0.6811    0.8278    0.7473       209
      <software>     0.6048    0.7701    0.6775       648
           <url>     0.4032    0.7143    0.5155        35
       <version>     0.7536    0.9134    0.8258       231

all (micro avg.)     0.6390    0.8085    0.7138      1123

Evaluation runtime: 1801.454 seconds 


* SciBERT-CRF - active sampling ratio 15 (best ratio)
2021-05-05

Evaluation:
number of alignment issues with test set: 999
                  precision    recall  f1-score   support

       <creator>     0.7555    0.8278    0.7900       209
      <software>     0.6931    0.7284    0.7103       648
           <url>     0.4528    0.6857    0.5455        35
       <version>     0.8024    0.8788    0.8388       231

all (micro avg.)     0.7171    0.7765    0.7456      1123

Evaluation runtime: 1790.766 seconds 


