<tei xmlns="http://www.tei-c.org/ns/1.0"><teiHeader><fileDesc xml:id="PMC3261704" /><encodingDesc><appInfo><application version="0.5.6-SNAPSHOT" ident="GROBID" when="2019-06-07T18:14+0000"><ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref></application></appInfo></encodingDesc></teiHeader>
<text xml:lang="en">
<p><rs type="software">Google Scholar</rs><rs type="software">Google 
Scholar</rs>search for biomarker signatures in cancer. For about a decade, 
scientists have scoured high-throughput data to find collec-
tions of genes or proteins that can be used in diagnosis or 
prognosis of cancer. However, the tools used to find signatures 
in massive data sets can yield spurious associations with 
phenotype (Ioannidis, 2005), even when the results appear 
to be statistically sound in self-assessment. In most cases, 
unfortunately, these signatures do not generalize; taken to 
the task of showing the diagnostics or prognostics value 
of these signatures, the accuracy of the predictions is much 
poorer on impartial assessments on previously unseen patients 
than on the original data. This problem with cancer signatures 
is of sufficient general interest to be highlighted recently in the 
popular media (Kolata, 2011). 
In order to alleviate the overestimation of accuracy from the 
many bias sources described above, we proposed a few guidelines: </p>

<p>(i) use third-party validation to test a model with previously 
unseen data 
(ii) use more than one metric to evaluate the methods 
(iii) report well-performing methods even if they are not the 
best performers on a particular data set 
(iv) increase the awareness of editors and reviewers that 
superior performance in self-assessment is a biased 
demonstration of the method's value; instead, impartial 
assessment should be the preferred evaluation 
(v) Establish a scientific culture that values timely, well-
conducted follow-up studies that confirm or refute 
previous results </p>

<p>To a large extent, the remedies suggested above have been 
addressed in the context of genome-wide association studies 
(Chanock et al, 2007), and are embodied in existing indepen-
dent assessments presented to the scientific community in 
efforts such as CASP (http://predictioncenter.org/), CAPRI 
(http://www.ebi.ac.uk/msd-srv/capri/) and DREAM (http:// 
www.the-dream-project.org). In contrast to the usual practice 
of 'post-diction' (retrospective prediction) of known results 
as a way to test their methods, participants to these third-
party collaborative competitions (alternatively known as 
challenges) submit predictions that are evaluated by impartial 
scorers against an independent data set that is hidden from 
the participants. The level of performance in these evalua-
tions better tests the generalization ability of the methods, 
because the predictions are made based on unseen data, thus 
minimizing many of the above-discussed biases. We envision 
that a repository of blind challenges and data sets could be 
created (DREAM, for example, has 20 such data sets and 
challenges) with data produced on demand by third parties, 
especially funded to create verification data and challenges. 
This repository could be used to test the validity of many of 
the tasks that we deal with in Systems Biology, Bioinformatics 
and Computational Biology. 
In summary, systematic bias, information leak and over-
fitting can all be considered facets of the same self-assessment </p>

<p>trap. That is, by knowing too much about the desired results, 
the researcher gets snared into a trap of consciously or 
unconsciously overestimating performance. Moreover, the 
researcher is further lured to the trap by the common 
assumption that top performance is required for scientific 
value and publication. By exposing the self-assessment trap, 
we hope to lessen its effect with the ultimate goal of advancing 
predictive biology and improving human healthcare. </p>

<p>Supplementary information </p>

<p>Supplementary information is available at the Molecular 
Systems Biology website (www.nature.com/msb). </p>

<p>Conflict of interest </p>

<p>The authors declare that they have no conflict of interest. </p>

<p>Raquel Norel, John Jeremy Rice and Gustavo Stolovitzky </p>

<p>IBM Computational Biology Center, IBM T.J. Watson Research Center, 
Yorktown Heights, NY, USA </p>



<p>Molecular Systems Biology is an open-access journal 
published by European Molecular Biology Organiza-
tion and Nature Publishing Group. This work is licensed under a 
Creative Commons Attribution-Noncommercial-Share Alike 3.0 
Unported License. </p>

<p>Correspondence 
R Norel et al </p>



<p>&amp; 2011 EMBO and Macmillan Publishers Limited </p>

</text></tei>