<?xml version="1.0"?>
<teiCorpus xmlns="http://www.tei-c.org/ns/1.0" version="4.0.0">
   <teiHeader xmlns="">
      <fileDesc>
         <titleStmt>
            <title>Softcite annotated corpus</title>
            <respStmt>
               <resp>Principal Investigator</resp>
               <name>James Howison</name>
            </respStmt>
            <respStmt xml:id="curator">
               <resp>curator</resp>
               <name>James Howison, Cai Fan Du, Patrice Lopez</name>
            </respStmt>
            <respStmt xml:id="annotator0">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator1">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator2">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator3">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator4">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator5">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator6">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator7">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator8">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator9">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator10">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator11">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator12">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator13">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator14">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator15">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator16">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator17">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator18">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator19">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator20">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator21">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator22">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator23">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator24">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator25">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator26">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator27">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator28">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator29">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator30">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator31">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator32">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator33">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator34">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator35">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
            <respStmt xml:id="annotator36">
               <resp>annotator</resp>
               <name>ANONYMIZED</name>
            </respStmt>
         </titleStmt>
         <notesStmt>
            <note>The Softcite dataset is a gold standard corpus of manually annotated software mentions from academic PDFs.</note>
            <note>This corpus file contains one TEI entry per scholar publication having at least one software mention. Each paragraph containing at least one manually annotated software mentions is encoded under the TEI body element.</note>
         </notesStmt>
         <publicationStmt>
            <publisher>Howison Lab, University of Texas at Austin, School of Information</publisher>
            <date when="2019"/>
            <availability status="free">
               <licence target="https://creativecommons.org/licenses/by/4.0/">
                  <p>Attribution 4.0 International (CC BY 4.0)</p>
               </licence>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <bibl>Softcite corpus, version 0.6.0, 2020</bibl>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <appInfo>
            <application ident="GROBID" version="0.5.6-SNAPSHOT" when="2019-07-14T13:21+0000">
               <desc>A machine learning software for extracting information from scholarly documents</desc>
               <ref target="https://github.com/kermitt2/grobid"/>
            </application>
         </appInfo>
         <appInfo>
            <application ident="GROBID-SOFTWARE-MENTIONS" version="0.6.0-SNAPSHOT" when="2019-11-21T14:59+0000">
               <desc>A GROBID module to recognize in textual documents and PDF any mentions of software</desc>
               <ref target="https://github.com/ourresearch/software-mentions"/>
            </application>
         </appInfo>
         <classDecl>
            <taxonomy>
               <category xml:id="unique_annotator">
                  <catDesc>Document originally analyzed by one annotator</catDesc>
               </category>
               <category xml:id="multiple_annotators">
                  <catDesc>Document originally analyzed by more than one annotator</catDesc>
               </category>
               <category xml:id="with_reconciliation">
                  <catDesc>Document analyzed by one or more annotators with review by the curator team</catDesc>
               </category>
               <category xml:id="with_reconciliation_and_scripts">
                  <catDesc>Document analyzed by one or more annotators with review by the curator team, combined with additional corrections driven by automated consistency checks</catDesc>
               </category>
            </taxonomy>
         </classDecl>
      </encodingDesc>
   </teiHeader>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c0c3dcac8f">
            <titleStmt>
               <title>Impact of sustainable development indicators on economic growth: Baltic countries in the context of developed Europe</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3846/1648-0627.2009.10.107-117</idno>
                  <idno type="origin">10.3846%2F1648-0627.2009.10.107-117</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The variables are statistically measured. -A correlation analysis is used as a statistical method to define the relationship. Calculations were made using <rs cert="1.0" resp="#annotator11" subtype="used" type="software" xml:id="c0c3dcac8f-software-simple-0">MS Excel</rs> and are presented in Appendix 1.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d85f7c7dac">
            <titleStmt>
               <title>Co-integration Rank Determination in Partial Systems Using Information Criteria</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/obes.12195</idno>
                  <idno type="origin">10.1111%2Fobes.12195</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>where := (a, 0, 0, 0, c, c/ 2, c/ 3), := (1, 0, 0, 0, 0, 0, 0) and 1 := I 7 . 4 For the sake of comparison and with the intention of gathering additional insights, the DGP is chosen by extending the Monte Carlo set-up already exploited by <ref type="bibr">Cavaliere et al. (2012)</ref>, <ref type="bibr">CDRT and Cavaliere et al. (2016)</ref> to the case of larger dimensional VARs. Moreover, in order to fully exploit the potential of the conditional analysis we specify a 'full' (and rather arbitrary) structure for the covariance matrix such that the resultant matrix in equation <ref type="formula">(2)</ref> is non-zero for all DGPs considered. <ref type="bibr">5</ref> The initial conditions X −1 and X 0 are fixed to zero. Data are generated by setting a = −0.4 and = 0.5. The settings for the parameter c are discussed below because in our Monte Carlo design the condition c = 0 is related to the 3 All calculations in this and in the next section have been performed in <rs cert="1.0" resp="#annotator8" subtype="used" type="software" xml:id="d85f7c7dac-software-simple-0">OxMetrics</rs> and <rs cert="1.0" resp="#annotator8" subtype="used" type="software" xml:id="d85f7c7dac-software-simple-1">MATLAB</rs>. Codes are available from the authors upon request. 4 Note that the assumption := (1, 0,…, 0) does not imply any loss of generality. Indeed, any process X t satisfying equation (1) under the I(1,r 0 ) conditions with r 0 = 1 and = (1, 0,…, 0) can be suitably linearly transformed into the new process X * t such that * X * t ∼ I (0) with * := (1, 0,…, 0) . The likelihood ratio test is invariant to such rotation. <ref type="bibr">5</ref> The matrix has been obtained using the following spectral decomposition: = GDG , where the diagonal elements of D are generated randomly from a truncated standard normal and the elements of G are generated randomly using the standard normal distribution and such that GG = I p . The specific matrix used in our simulations is available upon request to the authors. We also considered Monte Carlo experiments based on specifications of leading to = 0 in equation <ref type="formula">(2)</ref>. Also in this case, results are available upon request to the authors. failure of the weak exogeneity of the conditional variables. Samples of length T = 100, 200 and 1,000 are generated M = 5, 000 times from VARs of the form equation <ref type="formula">(7)</ref> with k = 2 lags. <ref type="bibr">6</ref> For each generated sample, the partial (conditional) system <ref type="formula">(2)</ref> is estimated and the procedure for co-integration rank determination discussed in section III is applied. In particular, to evaluate the impact of the dimension of the conditioning variable vector Z t on the ability of the information criteria to determine the true co-integration rank, we consider the partition X t = (Y t , Z t ), where the dimension of Y t ranges from two to four, i.e. p Y = 2, 3 and 4, which implies a dimension for Z t of p Z = 5, 4 and 3, respectively. Thus, it is seen that the scalar parameter c in the matrix in equation <ref type="formula">(7)</ref> determines whether Z t is weakly exogenous (c = 0) or not (c = 0) for .</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d3044a8f57">
            <titleStmt>
               <title>THE SHIFTING DEMAND FOR HOUSING BY AMERICAN RENTERS AND ITS IMPACT ON HOUSEHOLD BUDGETS: 1940-2010*</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/jors.12298</idno>
                  <idno type="origin">10.1111%2Fjors.12298</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Mayo notes that income elasticity estimates based on current income tend to be smaller than those based on permanent income. Our income elasticity may have a further downward bias due to measurement error inherent in the census asking people about current rent, but last year's income. <ref type="bibr">20</ref> We use the household head's education as an instrument for income; education is correlated with permanent income and, we assume, uncorrelated with transient income and errors from using lagged income. Column 4 of <ref type="table">Table 6</ref> reports the results from a 2SLS IV estimation of the demand function of column 1. The IV-estimated price elasticity, −0.39, is only slightly larger than the OLS estimate, but the IV-estimated income elasticity, 0.53, is much higher than the OLS estimate. These estimates are more in line with past studies than were the OLS estimates. <ref type="bibr">21</ref> Because the <ref type="bibr">18</ref> The dependent variable in our demand equations is the natural log of reported rent less the natural log of price. In our log-log specification, subtracting the log of price from both sides alters only the coefficient on the log of price. In particular, it has no effect on the disturbances. Consequently, any instrument valid for estimating a rent regression is also valid for estimation when the dependent variable is the log of rent less the log of price. It is tempting to look for land price data to use as an instrument, given the importance of land's cost in supply. However, a valid instrument cannot be correlated with the part of an endogenous explanatory that is correlated with the disturbance term <ref type="bibr">(Murray, 2006)</ref>. Land values are largely locally driven, so shocks to local housing demand are likely to be reflected in local land prices. We would need an instrument with both temporal and cross-sectional variation that is uncorrelated with the endogenous component of land price. Such is hard to come by. <ref type="bibr">19</ref> The construction cost index (http://enr.construction.com/economics) is based on "200 hours of common labor at the 20-city average of common labor rates, plus 25 cwt of standard structural steel shapes at the mill price prior to 1996 and the fabricated 20-city price from 1996, plus 1.128 tons of Portland cement at the 20-city price, plus 1,088 board-ft of 2 × 4 lumber at the 20-city price." The building cost index is based on "68.38 hours of skilled labor at the 20-city average of bricklayers, carpenters and structural iron workers rates, plus 25 cwt of standard structural steel shapes at the mill price prior to 1996 and the fabricated 20-city price from 1996, plus 1.128 tons of Portland cement at the 20-city price, plus 1,088 board ft of 2 × 4 lumber at the 20-city price." Many of the components of the index are determined in regional markets, but labor's cost is determined more locally. Since rent is one determinant of local wages among many, there remains room for concern about the construction cost index as a valid instrument. <ref type="bibr">20</ref> The incomes in the regression have been adjusted for price differences between the census year and the preceding year, but not for any growth or fall in incomes. <ref type="bibr">21</ref> The P-values of the F-statistics for the relevance of the instruments in the rental-housing price and income reduced form equations are 0.053 and less than 0.0005, respectively. While the instruments appear relevant, we find that the instruments are weak for rental housing's price (using Finlay and Magnuson's 2009 riv test routine in <rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="d3044a8f57-software-simple-0">STATA</rs>, which is robust to clustered disturbances, and conditioning on the IV estimate of the income elasticity). The instruments' weakness arises because fixed-effects for city and year in the model absorb much of the variation in both the cost indices and the price of rental housing, leaving IV estimates are only for a sample of 11 cities, <ref type="bibr">22</ref> we also report, in column 3 of <ref type="table">Table 6</ref>, the results of an OLS regression for those 11 cities. Note that the 11-city income and price elasticities, −0.37 and 0.18, are very close to the full sample OLS estimates; the time dummies also have similar coefficients in the two regressions. The similar estimates in columns 1 and 3 encourage us to consider the IV estimates from the 11 cities as instructive for the 16 cities. <ref type="bibr">23</ref> The estimated demographic effects are unsurprising and almost all are statistically significant. Larger households consume more housing; single-parents living with no other adult consume more housing than households with two or more adults; non-Hispanic whites consume more housing than other groups; cross-price elasticities are generally small and often statistically insignificant, though the estimate of 0.18 for transportation strikes us as unduly large; the implicit OLS estimate of the cross-price elasticity of housing with respect to the price of food is −0.31 and statistically significant; and the baby-boomers and their successors consume more housing than earlier birth cohorts. This last result is consonant with the basic theme of this paper: tastes for housing change over time. Household heads born prior to World War II have a substantially different view of what constitutes suitable housing than do household heads from the baby-boom generation and their successors; prewar households consume about 10 percent less housing than babyboomers, ceteris paribus. (We experimented with several variants of the age variable specification. The estimated cohort effects were robust to these.)</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="b8da0426e6">
            <titleStmt>
               <title>Are There Environmental Benefits from Driving Electric Vehicles? The Importance of Local Factors</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/aer.20150897</idno>
                  <idno type="origin">10.1257%2Faer.20150897</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>To analyze any policy which affects multiple counties, we need a sense of the relative importance of driving in the counties. We weight all summary statistics using vehicle miles traveled (VMT) in each county, as estimated by the <rs corresp="#b8da0426e6-software-0" resp="#curator" type="publisher">EPA</rs> for their <rs resp="#curator" type="software" xml:id="b8da0426e6-software-0">Motor Vehicle Emission Simulator (MOVES)</rs>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f82ef5e24c">
            <titleStmt>
               <title>A Short Introduction to the World of Cryptocurrencies</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.20955/r.2018.1-16</idno>
                  <idno type="origin">10.20955%2Fr.2018.1-16</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>To use the Bitcoin system, an agent downloads a <rs cert="1.0" resp="#curator" subtype="used" type="software" xml:id="f82ef5e24c-software-simple-0">Bitcoin wallet</rs>. A <rs cert="1.0" resp="#annotator8" subtype="used" type="software" xml:id="f82ef5e24c-software-simple-1">Bitcoin wallet</rs> is software that allows the receiving, storing, and sending of (fractions of) Bitcoin units. 3 The next step is to exchange fiat currencies, such as the U.S. dollar, for Bitcoin units. The most common way is to open an account at one of the many Bitcoin exchanges and to transfer fiat currency to it. The account holder can then use these funds to buy Bitcoin units or one of the many other cryptoassets on the exchange. Due to the widespread adoption of Bitcoin, the pricing on large exchanges is very competitive with relatively small bid-ask spreads. Most exchanges provide order books and many other financial tools that make the trading process transparent.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="a70423884d">
            <titleStmt>
               <title>Spatial Interactions in Hedonic Pricing Models: The Urban Housing Market of Aveiro, Portugal</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1080/17421772.2011.647058</idno>
                  <idno type="origin">10.1080%2F17421772.2011.647058</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>What remains now is to estimate the within-submarket spatial weight v 0 . Since dwellings within a submarket are in general located closer to each other than those across different submarkets, it is expected that v 0 will be large compared to the cross-submarket spatial weights. We propose maximum likelihood to conduct this estimation. Specifically, for any candidate value of v 0 , we construct the corresponding row-standardized spatial weights matrix W, estimate the spatial error model using maximum likelihood, and evaluate the value of the maximized likelihood. In this way, we construct spatial weights matrices using various candidate values for v 0 , estimate the corresponding spatial error models by maximum likelihood using <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a70423884d-software-4">Geoda</rs> <ref type="bibr">(Anselin, 2005)</ref>, and maximize the likelihood over all such candidate values. Standard errors are estimated by numerical approximation to compute the Fisher information at this maximized value for v 0 . 14 Finally, as discussed earlier, we use the <ref type="bibr">Born &amp; Breitung (2011)</ref> regression test to examine the validity of spatial error dependence against a hybrid model including a spatial lag.</p>
            <p>Next, we used the <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a70423884d-software-5">GEODA</rs> software <ref type="bibr">(Anselin, 2005; Anselin et al., 2006)</ref> to perform a series of Lagrange Multipliers (LM) tests <ref type="bibr">(Anselin, 2005)</ref> for both spatial lag dependence and spatial error dependence. Specifically, in addition to OLS, we estimated by maximum likelihood (ML) alternative spatial regression models and investigated whether a spatial error or a spatial lag model, or indeed a model <ref type="figure">Figure 2</ref>. Moran scatter plot for residuals (contiguity weight matrix). Source: Results were produced using <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a70423884d-software-6">GEODA</rs> software <ref type="bibr">(Anselin, 2005)</ref>. without spatial effects, best fit the data. We report the results for a spatial weights matrix based on rook and queen contiguity in <ref type="table">Table 6</ref>; results for other specifications of spatial weights are similar. Like Moran's I statistics, we find no evidence of spatial dependence. This is despite the fact that we have not accounted for spatial heterogeneity in these estimates*a feature that can contribute to spatial dependence. The Lagrange Multiplier (LM) test statistics are significant neither for the spatial error (p-value 0.41) nor the spatial lag (p-value 0.77) models. The null hypothesis of both tests, which is the lack of spatial dependence, cannot be rejected at the 5% significance level.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d5d89d8d6e">
            <titleStmt>
               <title>What drives compliance? An application of the theory of planned behaviour to urban water restrictions using structural equation modelling</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1080/00036846.2016.1218430</idno>
                  <idno type="origin">10.1080%2F00036846.2016.1218430</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The software package <rs resp="#curator" type="software" xml:id="d5d89d8d6e-software-1">AMOS (Analysis of Moment Structures)</rs> by <ref type="bibr">Arbuckle (2006)</ref> was used to analyse the data. Initially, a test of multivariate normality was conducted and, as a result, six observations were removed. These observations had Mahalanobis distance values that deviated significantly from the other observations 3 -their removal resulted in a sample size of 504.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f78d8af6e0">
            <titleStmt>
               <title>From “Made in China” to “Innovated in China”: Necessity, Prospect, and Challenges</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/jep.31.1.49</idno>
                  <idno type="origin">10.1257%2Fjep.31.1.49</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Can China transition from a world assembly line to an innovation powerhouse? It's easy to list reasons to be skeptical. There is no shortage of news stories of intellectual property rights violations by Chinese companies. <ref type="bibr">4</ref> There is criticism that the Chinese school system puts too much weight on rote learning and not enough on creative and critical thinking. On the other hand, more optimistic examples are available, too. Tencent, the company that provides the popular communication tool, <rs cert="0.8" resp="#annotator2" type="software" xml:id="f78d8af6e0-software-simple-0">WeChat</rs>, which combines group chat, voice calls, video sharing, and financial exchanges, is generally regarded as among the most innovative internet companies in the world. Huawei, the telecom equipment producer, is said to take out more patents a year than either Apple or Cisco. The world's first quantum satellite was launched by China in August 2016. To address whether such examples of innovation are exceptions or the norm, we offer a systematic look at the data in the next section. It is hard to quantify with precision the relative contributions to total factor productivity growth from different sources. From the China Statistical Yearbook on Science and Technology, we compute and compare investment made by firms in the survey in (a) importing and digesting foreign technologies, (b) buying and digesting technologies from other domestic firms, and (c) developing their own in-house technological improvement. In 2000, the survey firms collectively spent nearly 20 percent of their technology improvement budget on importing and digesting foreign technology, about 2 percent on buying technologies from other domestic sources, and 78 percent on developing their own in-house technological improvement. Over time, the share of the first item declines, whereas the last two items expand. By 2014, the survey firms collectively spent 11 percent of their technological improvement budget on importing and digesting international technologies, about 5 percent on buying technologies from domestic sources, and the remaining 84 percent on developing their own in-house technological advancement, with the last two categories showing a significant increase over the shares in 2000 (see online Appendix <ref type="figure">Figure A1</ref>). These numbers indicate in an indirect way the improvement in the domestic innovation capacity in China's manufacturing sector.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c89554d7f3">
            <titleStmt>
               <title>Geocomputation: A Practical Primer, edited by Chris Brunson and Alex Singleton. 2015. London: Sage Publication Ltd. 369+xx. ISBN: 9781446272930. $52.00.</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/jors.12244</idno>
                  <idno type="origin">10.1111%2Fjors.12246</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>In addition to these thematic blocks, a most welcome feature of the book is the series of chapters intelligently inserted in each block that cover aspects of geocomputation referring to tools and software. In particular, it is refreshing to see how all of these relate to open-source projects freely available to use. The landscape where the only choices to perform geocomputational analyses were proprietary, desktop, point-and-click, GIS programs, or custom routines in low-level languages such as C or FORTRAN is no longer the norm. Instead, an entire ecosystem of open-source software, more flexible than traditional desktop GIS and more accessible than low-level computer programming has emerged. Just a few years ago, it was not very clear that one day it would be possible to carry out serious research in geocomputation using only free and/or open-source software. That the field has evolved in this direction to a point where there is a full stock of tools <ref type="bibr">(Rey, 2009</ref>) that realize such a vision is greatly inspiring and it is something that the larger community of researchers who have made it possible should feel proud of. The book contains an overview of open-source software for GIS <ref type="bibr">(O'Brien, chapter 17)</ref>, and two other chapters are devoted to the most common programming languages used in geocomputation, <rs cert="1.0" resp="#curator" type="software" xml:id="c89554d7f3-software-simple-0">R</rs> (Cheshire and Lovelace, chapter 1) and Python <ref type="bibr">(Rey, chapter 14)</ref>. An additional chapter (15) by Brunsdon and Singleton elaborates on the idea of reproducibility, describing the concept, why it should be adopted, and how it can be realistically implemented. This bit echoes a larger debate that relates to the very core of what Science is about and how to keep it serving society in the 21st century, and its inclusion in the book is a reassuring sign of the forward-looking attitude of the editors.</p>
            <p>At a more global level, the book recognizes and embraces many of the "big picture" shifts occurring in the field and accommodates them throughout the book. In addition to the already mentioned new approaches to geocomputational software and tools, there is an explicit recognition that the data landscape is changing and that this will require adaptation to be able to extract the most value and knowledge. This is very clear in parts of the book, such as chapter 9 (Miller), which recognize the need to accommodate and properly represent these new forms of data. But it is also more subtly present in the choice of methods reviewed. For example, the inclusion of a chapter on self-organizing maps, a machine learning technique to reduce data dimensionality and visualize large complex datasets, is very much in that same spirit. Very welcome is also the role given by the editors to visualization. One could argue that graphics have always been an essential part of GIS and, by extension, geocomputation. However, it is easy to get lost in the complexity and computational aspects of some of the methodologies and assume that because the results come from very complicated techniques, the presentation of such results is of secondary importance. The book takes a clear stance on this and recognizes not only its importance, but its nontriviality, including chapters that walk the reader through some of the cutting-edge tools such as <rs cert="1.0" resp="#curator" type="software" xml:id="c89554d7f3-software-simple-1">R</rs>, <rs cert="1.0" resp="#annotator3" type="software" xml:id="c89554d7f3-software-simple-2">Python</rs>, or <rs cert="1.0" resp="#annotator3" type="software" xml:id="c89554d7f3-software-simple-3">JavaScript</rs> libraries for the web.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f91f5920ad">
            <titleStmt>
               <title>THE EFFECTS OF YOUNG ADULT-DEPENDENT COVERAGE AND CONTRACEPTION MANDATES ON YOUNG WOMEN</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/coep.12223</idno>
                  <idno type="origin">10.1111%2Fcoep.12223</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Moreover, the cost of prescription contraception is relatively expensive, especially for young adults whose incomes tend to be lower than other groups. For example, <ref type="bibr">Trussell et al. (2013)</ref> estimates the average annual cost of short-acting prescription contraception to be $390.53 for injections, $654.30 for the birth control pill, and $1,023.86 for the birth control patch. <ref type="bibr">1</ref> The reported spike in requests and <rs cert="0.7" resp="#annotator2" type="software" xml:id="f91f5920ad-software-simple-0">Google</rs> searches for intrauterine devices immediately after the 2016 election is strong, although admittedly anecdotal, evidence of the importance women place on having the cost of their contraception covered <ref type="bibr">(Ross 2016)</ref>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d17a05a47f">
            <titleStmt>
               <title>Stochastic Volatility in a Macro-Finance Model of the U.S. Term Structure of Interest Rates 1961-2004</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/j.1538-4616.2008.00153.x</idno>
                  <idno type="origin">10.1111%2Fj.1538-4616.2008.00153.x</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Similarly, using (1): z t+1 =ẑ t+1,t + Gη t+1 + 0 (y t+1 −ŷ t+1,t ), wherê z t+1,t = K + 0ŷt+1,t + L−1 l=0 l z t−l , (C3) and using <ref type="formula">(12)</ref>: r t+1 =r t+1,t + B 0 (y t+1 −ŷ t+1,t ) + B 1 (z t+1 −ẑ t+1,t ) + e t+1 , wherê r t+1,t = α + B 0ŷ t+1,t + B 1ẑ t+1,t + L l=2 B l z t+2−l . <ref type="bibr">(C4)</ref> 14. I am very grateful to Zhuoshi Liu for suggesting this algorithm and converting my basic files from<rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d17a05a47f-software-simple-0">Mathematica</rs> to <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d17a05a47f-software-simple-1">Matlab</rs>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="b4e3e280d1">
            <titleStmt>
               <title>Proactive planning and valuation of transmission investments in restructured electricity markets</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s11149-006-9003-y</idno>
                  <idno type="origin">10.1007%2Fs11149-006-9012-x</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>As mentioned before, the KKT conditions of the period-3 problem of the PNP model constitute a LCP. We solve it, for each contingent state, by minimizing the complementarity conditions subject to the linear equality constraints and the non-negativity constraints. 17 The period-2 problem of the PNP model is an EPEC, in which each firm faces a Mathematical Program subject to Equilibrium Constraints (MPEC). <ref type="bibr">18</ref> We attempt to solve for equilibrium, if at least one exists, by iterative deletion of dominated strategies. That is, we sequentially solve each firm's profit-maximization problem using as data the optimal values from previously solved problems. Thus, starting from a feasible solution, we solve for g 1 using g <ref type="bibr">(−1)</ref> as data in the first firm's optimization problem (where g (−1) means all firms' generation capacities except for firm 1's), then solve for <ref type="bibr">17</ref> Recall that any LCP can be written as the problem of finding a vector x ∈ n such that x = q + M · y, x T · y = 0, x ≥ 0, and y ≥ 0, where M ∈ n×n , q ∈ n , and y ∈ n . Thus, we can solve it by minimizing x T · y subject to x = q + M · y, x ≥ 0, and y ≥ 0. If the previous problem has an optimal solution where the objective function is zero, then that solution also solves the corresponding LCP. Greater details about the methodology used for solving LCPs are given in <ref type="bibr">(Hobbs, 2001)</ref>. 18 See <ref type="bibr">(Yao et al., 2004)</ref> for definitions of both EPEC and MPEC.g 2 using g <ref type="bibr">(−2)</ref> as data, and so on. We solve each firm's profit-maximization problem using a sequential quadratic programming algorithm implemented in<rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="b4e3e280d1-software-simple-0">MATLAB</rs> . We test our model from a set of different starting points and using different generation-firms' optimization order. All these trials gave us the same results. For the PNP model, the optimal levels of generation capacity under absence of transmission investments are (g * 1 , g * 2 , g * 3 , g * 4 , g * 5 ,g * 6 ) = (100 <ref type="bibr">92, 103.72, 101.15, 95.94, 77.07, 87.69)</ref> in MW. <ref type="table">Table 3</ref> lists the corresponding generation quantities (q i ), adjustment quantities (r i ) and nodal prices (P i ) in the normal state. <ref type="figure">Figure 4</ref> illustrates these results for the Cornell network. In <ref type="figure">Fig. 4</ref>, thick lines represent the transmission lines reaching their thermal capacities (in the indicated direction) and circles are located in the nodes with the highest prices (above $48/MWh).</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="a647f3a5a5">
            <titleStmt>
               <title>A Numerical Toolbox to Solve N-Player Affine LQ Open-Loop Differential Games</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s10614-011-9257-z</idno>
                  <idno type="origin">10.1007%2Fs10614-011-9257-z</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>In the rest of this paper we will concentrate on the case that the players base their decisions on a performance criterion that has an infinite-planning horizon. We will present an algorithm and a corresponding <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-0">MATLAB</rs> numerical toolbox which solves any form of an infinite-planning horizon affine linear quadratic open-loop differential game. The software, called <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-1">LQDG Toolbox</rs>, is available as a freeware from the authors of this paper. 1 By rewriting a specific application into the standard framework, one can use the toolbox to calculate and verify the existence of both the open-loop non-cooperative Nash equilibrium (equilibria) and cooperative Pareto equilibrium (equilibria) of any infinite-planning horizon affine linear quadratic open-loop differential game. In case there is more than one equilibrium for the non-cooperative case, the toolbox determines all solutions that can be implemented as a state-feedback strategy that is a common assumption in most of the applications. 2 Alternatively, the toolbox can apply a number of choice methods in order to discriminate between multiple equilibria. For instance, one can choose to report only Pareto-undominated solutions or only those that are characterised by the lowest combined loss of all the players. In order to determine the cooperative solution, the user is asked to specify the relative importance of each player in the cooperative game. Moreover, the user can predefine a set of coalition structures for which they would like to calculate the noncooperative Nash solution(s). Conversely, a coalition structure generator is provided that automatically creates a whole space of coalition structures for a given number of players. Furthermore, the toolbox offers plotting facilities as well as other options to analyze the outcome of the game. For instance, it is possible to disaggregate each player's total loss into its contributing elements, which correspond to the quadratic expressions constituting the player's loss function.</p>
            <p>Step A2 in the above algorithm verifies whether the algebraic Riccati equations (20) have a stabilizing solution. Of course one can use here <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-2">MATLAB</rs> to verify this. Concerning the numerical stability of Algorithm 5 we notice that various suggestions have been made in the literature to calculate solutions of Riccati equations in a numerically reliable way (see e.g. <ref type="bibr">Laub (1979, 1991)</ref>, <ref type="bibr">Paige and van Loan (1981)</ref>, <ref type="bibr">van Dooren (1981)</ref>, <ref type="bibr">Mehrmann (1991)</ref> and Abou-Kandil and Bertrand (1986) for a more general survey on various types of Riccati equations). These methods can also be used to improve the numerical stability of Algorithm 5. In particular, if one considers the implementation of large scale models one should consult this literature.</p>
            <p>Next we proceed with an outline of the numerical toolbox. The software verifies the existence of and, provided that a finite number of equilibria exists, calculates the outcome of the N -player extension of the game (1-3). The scheme presenting all the components of the toolbox software is displayed in <ref type="figure">Fig. 1</ref>. The main file, called<rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-3">LQDGsolver.exe</rs>, solves the LQDG which is to be defined in the input. The input file can be created by the user using an intuitive input interface provided (file <rs resp="#curator" type="software" xml:id="a647f3a5a5-software-simple-4">TBXinput- GUI.exe</rs>). Alternatively, more proficient users might choose to create the input file directly (in <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-5">MATLAB</rs> or text formats). <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-6">LQDGsolver</rs> produces the following output for every coalition structure considered:</p>
            <p>The above output is saved in <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-7">MATLAB</rs> and text formats in a directory that corresponds to the chosen name of the project. The following output files are created:</p>
            <p>• PROJECT_NAME_model.txt: text file containing the structural and reduced form of the dynamic system; • inputPROJECT_NAME.m: binary <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-8">MATLAB</rs> file containing (processed) input to the project; • PROJECT_NAME_validation.txt: text file containing all the information about the various stages of model validation; • PROJECT_NAME_output.txt: text file containing all the output produced; and • outputPROJECT_NAME.m: binary <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-9">MATLAB</rs> file containing all the project's output.</p>
            <p>The plotting tool is provided that uses the above output to draw the dynamics of every variable in the model for a chosen coalition structure and equilibrium. Less advanced users can use a simple output interface (file <rs cert="1.0" resp="#annotator14" type="software" xml:id="a647f3a5a5-software-simple-10">TBXoutputGUI.exe</rs>) that allowsboth to edit the toolbox output and to plot the graphs required. Conversely, more advanced users can directly analyse output of all numerical simulations and create graphs.</p>
            <p>Step T1: <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-11">LQDG Toolbox</rs> Initialisation To define the LQDG problem the user is supposed to provide a number of compulsory components of the model. As it has been mentioned before, it is the most convenient to use the interface provided in order to create the project. The main window of the user interface is shown in <ref type="figure">Fig. 2</ref>. More proficient users also may create the input file directly in <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-12">MATLAB</rs> or text formats. Compulsory components of the LQDG problem include:</p>
            <p>• the nonzero P i and P i j matrices from the structural form model (30-31); • the parameters from the performance criterion (32); • the initial condition p 0 ; and • the coalition structures to be considered. 8 • If the model includes constants, i.e., at least one element of matrices P 5 and P 10 is non-zero, then it is mandatory to specify a strictly positive discount rate.Step T2: Model Validation by <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-13">LQDG Toolbox</rs> Based on the input that is provided in</p>
            <p>Step T1: <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-14">LQDG Toolbox</rs> Initialisation The above maximisation problems can be rewritten in terms of (32) as:</p>
            <p>Step T2: Model Validation by <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-15">LQDG Toolbox</rs> With this input the toolbox next calculates the standard form (17-18) by considering the new variables:</p>
            <p>Step T1: <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-16">LQDG Toolbox</rs> Initialisation We can rewrite the above example to fit our standard model (30-31) by specifying:</p>
            <p>Step T2: Model Validation by <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-17">LQDG Toolbox</rs> With this input the toolbox next calculates the standard form (17-18). Clearly, (i) R ii &gt; 0; (ii) (A, B i ) is stabilizable; and (iii) G is invertible (see Appendix A for a definition of G).</p>
            <p>Step T1: <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-18">LQDG Toolbox</rs> Initialisation We can rewrite the above example to fit our standard model (30-31) by specifying: P 1 = P 2 = P 3 = P 4i = P 5 = P 6 = P 8 = 0, P 7 = A, P 9i = B i and P 10 = 0 0 .</p>
            <p>Step T2: Model Validation by <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-19">LQDG Toolbox</rs> With this input, the toolbox next calculates the standard form (17-18) and checks that both Q i and R i are positive definite, (A, B i ) are stabilizable and G is invertible.</p>
            <p>In this paper we considered a dynamic linear affine structural form model that is affected by different players who all like to minimise their own performance criterion that is a quadratic affine function of the variables occurring in the model. The costs are assumed to be discounted over time and the considered planning horizon by the players is assumed to be infinite. Under the assumption that in the minimisation of their performance the players do not cooperate, we presented both necessary and sufficient conditions under which this problem has a unique open-loop Nash equilibrium, a multiple but finite number of equilibria and or an infinite number of equilibria. A computational framework was provided for how one can numerically solve the problem. The algorithm has been implemented in a form of a numerical toolbox available on the internet. Users, starting from the structural model, can calculate for their specific application the equilibrium strategies and involved cost (if they exist). The toolbox also provides the possibility to calculate for different coalition structures whether the corresponding game will have an open-loop Nash solution. For that purpose the user has to define which coalition structures they like to analyze and what the relative importance is of each player within a certain coalition. We demonstrated both theoretically and numerically in a worked example on dynamic duopolistic competition the use of the toolbox.<rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-20">LQDG Toolbox</rs> is implemented in <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-21">MATLAB</rs>. In particular, it uses some standard functions of <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-22">MATLAB</rs> to calculate the eigenstructure of a (|N |n) × (|N |n) matrix, where |N | is the number of involved players andn is the state dimension of the model. Since no additional efforts are taken to calculate this eigenstructure in a numerically efficient way, the practical use of the current toolbox is limited to some extent. This is because for either a large number of players and/or a large state dimension, the accuracy and efficiency is restricted by that of the implemented <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-23">MATLAB</rs> functions. So for large N and/orn the user should look for an own code to implement the algorithm. Another way one might choose to calculate the equilibrium strategies is by using iterative algorithms. In the literature a number of iterative schemes have been suggested (see e.g. <ref type="bibr">Engwerda 2007)</ref>. A disadvantage of these schemes is that on the one hand they do not provide an answer to the question whether the game will have a unique equilibrium. On the other hand these schemes may converge without providing the appropriate equilibrium strategy. If this happens one is stuck with the question how to proceed.</p>
            <p>For the corresponding problem with a finite planning horizon, at least from a theoretical point, it is clear under which conditions there exists a unique equilibrium (see e.g. <ref type="bibr">Engwerda 2005)</ref>. From a computational point it is also clear how one can calculate this equilibrium. Either one can solve the involved set of nonlinear differential two-point boundary-value equations directly using standard <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-24">MATLAB</rs> functions. Another possibility is to transform the involved set of Riccati differential equations (in the spirit of <ref type="bibr">Reid (1972)</ref>) to a set of linear differential equations and then solve this set first (see e.g. <ref type="bibr">Tabak 1975; Engwerda 2007)</ref>. Since the calculations require the numerical solution of a set of (nonlinear) differential equations the dimension of the games for which one can still calculate the equilibrium actions (using standard<rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-simple-25">MATLAB</rs> functions) without problems is usually smaller than in the infinite horizon case.</p>
            <p>Finally, we would like to mention that for discrete time systems much work has been done by <ref type="bibr">Neck et al.</ref> in the development of the numerical software <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a647f3a5a5-software-58">OPTGAME</rs> for the calculation of Nash equilibria in (non-)linear systems in case the performances of players are quadratic (see e.g. <ref type="bibr">Neck et al. 2001)</ref>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d8bf6fa9c8">
            <titleStmt>
               <title>How transparent about its inflation target should a central bank be?</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s00191-018-0558-4</idno>
                  <idno type="origin">10.1007%2Fs00191-018-0558-4</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>We use the Matérn v = 5 2 covariance function, which is the default one in<rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d8bf6fa9c8-software-10">DiceKriging</rs>-package of <rs corresp="#d8bf6fa9c8-software-10" resp="#curator" type="publisher">R Development Core Team</rs> <ref type="formula">(2009)</ref>:</p>
            <p>As soon as we have a satisfying metamodel L, we determine the pair (φ * π , φ * u ) that minimizes the estimated value of loss L, denoted by L * . This is done through the packages <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d8bf6fa9c8-software-11">rgenoud</rs> (R-GENetic Optimization Using Derivatives, see Mebane and Sekhon 2011) and DiceOptim (see <ref type="bibr">Roustant et al. 2010</ref>) provided by <rs corresp="#d8bf6fa9c8-software-11" resp="#curator" type="publisher">R Development Core Team</rs> (<rs corresp="#d8bf6fa9c8-software-11" resp="#curator" type="version">2009</rs>). This is a quite powerful optimization function that efficiently combines evolutionary algorithm methods for global purpose with a derivative-based (quasi-Newton) method for local search of optima. <ref type="table">Table 8</ref> Kriging models reports -2 parameters (Section 4.1)</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d1084322b7">
            <titleStmt>
               <title>Isolated Capital Cities, Accountability, and Corruption: Evidence from US States</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/aer.104.8.2456</idno>
                  <idno type="origin">10.1257%2Faer.104.8.2456</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>For the sake of robustness, we will also look at different approaches to measuring corruption. First, we follow <ref type="bibr">Saiz and Simonsohn (2013)</ref> in building a measure from an online search, using the <rs cert="0.7" resp="#annotator2" subtype="used" type="software" xml:id="d1084322b7-software-simple-0">Exalead</rs> tool, for the term "corruption" close to the name of each state (performed in 2009). 8 Lastly, we consider additionally (in the online Appendix) the measure of corruption perceptions in state politics introduced by <ref type="bibr">Boylan and Long (2003)</ref>, based on a set of questions posed to reporters covering State Houses, and the TRACFed-based measure of convictions of local officials. The former is less objective, but gives us another measure of statelevel corruption; the latter provides a measure of spillovers across different levels of government. 9 5 Still, there obviously is variation related to the functioning of local District Attorney (DA) offices and federal agencies, introducing measurement error in the variable (Alt and Lassen forthcoming, Gordon 2009). <ref type="bibr">6</ref> As an illustration of the former, consider the case involving former Alabama governor Don Siegelman, who was convicted of corruption charges in 2006. As can be gleaned from the 2006 PIS Report, four people were convicted in addition to the governor, in relation to the same episode, and none of them were state officials. 7 This restricted measure is much noisier, not the least because, since there are relatively few state-level officials compared to other levels, their share in aggregate convictions is relatively small-typically about 10 percent overall, as compiled in the PIS Report. The average number of convictions per state-year in the overall measure is about 14, whereas the number for the restricted measure constructed from TRACfed is just under one. <ref type="bibr">8</ref> The choice of <rs cert="0.7" resp="#annotator2" subtype="used" type="software" xml:id="d1084322b7-software-simple-1">Exalead</rs> is due to its being one of the few engines offering reliable "proximity" searches <ref type="bibr">(Saiz and Simonsohn 2013, p.138)</ref>. They argue that this measure performs well in reproducing the standard stylized facts found by the literature on corruption, both at the state and country levels. <ref type="bibr">9</ref> These measures are typically significantly correlated with one another (see the online Appendix). In particular, the baseline measure of federal convictions is highly correlated with the measure restricted to state officials (just under 0.6), and somewhat less so with the measure restricted to local officials (about 0.4). The two restricted measures are significantly correlated with each other (0.33), consistent with the existence of spillovers. The <rs cert="0.7" resp="#annotator2" subtype="used" type="software" xml:id="d1084322b7-software-simple-2">Exalead</rs> measure has a more tenuous correlation with the baseline (0.25, significant at the 10 percent level).</p>
            <p>In column 1, we see a strong positive correlation between corruption and AvgLogDistanc e not (measured as an average of the measures calculated up to 1970, i.e., before the time period for which corruption is measured), without any controls campante and do: Isolated capItal cItIes and corruptIon Vol. 104 no. 8other than geographical size. <ref type="bibr">17</ref> Column 2 then introduces a basic set of controls, as of 1970. The coefficient of interest is highly significant, and fairly stable in size. Columns 3 and 4 add as controls other correlates of corruption that are established in the literature, and our preferred specification is that of column 3, which essentially reproduces the basic specification in <ref type="bibr">Glaeser and Saks (2006)</ref>. While in column 4 the size of the coefficient is slightly reduced, it is robustly statistically significant at the 1 percent level, quite remarkably in light of the small sample size. <ref type="bibr">18</ref> The same pattern is also present for our first alternative measure of isolation, AvgLogDistanc e adj , as shown by columns 5-8 reproducing the four specifications. <ref type="bibr">19</ref> The effect is also meaningful quantitatively. Our preferred specification's coefficient (1.03) implies that an increase of one standard deviation in the isolation of the capital city (around 0.09, or roughly the increase experienced by Carson City, NV between 1920 and 2000), would yield a corresponding increase in corruption (0.10) of around 0.75 standard deviation. <ref type="bibr">20</ref> Let us now consider the robustness of our results, beyond the different specifications in <ref type="table">Table 2</ref>. We first consider alternative measures of corruption, in <ref type="table">Table 3</ref>. Columns 1 and 2 reproduce the main specification from our baseline results (columns 3 and 7 in <ref type="table">Table 2</ref>), for the measure of corruption convictions restricted to state officials. <ref type="bibr">21</ref> They very much confirm the message from <ref type="table">Table 2</ref>. Even quantitatively, the results are fairly similar, and especially so when we take into account that this is a noisier measure: an exercise along the lines of what we have done for the baseline results would yield an effect of just over 0.55 standard deviation. <ref type="bibr">22</ref> We then look at the alternative <rs cert="0.7" resp="#annotator2" subtype="used" type="software" xml:id="d1084322b7-software-simple-3">Exalead</rs> measure of corruption. <ref type="bibr">23</ref> Columns 3 and 4 again mimic the main baseline specifications, and again find very similar results. The estimated quantitative effect is now of about 0.7 standard deviation, once again very close to the baseline. <ref type="bibr">24</ref> The next step is to check for alternative measures of the isolation of the capital city. We find that the results are still present with both versions of AvgDistance, adjusted and unadjusted, as shown in columns 5 and 6. For the coarser measures, CapitalShare and CapitalLargest, we see negative coefficients in columns 7 and 8, <ref type="bibr">17</ref> The online Appendix shows that the results are still present without the controls. <ref type="bibr">18</ref> The results are not sensitive to outliers: they are still present when we run the regressions excluding one census region at a time. They also survive measures of party competition and of the breakdown of state revenues between taxes and other sources. These can all be seen in the online Appendix. <ref type="bibr">19</ref> We do not include controls for geographical size, since this is built into the measure of concentration. The results are not sensitive to that choice (see the online Appendix). <ref type="bibr">20</ref> For the sake of comparison, <ref type="bibr">Glaeser and Saks (2006)</ref> find in their sample an effect of about half of a standard deviation of a corresponding one-standard-deviation increase in education, a variable that has been consistently found to be (negatively) correlated with corruption <ref type="bibr">(Alt and Lassen 2003, Glaeser and Saks 2006)</ref>. <ref type="bibr">21</ref> We run weighted regressions, using yearly standard deviations of the measures of convictions for each state over the sample period of 1986-2011, in order to adjust for the fact that the small number of convictions entails noise in the measures. The results are essentially the same if we run unweighted regressions instead (see the online Appendix). <ref type="bibr">22</ref> A positive correlation also holds for a narrow measure restricted to convictions of local officials (see the online Appendix). This is consistent with the idea that a culture of corruption at the state level spills over to other levels of government within the state. <ref type="bibr">23</ref> Since the measure of corruption is computed over a more recent period, we use here the average of the measures of isolation up to 2000, and use the demographic control variables as of 2000 as well. <ref type="bibr">24</ref> The regression results are also robust when we use the <ref type="bibr">Boylan and Long (2003)</ref> measure of corruption perceptions in state politics. They are also quantitatively very similar to our baseline: the estimated coefficient implies that an increase in AvgLogDistanc e not by one standard deviation is associated with an increase in the measure of corruption perception of about 0.75 standard deviation. (See the online Appendix.) campante and do: Isolated capItal cItIes and corruptIon <ref type="bibr">Vol. 104 no. 8</ref> consistent with the baseline results. The quantitative implications, however, suggest in both cases a smaller effect, of about one-third of a standard deviation. This is consistent with a substantial measurement error being introduced by the use of these coarse measures. <ref type="bibr">25</ref> We then probe the results with a few "placebo" regressions, meant to check whether the patterns we find in the data are actually related to the isolation of capital cities and its conjectured link with corruption and accountability. Columns 1-4 in <ref type="table">Table 4</ref> use the isolation of the largest city-since the latter is also the capital city in 17 out of 50 states, one might wonder whether the measure of isolation of the capital could be in fact proxying for that. It has no independent effect, and its inclusion does not affect the significance or size of the coefficient on the isolation of the capital.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="fcc1c07d38">
            <titleStmt>
               <title>Building research skills in the Macalester economics major</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1080/00220485.2017.1353464</idno>
                  <idno type="origin">10.1080%2F00220485.2017.1353464</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#multiple_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The second stage of research occurs in Introduction to Econometrics, usually taken in the junior year. This course convenes in our econometrics lab, which has 28 computers. Enrollments are capped at 22 students per section due to the heavy burden that draft editing imposes on the professor. The course covers standard econometric topics: estimation using OLS, hypothesis testing, serial correlation and time series, as well as introduction to more advanced topics such as panel data, instrumental variables, and logit/probit estimation. Data analysis is conducted with <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="fcc1c07d38-software-simple-0">STATA</rs>, which students begin using the second week of class, and the instruction tends to be applied rather than mathematical/algebraic. A required one-hour lab was added 25 years ago to provide extra instruction time to cover research methods. The course also receives extensive support from our library staff who have developed instruction modules and resource guides to assist students with their research.</p>
            <p>We are fortunate to have facilities that support student research. About three decades ago, the department received an NSF grant to build a computer lab, and the college continues to invest in this space with new hardware (28 Macs) and software (<rs cert="0.9" resp="#annotator20" subtype="used" type="software" xml:id="fcc1c07d38-software-simple-1">STATA</rs>). Our introductory and advanced econometrics courses benefit a great deal from the lab, and it is a hub for research activity throughout the year. The walls of the lab are covered with plaques that commemorate research-paper prizes our students have won, and we hope that they inspire the younger students.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="daf27686de">
            <titleStmt>
               <title>DOES AGGLOMERATION MATTER EVERYWHERE?: NEW FIRM LOCATION DECISIONS IN RURAL AND URBAN MARKETS*</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/jors.12202</idno>
                  <idno type="origin">10.1111%2Fjors.12202</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#multiple_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Establishment attributes such as location, industry and ownership are obtained from the National Establishment Time Series (NETS). <ref type="bibr">18</ref> The database provides information on the universe of all establishments that opened for business in Iowa and North Carolina in 2000-2002. The database also includes DUNS (Data Universal Numbering System) numbers of establishments and of their headquarters or parent companies. If an own DUNS number is the same as that of a headquarters or a parent company, the establishment is considered a "stand-alone start-up." 19 Otherwise, it is considered an "expansion start-up." Urban (rural) expansion start-ups are about 8 percent (6 percent) of urban (rural) start-ups in the sample period. We use three-digit and four-digit North American Industry Classification System (NAICS) codes used in 1997 Standard Use Table from the that our main results are robust to unobservable time-varying location-specific factors. Estimates and elasticities are available upon request. <ref type="bibr">16</ref> We used <rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="daf27686de-software-1">STATA</rs> command "multin" in a package named "groupcl" to estimate the grouped conditional logit. See <ref type="bibr">Guimaraes and Lindrooth (2007)</ref> for information on the estimator. (<rs corresp="#daf27686de-software-1" resp="#annotator4" type="url">http://www.stata.com/meeting/5nasug/NASUG_Guimaraes.pdf</rs>, accessed on June 19, 2014). <ref type="bibr">17</ref> The following industries are excluded: Agriculture (two-digit NAICS: 11), Mining <ref type="formula">(21)</ref>Bureau of Economic Analysis to define industries. The total number of industries is K = 112.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f62ce9f2d0">
            <titleStmt>
               <title>Syrian Consumers: Beliefs, Attitudes, and Behavioral Responses to Internet Advertising</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3846/btp.2013.31</idno>
                  <idno type="origin">10.3846%2Fbtp.2013.31</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The model testing hypotheses of the present study <ref type="figure">(Fig. 1)</ref> is evaluated through the statistical method of structural equa tion modeling (SEM) using <rs resp="#curator" type="software" xml:id="f62ce9f2d0-software-1">SpSSAMOS</rs> (v. <rs corresp="#f62ce9f2d0-software-1" resp="#annotator14" type="version">18</rs>) software.</p>
            <p>Bootstrapping is used as well to confirm the significance of mediations exhibited in the model regardless of normality of our data <ref type="bibr">(Byrne 2010)</ref>. We use the following statistics in testing the goodness of fit: Chi square (c²) <ref type="bibr">(Bollen 1989</ref>); normed fit index (NFI) <ref type="bibr">(Bentler, Bonett 1980)</ref>; comparative fit index (CFI) <ref type="bibr">(Bentler 1990</ref>); root mean square residual (RMR) <ref type="bibr">(Hu, Bentler 1995)</ref>; and root mean square error of approximation (RMSEA) <ref type="bibr">(Browne, Cudeck 1993)</ref>. Those statistics will help to test for how good the model is in fit ting the collected data. <ref type="table">Table 5</ref> shows the results of the proposed model test ing. The values of RMR (0.036 &lt; 0.05) <ref type="bibr">(Hu, Bentler 1995)</ref>, CFI (0.958 &gt; 0.9) <ref type="bibr">(Bentler 1990)</ref>, and NFI (0.942 &gt; 0.9) <ref type="bibr">(Bentler, Bonett 1980</ref>) indicate a good fit for the proposed model. However, RMSEA (0.88 &gt; 0.08) demonstrates a poor fit of the proposed model for our data <ref type="bibr">(Jöreskog, Sörbom 1989</ref>) -besides there are three insignificant paths: Social Role → Attitude (pvalue = 0.281 &gt; 0.05), Materialism → Attitude (pvalue = 0.218 &gt; 0.05), and Falsity → Attitude (pvalue = 0.176 &gt; 0.05). Therefore, our model needs to be revised, and then retested. <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="f62ce9f2d0-software-simple-2">AMOS</rs> suggests adding two new paths (Entertainment → Click on ad; Irritation → Leave website) as modification indices to improve the model fit ting for the data. Therefore, we eliminate the insignificant paths, draw the new suggested two paths, and retest the alternative mo del. <ref type="table">Table 6</ref> shows the results of testing the alternative model. All paths are significant. The goodness of fit statistics show that the alternative model is presenting agood fit for our data. In this regard, we notice that c² value is insignificant (c²/ df = 1.076, pvalue &gt; 5%), the normed fit index is higher than 0.9 (NFI = 0.982), the comparative fit index is higher than 90% (CFI = 0.999), the root mean square residual is less than 0.05 (RMR = 0.032), and the root mean square error of approximation is less than 0.08 (RMSEA = 0.020) <ref type="bibr">(Jöreskog, Sörbom 1989; Hu, Bentler 1995; Bentler 1992; MacCallum et al. 1996)</ref>. Consequently, we conclude that the alternative model expresses a good fit ting for our data. As well as, <ref type="table">Table 8</ref> shows that all indirect effects are significant. That is, we come to a decision that H1 and H2 are partially supported in the light of the following results (see <ref type="figure">Fig. 2</ref>).</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d88702e398">
            <titleStmt>
               <title>Ownership Type Influence on Dividend Payments in CEE Countries</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3846/btp.2013.27</idno>
                  <idno type="origin">10.3846%2Fbtp.2013.27</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>With the help of statistical software <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="d88702e398-software-0">SpSS</rs> <rs corresp="#d88702e398-software-0" resp="#annotator2" type="version">20.0</rs> the authors run of the regressions and test the possible violations of the regression assumptions. <ref type="table">Table 2</ref> shows the correlation matrix of the independent variables used in the regressions. High correlation (r = -0.771) between the strategic and local ow nership is observed as indicated in the table 2, which couldhave been supposed by the authors as the majority of strategic investors in CEE companies are foreign institutions. This high correlation might lead to the multicollinearity problem in the regression, thus, it would be worth to omit one of these variables in the final regression model. Running the regression models shows that the owners hip structure has higher impact on the dividend payout ratio than on the dividend yield: F test (Dp -dependent variable) is 38.19 vs. F test (DY -dependent variable) is 18.54. It should be noted that both of the regressions ap pear to be significant due to the high F test values, despite the fact that adjusted R in the case with the dividend yield regression model is 2.5% and in the case with the dividend payout regression model it is 9.6% <ref type="figure">(Tables 3, 4)</ref>.Dividend payout Ratio <ref type="table">(Table 3)</ref>: The regression results provide evidence that there is a significant positive relations hip between the dividend payout ratio and the strategic investors. Other variables do not seem to exert a significance influence on the amount of dividend payout.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="b379ec0909">
            <titleStmt>
               <title>Methodology for the Selection of Financial Indicators in the Area of Information and Communication Activities</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3846/btp.2013.11</idno>
                  <idno type="origin">10.3846%2Fbtp.2013.11</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Results and discussion of the paper are based on the analysis of secondary sources and selected data of questionnaire survey, which are involved on measuring the performance of Czech companies. To process the results of the questionnaire survey were used both of basic types of descriptive statistics and factor analyze on the selected data set. The data were processed by using the statistical program <rs corresp="#b379ec0909-software-0" resp="#curator" type="publisher">IBM</rs> <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="b379ec0909-software-0">SPSS Statistics</rs> <rs corresp="#b379ec0909-software-0" resp="#annotator14" type="version">20</rs>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f898c50983">
            <titleStmt>
               <title>Health Insurance and Children in Low- and Middle-income Countries: A Review</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/1475-4932.12331</idno>
                  <idno type="origin">10.1111%2F1475-4932.12331</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>1 Electronic databases. We conducted several database searches. We started with three databases where relevant quantitative impact evaluations were expected to be found: EconLit, the Impact Evaluation Repository of the International Initiave for Impact Evaluation (3ie), and PubMed. In addition, as a check, we performed a cross-database search using the<rs corresp="#f898c50983-software-0" resp="#curator" type="publisher">EBSCO</rs> Discovery Service (<rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="f898c50983-software-0">OneSearch</rs>) that covered the following databases: Medline, SocIndex, Science Direct, JSTOR journals, PsycInfo and SocWork Abstracts. 2 Citation chasing. The reference list of every included paper was checked to identify any possible additional studies. 3 Internet searches. Finally, we used the advanced search options in <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="f898c50983-software-simple-2">Google</rs> and <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="f898c50983-software-simple-3">Google Scholar</rs> for a search of the World Wide Web.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="be0482845f">
            <titleStmt>
               <title>The Ukraine conflict, economic–military power balances and economic sanctions</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1080/14631377.2016.1139301</idno>
                  <idno type="origin">10.1080%2F14631377.2016.1139301</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The Russian government has responded to sanctions by adopting numerous counter-measures (EWF 7). With respect to Ukraine, it imposed restrictions on exports ofagricultural and industrial goods to Russia, cut energy subsidies and demanded pre-payment of gas supplies. <ref type="bibr">19</ref> it banned imports of food from the countries imposing sanctions and made efforts to find alternative suppliers in non-participating nations. in March 2015 Russia prohibited Western iT firms based in Russia from bidding for state contracts (about 70% of their in-country revenue) if they are observing Western restrictions on doing business in Crimea. in November 2015 the Russian government required all state agencies to purchase Russian-sourced software (e.g. for spreadsheets) instead of foreign products (e.g. <rs cert="1.0" resp="#annotator3" type="software" xml:id="be0482845f-software-simple-0">Excel</rs>) from the start of 2016. in an unrelated action, but one relevant to this article, Russia imposed economic sanctions on Turkey (restrictions of charter flights for tourists, removal of visa-free travel, suspension of investment projects, a ban on hiring Turkish citizens) in late November 2015 in response to the Turkish air force using a US-supplied F-16 to shoot down a Russian bomber engaged in ground attacks against Turkomen insurgents in north Syria <ref type="bibr">(BBC, 2015)</ref>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="dfa8190e27">
            <titleStmt>
               <title>MIT Press Journals 2018 catalog</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1162/qjec.2005.120.1.273</idno>
                  <idno type="origin">10.1162%2Fqjec.2005.120.1.273</idno>
               </bibl>
            </sourceDesc>
            <biblStruct/>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The journal is hosted on an open-access, open-review, rapid publication platform called <rs cert="0.6" resp="#annotator14" type="software" xml:id="dfa8190e27-software-3">PubPub</rs>, created by students at the <rs corresp="#dfa8190e27-software-3" resp="#curator" type="publisher">Media Lab</rs>. <rs cert="0.6" resp="#curator" type="software" xml:id="dfa8190e27-software-simple-2">PubPub</rs> is a collaborative publication environment with rich commenting features, and powerful, intuitive authoring tools. JoDS articles are authored directly within the <rs cert="0.6" resp="#curator" type="software" xml:id="dfa8190e27-software-simple-3">PubPub</rs> environment, which provides support for multimedia, image integration, and large data sets.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="e39f016514">
            <titleStmt>
               <title>MIT Press Journals 2018 catalog</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1162/qjec.2006.121.1.289</idno>
                  <idno type="origin">10.1162%2Fqjec.2006.121.1.289</idno>
               </bibl>
            </sourceDesc>
            <biblStruct/>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The journal is hosted on an open-access, open-review, rapid publication platform called <rs cert="1.0" resp="#annotator8" subtype="used" type="software" xml:id="e39f016514-software-0">PubPub</rs>, created by students at the <rs corresp="#e39f016514-software-0" resp="#curator" type="publisher">Media Lab</rs>. <rs cert="1.0" resp="#curator" subtype="used" type="software" xml:id="e39f016514-software-simple-2">PubPub</rs> is a collaborative publication environment with rich commenting features, and powerful, intuitive authoring tools. JoDS articles are authored directly within the <rs cert="1.0" resp="#curator" subtype="used" type="software" xml:id="e39f016514-software-simple-3">PubPub</rs> environment, which provides support for multimedia, image integration, and large data sets.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d58afae3d3">
            <titleStmt>
               <title>SOME PRACTICAL GUIDANCE FOR THE IMPLEMENTATION OF PROPENSITY SCORE MATCHING</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/j.1467-6419.2007.00527.x</idno>
                  <idno type="origin">10.1111%2Fj.1467-6419.2007.00527.x</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#multiple_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The bulk of software tools to implement matching and estimate treatment effects is growing and allows researchers to choose the appropriate tool for their purposes. The most commonly used platform for these tools is <rs cert="0.9" resp="#annotator20" type="software" xml:id="d58afae3d3-software-simple-0">Stata</rs> and we will present the three most distributed ones here. <ref type="bibr">Becker and Ichino (2002)</ref> provide a programme for PSM estimators <ref type="bibr">(pscore, attnd, attnw, attr, atts, attk)</ref> which includes estimation routines for NN, kernel, radius, and stratification matching. To obtain standard errors the user can choose between bootstrapping and the variance approximation proposed by <ref type="bibr">Lechner (2001a)</ref>. Additionally the authors offer balancing tests (blocking, stratification) as discussed in Section 3.4. <ref type="bibr">Leuven and Sianesi (2003)</ref> provide the programme psmatch2 for implementing different kinds of matching estimators including covariate and propensity score matching. It includes NN and caliper matching (with and without replacement), KM, radius matching, LLM and Mahalanobis metric (covariate) matching. Furthermore, this programme includes routines for common support graphing (psgraph) and covariate imbalance testing (pstest). Standard errors are obtained using bootstrapping methods.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="bf257a9086">
            <titleStmt>
               <title>Imposing regional monotonicity on translog stochastic production frontiers with a simple three-step procedure</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s11123-009-0142-x</idno>
                  <idno type="origin">10.1007%2Fs11123-009-0142-x</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>All estimations and calculations have been done within the ''<rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="bf257a9086-software-8">R software environment for statistical computing and graphics</rs>'' (<rs corresp="#bf257a9086-software-8" resp="#curator" type="publisher">R Development Core Team</rs> 2009) using the ''R'' <ref type="bibr">(Coelli and Henningsen 2009), ''<rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="bf257a9086-software-simple-2">mic- Econ</rs>'' (Henningsen 2008)</ref>, and ''<rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="bf257a9086-software-simple-3">quadprog</rs>'' <ref type="bibr">(Turlach and Weingessel 2007)</ref>. The commands that have been used for this analysis are available in Appendix 3. The estimation results of the unrestricted stochastic frontier production function are presented in <ref type="table">Table 1</ref>. <ref type="bibr">10</ref> The b and d coefficients are defined as before, r 2 is the total error variance (r 2 u þ r 2 v ), and c is the proportion of the variance of technical inefficiency in the total error variance (r 2 u =r 2 ). The monotonicity condition is violated at 39 out of 344 observations and quasiconcavity is not fulfilled at four observations. While the education of the household head has no significant influence on technical efficiency, the proportion of ''bantog'' (upland) fields significantly (at the 10% level) increases the farm's efficiency.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="cc9c3445f9">
            <titleStmt>
               <title>Report of the Committee on the Status of Women in the Economics Profession 2011</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/aer.102.3.691</idno>
                  <idno type="origin">10.1257%2Faer.102.3.691</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Greene (NYU) and John Siegfried (Vanderbilt) contributing on two modules. The modules deal with heteroskedasticity; endogenous regressors with natural experiments, instrumental variables, and two-stage estimators; panel data; and sample selection issues. Each module features "real-world" datasets and sample programs written in <rs cert="1.0" resp="#annotator16" subtype="used" type="software" xml:id="cc9c3445f9-software-simple-0">Stata</rs>, <rs cert="1.0" resp="#annotator16" subtype="used" type="software" xml:id="cc9c3445f9-software-simple-1">SAS</rs>, and <rs cert="1.0" resp="#annotator16" subtype="used" type="software" xml:id="cc9c3445f9-software-simple-2">Limdep</rs>. Information on website "hits" is now available for the period corresponding to the 2011 "spring semester" at most US schools, from January-May 2011. The monthly average over this period was 220 hits/ views, with more hits in early and middle months of the "term." It appears (as supported by anecdotal accounts) that the modules and datasets are being used in many econometrics classes as general teaching tools, not just to introduce research topics in economic education.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="e35b62ad79">
            <titleStmt>
               <title>Distributional and revenue effects of a tax shift from labor to property</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s10797-018-9484-5</idno>
                  <idno type="origin">10.1007%2Fs10797-018-9484-5</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The matched dataset is then used to simulate a property tax reform that applies current market values instead of cadastral values. In a first scenario, we assess the potential revenue gain induced by the use of up-to-date property values. Next, we simulate a revenue-neutral scenario in which the additional revenue is used to lower social insurance contributions (SIC) via a lump sum SIC credit. As a third scenario, we simulate a proportional reduction of social insurance contributions, again under revenue neutrality. All simulations are carried out using <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="e35b62ad79-software-simple-0">EUROMOD</rs>-the tax-benefit microsimulation model for EU member states. It allows us to evaluate changes in households' disposable income induced by the different scenarios. Our baseline simulations focus on first-order effects of the three reform scenarios. In addition, we also discuss the relevance of second-order effects in our context, and provide robustness checks in the Appendix accounting for potential labor supply responses.</p>
            <p>In addition, our results speak to the literature analyzing the distributive effects of tax shifts from labor income toward other tax bases such as consumption (e.g., <ref type="bibr">Pestel and Sommer 2017)</ref>. So far, little empirical work has been dedicated to property tax related simulations, mostly driven by data limitations. A notable exception is <ref type="bibr">Moscarola et al. (2015)</ref>, assessing labor market reactions to a property and labor tax reform in Italy. In a similar vein, <ref type="bibr">Figari et al. (2017)</ref> investigate the fiscal and distributional consequences of including homeowners' imputed rent in personal taxable income as a kind of property tax for six European countries. Using up-to-date property values to determine property taxes could be regarded as an important complement (and maybe even as a substitute depending on the specific design) to housing income taxation. Finally, <ref type="bibr">Kuypers et al. (2017)</ref> currently create a <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="e35b62ad79-software-simple-1">EUROMOD</rs> input database directly from the HFCS dataset. Their approach aims at broadening the scope of <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="e35b62ad79-software-simple-2">EUROMOD</rs> by including information on wealth from HFCS, but they do not combine this with EU-SILC data. The novelty of our paper is the creation of a new dataset via statistical matching that allows analyses regarding two variables which have never been jointly observed, namely the current property tax liability and the actual market value of the property. Our approach may potentially be extended to other European countries covered by <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="e35b62ad79-software-simple-3">EUROMOD</rs>, providing a fruitful avenue for further research.</p>
            <p>Our policy reform simulations are performed on <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="e35b62ad79-software-5">EUROMOD</rs> (version <rs corresp="#e35b62ad79-software-5" resp="#annotator5" type="version">G2.0</rs>), the tax-benefit microsimulation model designed for EU member states. It applies national tax-benefit policy rules to harmonized microdata and calculates their effects on household disposable income <ref type="bibr">(Sutherland and Figari 2013)</ref>. Unlike computable general equilibrium (CGE) approaches, the only assumptions we impose concern our proposed reform scenarios, or the elasticity of labor supply. Our approach is in the spirit of recent research, for instance on fiscal sustainability <ref type="bibr">(Dolls et al. 2017)</ref>, income distribution analysis <ref type="bibr">(Bargain et al. 2015)</ref>, or mortgage interest deductibility <ref type="bibr">(Figari et al. 2017</ref>). Thus, we follow well-established simulation techniques using <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="e35b62ad79-software-simple-6">EUROMOD</rs>, allowing for inferences about the distributional and revenue effects of a tax shift from labor to property. The German component of <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="e35b62ad79-software-simple-7">EUROMOD</rs> reproducing the 2010 German tax-benefit system has been validated through comparison with aggregate statistics provided by fiscal authorities <ref type="bibr">(Ochmann and Granados 2011)</ref>. We run all tax-benefit policy rules at their 2010 setting and then augment the model with a simulated change in property and labor taxation. Hence, our simulation model calculates household disposable income under the current as well as the reformed tax-benefit rules holding everything else constant and, therefore, avoiding endogeneity problems <ref type="bibr">(Bourguignon and Spadaro 2006)</ref>.</p>
            <p>Acknowledgements Open access funding provided by Paris Lodron University of Salzburg. Parts of the paper were written during a research visit of Markus Tiefenbacher at the University of Essex and University of Antwerp. We would like to thank numerous seminar participants at Essex, Antwerp, Salzburg, WIFO Vienna and the IIPF Annual Congress 2016 for helpful comments and discussions. Financial support from the Humer Foundation and the Salzburg Centre for European Union Studies (SCEUS) is gratefully acknowledged. The research leading to these results has also received support from the European Commission's 7th Framework Programme (FP7/2013-2017) under Grant Agreement No. 312691 (InGRID-Inclusive Growth Research Infrastructure Diffusion). The results presented here are based on <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="e35b62ad79-software-13">EUROMOD</rs> version<rs corresp="#e35b62ad79-software-13" resp="#annotator5" type="version">G2.0</rs>. <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="e35b62ad79-software-simple-10">EUROMOD</rs> is maintained, developed and managed by the Institute for Social and Economic Research (ISER) at the University of Essex, in collaboration with national teams from the EU member states. We are indebted to the many people who have contributed to the development of <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="e35b62ad79-software-simple-11">EUROMOD</rs>. The process of extending and updating <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="e35b62ad79-software-simple-12">EUROMOD</rs> is financially supported by the European Union Programme for Employment and Social Innovation "Easi" (2014-2020). The results and their interpretation are the authors' responsibility. They make use of microdata for Germany from the EU Statistics on Incomes and Living Conditions (EU-SILC) and the Eurosystem Household Finance and Consumption Survey (HFCS).</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="e5f17ad2f0">
            <titleStmt>
               <title>China's Impact on Africa The Role of Trade, FDI and Aid</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.2139/ssrn.2426106</idno>
                  <idno type="origin">10.1111%2Fkykl.12110</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Using a large number of instruments may overfit endogenous variables and may weaken the Hansen J-test of the instruments' joint validity. To keep the number of instruments at a minimum, we use the collapse option in <rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="e5f17ad2f0-software-1">STATA</rs> in all regressions <ref type="bibr">(Roodman, 2009)</ref>. <ref type="bibr">19</ref> This ensures that the number of instruments is always well below the number of countries. Overall, the results, reported in <ref type="table">Table 2</ref>, are broadly in line with the fixed-effects results. The lagged dependent variable is always significant at the one percent level. Depending on the model specification, the estimated coefficient is slightly above or below one, implying no strong evidence for convergence in sub-Saharan African countries. This finding is in line with the results of McCoskey <ref type="formula">(2002)</ref>, who also found no convergence for economic growth in sub-Saharan Africa, although smaller "convergence clubs" do exist.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="e46e3c1bca">
            <titleStmt>
               <title>Estimation of bubble dynamics in the Chinese real estate market: a State space model</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s10368-017-0398-y</idno>
                  <idno type="origin">10.1007%2Fs10368-017-0398-y</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#multiple_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Generally, in practical applications the system matrices A and B together with the variances R and Q are unknown and have to be estimated. Whenever the explanatory variables are not observable Least Squares estimation is not the correct way to go. However, even in this case, one can apply likelihood based inference, since the Kalman filter allows to construct the likelihood function associated with a state space model. The SSM is therefore estimated by maximizing the estimated likelihood function through a numerical procedure. We apply this procedure by using the <rs cert="0.7" resp="#annotator2" subtype="used" type="software" xml:id="e46e3c1bca-software-simple-0">Sspace</rs> package in the <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="e46e3c1bca-software-simple-1">EViews</rs>® software.</p>
            <p>Compared to other approaches in the literature reviewed in section 3, the novel contribution of our methodology is that the deviation ratio measures a purely irrational bubble component. In other words, our bubble component does not include price changes that are due to non-equilibrium between supply and demand, where demand includes not only the users' market but also investors' demand. This feature is important because real estate markets can be characterized by a fundamental mismatch between supply and demand which can take time to correct and by a component of demand that is investment driven and that is typically not decreasing in price. 1 A fair account of the bubble component should therefore exclude those components. Moreover, in our analysis we do not consider rents on purpose, because those are codetermined in the economy with the observable market prices, whereas we want to have a measure of the fundamental unobservable real estate price that depends as much as possible on fundamental macro-economic factors.As it is often the case with this type of macro variables, evaluating the data with Augment Dickey-Fuller test reveals that these variables are non-stationary in levels, but become stationary in first differences. We also adjust for seasonality with the <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="e46e3c1bca-software-simple-2">Eviews</rs> Census X12 method and take the log of all variables to correct for heteroscedasticity. SS is our measure for real estate demand and FS is our measure for real estate supply. We carry out Johansen tests to select co-integrated variables for demand and supply. After that we estimate the two functions by a SSM.</p>
            <p>The residuals of the above five equations are assumed to have independent and identical distributions. Equations <ref type="formula">(3)</ref> and <ref type="formula">(4)</ref> form a SSM are estimated by Kalman filter using the <rs cert="0.7" resp="#curator" subtype="used" type="software" xml:id="e46e3c1bca-software-simple-3">Sspace</rs> analysis tools in <rs cert="0.7" resp="#annotator2" subtype="used" type="software" xml:id="e46e3c1bca-software-0">Eviews</rs> <rs corresp="#e46e3c1bca-software-0" resp="#annotator2" type="version">7.0</rs>®. The estimated coefficients represent the long term and stable elasticity of sold real estate space with respect to each variables. Results are reported in <ref type="table">Table 2</ref> below.</p>
            <p>In this case, eqs. (5) and (6) form a SSM are estimated by Kalman filter using the <rs cert="0.7" resp="#curator" subtype="used" type="software" xml:id="e46e3c1bca-software-simple-6">Sspace</rs> analysis tools in <rs cert="0.7" resp="#annotator2" subtype="used" type="software" xml:id="e46e3c1bca-software-1">Eviews</rs> <rs corresp="#e46e3c1bca-software-1" resp="#annotator2" type="version">7.0</rs>®. The estimated coefficients represent the long term and stable elasticity of completed real estate space with respect to each variable. Results are reported in <ref type="table">Table 4</ref> below.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c3a8c052df">
            <titleStmt>
               <title>Identifying and Spurring High-Growth Entrepreneurship: Experimental Evidence from a Business Plan Competition</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/aer.20151404</idno>
                  <idno type="origin">10.1257%2Faer.20151404</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The randomization procedure was designed as follows. First, among the semifinalists, all those with business plan total scores below 30 were dropped, to maintain a minimum standard. This reduced the pool from 1,920 to 1,841 firms. Then a two-step stratified randomization was conducted by the author in <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="c3a8c052df-software-simple-0">STATA</rs> to choose the ordinary winners:</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="a918413cb3">
            <titleStmt>
               <title>POLONIA dynamics during the years 2006–2012 and the effectiveness of the monetary Policy of the National Bank of Poland</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s10663-015-9287-1</idno>
                  <idno type="origin">10.1007%2Fs10663-015-9287-1</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <ref type="bibr">Beirne (2011)</ref>
            <p>For further details we refer the Reader to <ref type="bibr">Meyer and Yu (2000)</ref>. Estimation of SV model was performed using <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="a918413cb3-software-simple-0">OpenBugs</rs> program of <ref type="bibr">Meyer and Yu (2000)</ref>. First 10 000 iterations were treated as the ''burn-out'' period, while next 10,000-as the sample from the posterior distribution. Estimation of conditional mean was performed in <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="a918413cb3-software-2">OxMetrics</rs> <rs corresp="#a918413cb3-software-2" resp="#annotator2" type="version">6.1</rs> program.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d1915bd71a">
            <titleStmt>
               <title>Hayek, Local Information, and Commanding Heights: Decentralizing State-Owned Enterprises in China</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/aer.20150592</idno>
                  <idno type="origin">10.1257%2Faer.20150592</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Indeed, most governments of the world have engaged in controlling these commanding heights since the end of World War II <ref type="bibr">(Yergin and Stanislaw 1998)</ref>. <ref type="bibr">3</ref> Strong state intervention in western economies largely started with the UK Attlee Labour 2 As of January 27, 2017, <ref type="bibr">Hayek (1945)</ref> has been cited about 14,000 times in <rs resp="#curator" type="software" xml:id="d1915bd71a-software-simple-0">Google Scholar</rs>, and it is viewed by a panel of top economists as one of the top 20 articles published in the AER in its first 100 years' history <ref type="bibr">(Arrow et al. 2011)</ref>. <ref type="bibr">3</ref> Most historical material on commanding heights across countries in this paper comes from <ref type="bibr">Yergin and Stanislaw (1998)</ref>. government in 1945, which promoted government planning and nationalization of industries. The General de Gaulle government in France followed suit, declaring that the state "must hold the levers of command." Similarly, after the independence of India, Prime Minister Nehru often evoked the commanding-heights metaphor. Typically, these countries, as well as Germany, Korea, Japan, many Latin American countries, and of course, all socialist countries, had strong state control/ownership over strategic sectors such as defense, iron and steel, railroads, ship-building, utilities, and telecom. In these sectors, the key consideration for decentralization is likely not to utilize local information but to ensure the benefits of "strategic control." Here the force of distance in decentralization should naturally be more muted.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c842004558">
            <titleStmt>
               <title>An examination of trust as a strategical factor of success in logistical firms</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3846/btp.2017.018</idno>
                  <idno type="origin">10.3846%2Fbtp.2017.018</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>GfK Hungary Market Research Institute contributed significantly to the structure of the questionnaire, we created the professional content, and the possible response forms and types were greatly influenced by the data quality and type that can be managed and expected by the evaluation software (<rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="c842004558-software-simple-0">SPSS</rs>). Questionnaires were completed using the Computer Assisted Web Interviewing (CAWI) method. The internet-based questionnaire technique provided an effective research background for this target group by allowing respondents to answer questions on delicate corporate issues (financial issues, role of suppliers, etc.) more honestly, as the interviewee's response was not affected by the presence of the interviewer.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d3ec996242">
            <titleStmt>
               <title>The Impact of Age, Education and Seniority on Motivation of Employees</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3846/btp.2015.433</idno>
                  <idno type="origin">10.3846%2Fbtp.2015.433</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Determination of the motivation level and the analysis of motivation factors in the enterprises in a perticular time were carried out through a questionnaire which consists of 30 closed questions <ref type="bibr">(Hitka 2009: 149)</ref>. The questionnaire was divided into two parts. Sociodemographic and quali fication characteristics of employees were searched in the first part. Basic data about respondents relating to their age, sex, seniority, completed education and job position were obtained in this part. The second part consisted of individual motivation factors through which information about work environment, working conditions, applied appraisal and reward system, about personnel management, health and social care system and system of employee benefits as well as information about employee satisfaction or dissatisfac tion, value orientation, relation to work and enterprise or coworkers' relationship in the enterprise can be found out. Motivation factors are in alphabetical order not to affect respondents's decision. In the questionnaire respondents evaluated individual motivation factors by one of the five levels of importance from a predefined 5point rating scale, 5 -the most important and 1 -unimportant <ref type="table">(Table 1</ref>) from a predefined evaluation scale <ref type="table">(Table 1)</ref>.Each motivation factor was marked by employees with one of five types of importance for required as well as for cur rent conditions. The required condition can be defined as an idea of employees how the motivation should look like, i.e. what would motivate them to increase their performance. The current conditions represents the employees' opinion how they are satisfied with current motivation in the enterprise. The questionnaires were evaluated using the programme<rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d3ec996242-software-1">STATISTICA</rs> <rs corresp="#d3ec996242-software-1" resp="#annotator14" type="version">7</rs> <ref type="bibr">(Statsoft 2004)</ref>. Descriptive statistics was used to describe the primary sampling unit. Statistical character istics, which compressed information about studied primary sampling units into smaller number of numerical character istics and made mutual comparison of sampling units easier, were computed for each motivation factor. Each motivation factor was described in summary by basic characteristics of size and variability of quantitative features -average x  , stand ard deviations s x and coefficients of variation. Subsequently the results of the enterprises were compared.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d1c8fbe5db">
            <titleStmt>
               <title>Exiting a Lawless State*</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/j.1468-0297.2008.02177.x</idno>
                  <idno type="origin">10.1111%2Fj.1468-0297.2008.02177.x</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#multiple_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>(a) a transition probability equal to the squared demand for the rule of law:Figure 5(a) shows that the stable equilibria of x* are 0 and 0.75. In the first case, no one opposes the establishment of the rule of law, and reform occurs in period 1. In the second case, three-fourths of the agents oppose the rule of law, and reform is delayed on average for 16 periods. <ref type="bibr">24</ref> <ref type="figure">Figure 5</ref>(b) depicts the growth paths of agentsÕ expected aggregate lifetime income in these two cases. <ref type="bibr">25</ref> Expected aggregate income is 20% lower along the path of delayed reform. Every agent is strictly worse off. <ref type="bibr">20</ref> As emphasised by <ref type="bibr">Greif (1994)</ref>, culture is an equilibrium selection device and so it is interesting to consider within this model the role that culture might have played in Russia. Two facts suggest that the ÔgoodÕ corner equilibrium, with x Ã ¼ 0, would not be a focal point: (a) managers commonly engaged in asset stripping well before the mass privatisation of 1992-4 <ref type="bibr">(Grigoriev, 1992)</ref> and (b) most beneficiaries of the mass privatisation of large state enterprises in Russia were the managers. For instance, <ref type="bibr">Varese (2001, Appendix B)</ref>, finds in his survey of one Russian city in 1993 that 51% of the 92 full-time officials of the Communist Party in 1988 were top managers of economic enterprises. <ref type="bibr">21</ref> We discuss other comparative statics results in <ref type="bibr">Hoff and Stiglitz (2004a)</ref>. <ref type="bibr">22</ref> Or, more accurately, if the intersection of h Ã (x; k) and the stripping ability curve lies along h a , not h p . 23 f ¼ 0.05, I L ¼ 0.01, I N ¼ 0.0475,g ¼ 1:05,z ¼ 0:9, k ¼ 0.3, and d ¼ 0.945. Using (4), these values imply h max ¼ 0.8. The <rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="d1c8fbe5db-software-0">EXCEL</rs> program is available at <rs corresp="#d1c8fbe5db-software-0" resp="#annotator4" type="url">http://www.econ.worldbank.org/staff/khoff</rs>. <ref type="bibr">24</ref> One can show by standard methods that given a transition probability p per period, the expected number of periods before reform is 1/p. If x Ã ¼ 0.75, then p(x Ã ) ¼ 0.0625 and the expected delay is 16 periods.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="b3b45b86ad">
            <titleStmt>
               <title>INFORMAL LABOR MARKETS AND ON-THE-JOB TRAINING: EVIDENCE FROM WAGE DATA</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/ecin.12279</idno>
                  <idno type="origin">10.1111%2Fecin.12279</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>20. The p value associated with the hypothesis that the empirical distribution function of Δ 1 w in the formal sector is "smaller" is 0.000, whereas the p value associated with the hypothesis that the empirical distribution function of Δ 1 w in the informal sector is "smaller" is 0.000, indicating no ranking between these two distributions in terms of first order stochastic dominance.However, the distribution of wage growth in the informal sector appears to be slightly more disperse than in the formal sector, suggesting that wages are more flexible in the informal sector, both for wage increases and wage decreases. A traditional variance ratio test rejects the hypothesis that the variance of Δ 1 w in the formal sector is equal to the variance of Δ 1 w in the informal sector in favor of an alternative that the variance of Δ 1 w in the formal sector is smaller. 21 However, this test is sensitive to the assumption that the data are drawn from a Normal distribution. Using a test for the equality of variances that is robust when dealing with skewed distributions <ref type="bibr">(Brown and Forsythe 1974; Levene 1960</ref>) also rejects the null hypothesis that the variances are equal. <ref type="bibr">22</ref> Next, I explore how wage growth in the informal sector compares to wage growth in the formal 21. For the variance ratio test, I used the <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="b3b45b86ad-software-simple-0">Stata</rs> command sdtest. The null hypothesis of this test is that the ratio r = σ 2 FS ∕σ 2 IS equals 1, i.e. H 0 : r = 1. The calculated test statistic is f = 0.6605, which is distributed as F <ref type="bibr">(32172,27221)</ref>, where the degrees of freedom in the numerator and denominator are (n FS − 1) and (n IS − 1), respectively. This null is rejected against the alternative H 1 : r &lt; 1 with a p value of 0.0000, and against the alternative H 1 : r ≠ 1 with a p value of 0.0000, but cannot be rejected against the alternative H 1 : r &gt; 1 with a p value of 1.0000.</p>
            <p>22. To test the hypothesis of equal variances, I used the<rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="b3b45b86ad-software-simple-1">Stata</rs> command robvar which provides three alternative test statistics that vary the measure of central tendency used to calculate the test. The first one uses the mean of each group under comparison, the second uses the median of each group, and the third uses the 10% trimmed mean of each group. The first variant of the test is the one proposed by <ref type="bibr">Levene (1960)</ref>, the other two are the ones proposed by <ref type="bibr">Brown and</ref> sector conditional on individual and firm characteristics and for different time spans. To this end, I estimate the following wage-growth regressions:</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="a5cbba426a">
            <titleStmt>
               <title>Minimality of State Space Solutions of DSGE Models and Existence Conditions for Their VAR Representation</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s10614-014-9465-4</idno>
                  <idno type="origin">10.1007%2Fs10614-014-9465-4</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#multiple_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>A <rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="a5cbba426a-software-simple-0">MATLAB</rs> script that implements this check is available in the Additional Material to the present paper.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="a9b5a62ee5">
            <titleStmt>
               <title>The Case Against Patents</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/jep.27.1.3</idno>
                  <idno type="origin">10.1257%2Fjep.27.1.3</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#multiple_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>To understand more about the actual effect of patents in the real world, consider the recent purchase by Google of Motorola Mobility, primarily for its patent portfolio-not for the ideas and innovations in that portfolio. Few if any changes or improvements to <rs corresp="#a9b5a62ee5-software-0" resp="#curator" type="publisher">Google</rs>'s <rs cert="1.0" resp="#annotator3" type="software" xml:id="a9b5a62ee5-software-0">Android operating system</rs> will result from the ownership or study of these software patents. Google's purpose in obtaining this patent portfolio is purely defensive: it can be used to countersue Apple and  Microsoft and blunt their legal attack on Google. These remarks apply to the vast bulk of patents: they do not represent useful innovation at all and are just weapons in an arms race. This is not news: the same message emerged decades ago from the <ref type="bibr">Levin, Klevorick, Nelson, and Winter (1987)</ref> and <ref type="bibr">Cohen, Nelson, and Walsh (2000)</ref> surveys of research and development managers. surveys of research and development managers.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c653ca98ae">
            <titleStmt>
               <title>Information aggregation in Arrow–Debreu markets: an experiment</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s10683-017-9548-x</idno>
                  <idno type="origin">10.1007%2Fs10683-017-9548-x</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>We conducted 7 experimental sessions involving 12 subjects each. Participants were recruited from the subject pool of the experimental economics laboratory at Ben-Gurion University of the Negev using the recruitment software <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="c653ca98ae-software-2">ORSEE</rs> <ref type="bibr">(Greiner 2015)</ref>. All sessions were programmed with the <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="c653ca98ae-software-3">z-Tree</rs> <ref type="bibr">(Fischbacher 2007)</ref> software. Subjects in each session were first given 45 min to familiarise themselves with the instructions, complete an accompanying set of control questions and participate in two non-paying practice rounds (see the ''supplementary Appendix'' for the translated instructions). The experiment itself included 10 rounds. All payoffs were stated in ECU (Experimental Currency Unit), and converted to New Israeli Shekels (NIS) at the end of the experiment at a conversion rate of 15 ECU = 1 NIS. The final payoff consisted of the payoff of one randomly chosen round in addition to a show-up fee of 25 NIS. The average payment including show-up fee was 73.33 NIS (approximately 21 USD). Average session duration was approximately 120 min.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="e7a439420e">
            <titleStmt>
               <title>The impact of mechanized processing of cassava on farmers’ production efficiency in Uganda</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1080/13504851.2016.1167817</idno>
                  <idno type="origin">10.1080%2F13504851.2016.1167817</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#multiple_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>where υ ij represents the technical efficiency of the ith farmer; Z 1 denotes years of farming experience of the ith farmers; Z 2 denotes years of formal education; Z 3 is a dummy variable scored 1 if farmer belongs to farmers' association and zero otherwise; Z 4 is a dummy variable scored 1 if farmer uses improved cassava cutting and zero otherwise; Z 5 is a dummy variable scored 1 if farmer has access to markets and zero otherwise; Z 6 is a dummy variable scored 1 if farmer uses mechanized cassava-processing technology and zero otherwise; Z 7 is dummy variable score 1 if farmer has access to credit and zero otherwise, Z 8 is a dummy variable scored 1 if farmer has off-farm income and zero otherwise; Z 9 is a dummy variable scored 1 if farmer sells cassava to processors and zero otherwise and Z 10 is a dummy variable scored 1 if farmer plants cassava as a sole crop and zero otherwise. The maximum likelihood of the estimates of β and δ coefficients in Equations <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref>, respectively, are estimated simultaneously using the computer program <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="e7a439420e-software-2">FRONTIER</rs> <rs corresp="#e7a439420e-software-2" resp="#annotator2" type="version">4.1c</rs> in which the variance parameters are expressed in terms of σ <ref type="bibr">Coelli 1996)</ref>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="a6fcc60693">
            <titleStmt>
               <title>The impact of public smoking bans on well-being externalities: Evidence from a policy experiment</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/sjpe.12150</idno>
                  <idno type="origin">10.1111%2Fsjpe.12150</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#multiple_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Identification of the average treatment effect on the treated through DD models relies on the parallel trend assumption so that values of our outcome of interest, well-being defined via the GHQ, should follow similar pre-treatment time trends in both Scotland and England. To examine whether this assumption holds, we display GHQ trends by country of residence, gender, marital status and the presence of dependent children in <ref type="figure">Figure 1</ref>. For the overall population (which includes both married and single individuals with and without children, upper part of <ref type="figure">Figure 1</ref>), GHQ trends appear to be broadly similar in Scotland and England before the introduction of the bans. More specifically, while for Scottish and English women GHQ trends appear to slightly converge between waves 14 and 15 (i.e. 1 year before the introduction of the ban in Scotland, although this change amounts to less than a half GHQ point), pre-Scottish ban trends appear very similar for male individuals. <ref type="bibr">15</ref> Both graphs show that in the year when the Scottish ban was introduced (between waves 15 and 16), GHQ levels in Scotland appear to decrease (implying a small increase in SWB), especially among women. Still for the overall population sample of men, SWB also appears to somewhat increase in England after the smoking ban. Graphs for married individuals of both genders (second row of <ref type="figure">Figure 1</ref>) appear to display GHQ trends comparable to the ones of the overall population. Single women in Scotland and England (third row of <ref type="figure">Figure 1)</ref> show virtually identical self-reported GHQ trends, also during the introduction of the two bans with slight increases in SWB between waves 15 and 16 (Scottish ban) followed by decreases in SWB between waves 16 and 17 (English ban). However, SWB reported by single men seem to vary during the pre-Scottish ban period, although only between GHQ scores of 10 and 11. Men and women with children (fourth row of <ref type="figure">Figure 1</ref>) exhibit relatively stable differences in GHQ levels between England and Scotland before the Scottish ban with increases in SWB during the Scottish ban and simultaneous slight decreases in England. However, the very stable GHQ levels <ref type="bibr">14</ref> In this case, we use England in wave 9 as a baseline country-specific time trend as no public smoking ban was in place at that time. Treatment effects are computed using differences between estimated interaction terms, i.e. interactions between country of residence and time dummies, before and after the Scottish ban in England and Scotland. More specifically, the corresponding treatment effect reported in each table is the one obtained by the following double difference: (Scotland*wave 16 -England*wave 16) -(Scotland*wave 15 -England*wave 15) where waves 15 and 16 are the pre-and post-Scottish ban waves in the BHPS respectively. Standard errors for these treatment effects are obtained using the lincom command in <rs cert="1.0" resp="#annotator31" subtype="used" type="software" xml:id="a6fcc60693-software-simple-0">Stata</rs>. <ref type="bibr">15</ref> The presence of anticipation effects on well-being is explored using placebo tests in <ref type="table">Table 7</ref>.among men and women without children (last row) do not appear to be affected by the introduction of the smoking bans. Overall, we observe mostly stable pre-treatment trends and increases in SWB in the year of the implementation of the Scottish ban, especially among women. Yet, specific sub-groups display some, although limited, variation in pre-treatment GHQ trends (e.g. single men) while others present no apparent changes in GHQ levels in the presence of the bans (e.g. men with no children).</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="aaf4d6db28">
            <titleStmt>
               <title>Comparative advantage and strategic specialization</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/roie.12300</idno>
                  <idno type="origin">10.1111%2Froie.12300</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>10 (It can be derived from Equation (B3) in Appendix B.) From Equation <ref type="formula">(10)</ref>, we know that an increase in t A increases the relative domestic price of the imported good. This is the Metzler paradox, in which the imposition of a tariff on imports decreases the relative internal price of that good. Under the paradoxical price changes, an increase in tariff will necessarily increase real purchasing power for domestic consumers. 11 The <rs cert="0.5" resp="#annotator2" subtype="used" type="software" xml:id="aaf4d6db28-software-simple-0">Matlab</rs> code is downloadable in mwkang.site11.com/code/rie2016. <ref type="bibr">12</ref> The Ricardian model introduced in this paper has a combination of effects that include expansion and contraction, so it is not obvious whether the condition oEX(a,a 0 )/oa &lt; 0 clearly represents the immiserizing growth effect. To clarify this issue, it is worth presenting a simple model of endowment economy. With the same CES utility functions as in Equation 18, country A's endowment for good (x, y) is ffiffi ffi k p a; 0 À Á and country B's endowment is 0; ffiffi ffi k p a 0 À Á . In this endowment economy, there exists closed form solutions for the equilibrium allocations, and the condition for country A's welfare to decrease as a increases is that E &lt; 0.5 for any value of a 0 &gt; 0, a ! a 0 , and k &gt; 0, which is the same as the condition in Equation <ref type="formula">(21)</ref>.</p>
            <p>From a(b) and b(a), we can derive the Nash equilibrium specialization (a*,b*) which satisfies a (b*) 5 a* and b(a*) 5 b*. By the symmetric structure of the model, we always have a* 5 b. From the derivations of t A ðt B ; a; bÞ and t B ðt A ; a; bÞ, I apply a numerical analysis using <rs cert="0.5" resp="#annotator2" subtype="used" type="software" xml:id="aaf4d6db28-software-2">Matlab</rs>. The Matlab code can be downloaded in <rs corresp="#aaf4d6db28-software-2" resp="#annotator2" type="url">mwkang.site11.com/code/rie2016</rs>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="bc7a939792">
            <titleStmt>
               <title>Structuring the Smartphone Industry: Is the Mobile Internet OS Platform the Key?</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s10842-011-0105-6</idno>
                  <idno type="origin">10.1007%2Fs10842-011-0105-6</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>In the early platform literature, the platform was seen as a base architecture for product development with standard components that could be built upon <ref type="bibr">(Cusumano and Suarez 2009</ref>). More recent research, especially on today's complex ICT industries, has argued that a platform is not solely a technology, but also the outcome of a set of business behaviors and relationships between actors in an ecosystem. This ecosystem for modern hightechnology platforms is characterized as having high levels of interdependence between actors, as well as high potential for innovation by each actor <ref type="bibr">(Gawer and Cusumano 2002; Gawer and Henderson 2007; Gawer 2009</ref>). As a result, even those firms with clear market dominance in one area-e.g., Nokia with handsets-are dependent on the innovation of complementary firms to maintain their leadership position. Hardware firms and software firms rely on each other to push technology forward. Even Apple and its relatively closed <rs cert="1.0" resp="#annotator27" type="software" xml:id="bc7a939792-software-simple-0">iOS</rs> depends upon thousands of application developers to continue to create desirable apps for end-users; those developers that create iPhone apps have bet on the <rs cert="1.0" resp="#curator" type="software" xml:id="bc7a939792-software-simple-1">iOS</rs> platform's success and must rely on Apple to maintain and update their access to its operating system for ongoing development. In this system the interdependence is not limited to the transactional, supply-chain flow of typical goods and services, but is also based on the strategic exchange and integration of innovation among primary firms and their complementors to advance the platform (Tee and Gawer 2009).</p>
            <p>The different business models and strategies for value capture in the smartphone industry reflect the diverse backgrounds and historical core competencies of the competing firms. The incumbents, such as Nokia and Palm/HP, hail from the mobile phone and personal digital assistant (PDA) industries, while newer entrants such as Apple and Google come from the personal computing and Internet worlds. Nokia, the longtime market leader, still dominates the industry in terms of handset sales, and has been the primary supporter of the correspondingly dominant <rs cert="1.0" resp="#annotator27" type="software" xml:id="bc7a939792-software-simple-2">Symbian</rs> operating system. However, Nokia is quickly losing smartphone market share to new competitors, especially Apple and Google, and the current state of the market (see <ref type="table">Table 2</ref>) can be seen as a battle between an entrenched global incumbent and newcomers to the mobile space.</p>
            <p>Relying on its vertically integrated manufacturing capabilities and strong relationships with network carriers around the world, Nokia rode the wave of mobile phone adoption to become the dominant mobile handset producer. However, despite strong sales of lower-end handsets in developing countries, its share of the global market has steadily declined from a peak of approximately 60% of all handsets to 30% in 2010. By the end of 2007, <rs cert="1.0" resp="#curator" type="software" xml:id="bc7a939792-software-simple-3">Symbian</rs> was the OS for approximately 65% of all smartphones, compared to 12% for <rs cert="0.9" resp="#annotator27" type="software" xml:id="bc7a939792-software-simple-4">Windows</rs> and 6.5% for the iPhone. Nokia alone held about 53% of the global smartphone market <ref type="bibr">(West and O'Mahony 2008)</ref>. By the end of 2010, however, only 37% of all smartphones were running <rs cert="1.0" resp="#curator" type="software" xml:id="bc7a939792-software-simple-5">Symbian</rs>, compared to 26% for <rs resp="#curator" type="software" xml:id="bc7a939792-software-simple-6">Android</rs> and 17% for <rs cert="1.0" resp="#curator" type="software" xml:id="bc7a939792-software-simple-7">iOS</rs> (Gartner 2010).</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f6e5ed2ab3">
            <titleStmt>
               <title>MIT Press Journals 2018 catalog</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1162/qjec.2005.120.1.131</idno>
                  <idno type="origin">10.1162%2Fqjec.2005.120.1.131</idno>
               </bibl>
            </sourceDesc>
            <biblStruct/>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The journal is hosted on an open-access, open-review, rapid publication platform called <rs cert="0.8" resp="#annotator14" type="software" xml:id="f6e5ed2ab3-software-0">PubPub</rs>, created by students at the <rs corresp="#f6e5ed2ab3-software-0" resp="#curator" type="publisher">Media Lab</rs>. <rs cert="0.8" resp="#annotator14" type="software" xml:id="f6e5ed2ab3-software-simple-2">PubPub</rs> is a collaborative publication environment with rich commenting features, and powerful, intuitive authoring tools. JoDS articles are authored directly within the <rs cert="0.8" resp="#annotator14" type="software" xml:id="f6e5ed2ab3-software-simple-3">PubPub</rs> environment, which provides support for multimedia, image integration, and large data sets.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="b7a7f7fb9d">
            <titleStmt>
               <title>CAN RBC MODELS EXPLAIN BUSINESS CYCLES IN KOREA?</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1017/s1365100515000619</idno>
                  <idno type="origin">10.1017%2Fs1365100515000619</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>For the estimation and simulation, I do a Bayesian estimation. I execute the Bayesian estimation in <rs cert="1.0" resp="#annotator26" subtype="used" type="software" xml:id="b7a7f7fb9d-software-0">Dynare</rs> version <rs corresp="#b7a7f7fb9d-software-0" resp="#annotator26" type="version">4.2.2</rs>. Readers who are interested in the details of the estimation are referred to <ref type="bibr">Griffoli's (2007-2008)</ref> <rs cert="1.0" resp="#annotator26" subtype="used" type="software" xml:id="b7a7f7fb9d-software-simple-2">Dynare</rs> User Guide.</p>
            <p>One thing that I should note here is that this version of <rs cert="1.0" resp="#annotator26" subtype="used" type="software" xml:id="b7a7f7fb9d-software-simple-3">Dynare</rs> does not allow the higher-order approximation of the model for the estimation. However, it is well known that log linearization is good enough if the main focus of the research is not on welfare analysis or investigation of the financial market but on examining the business cycle. <ref type="table">Table 2</ref> reports the parameter values, both calibrated and estimated from the Bayesian estimation. The model data are simulated 300,000 times. The measurement equations for Y t , C t , I t , and TBY t are given by</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="dd40e87f82">
            <titleStmt>
               <title>Risk attitudes in a social context</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s11166-011-9127-z</idno>
                  <idno type="origin">10.1007%2Fs11166-011-9127-z</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>All experimental sessions were conducted at the Behavioral and Experimental Economics laboratory Maastricht (BEELab) with 121 (65 male, 56 female) students from Maastricht University. The vast majority (88%) were students from Economics and Business Administration. The other subjects were students from Law, Cultural Sciences, and from University College Maastricht. They were recruited through email announcements and announcements on the students' intranet. Each subject participated in only one session. The experiment was programmed in <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="dd40e87f82-software-1">Z-tree</rs> <ref type="bibr">(Fischbacher 2007)</ref>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="e645b31c2b">
            <titleStmt>
               <title>Involving Undergraduates in Research To Encourage Them To Undertake Ph.D. Study in Economics</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/000282805774669772</idno>
                  <idno type="origin">10.1257%2F000282805774669772</idno>
               </bibl>
            </sourceDesc>
            <biblStruct/>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>One strategy that I pursue is to recruit undergraduate research assistants prior to their arrival at Cornell. I read through the folders of the students who have accepted offers of admissions to my college (approximately 160) and search for students with strong mathematics, statistics, or economics backgrounds. This usually means students who have taken calculus and/or either advanced-placement economics or advanced-placement statistics in high school. <ref type="bibr">4</ref> I write several of these students prior to their arriving at Cornell and offer them positions as research assistants at CHERI. Given the large fraction of our students who must take out loans or work as part of their financial-aid packages, the take-up rate on these offers is high. <ref type="bibr">5</ref> The current generation of entering first-year students is so computer-literate that they rapidly learn how to use spreadsheet and statistical software programs, such as <rs cert="1.0" resp="#annotator14" type="software" xml:id="e645b31c2b-software-simple-0">EXCEL</rs>, <rs cert="1.0" resp="#annotator14" type="software" xml:id="e645b31c2b-software-simple-1">Stat/Transfer</rs>, and <rs cert="1.0" resp="#annotator14" type="software" xml:id="e645b31c2b-software-simple-2">STATA</rs>. I heuristically explain the econometric models we are using to them, and even if they have not had classes in statistics, they quickly understand the research that I am conducting. Within a short time, we are working together on empirical research projects. These students also mention our work to their friends, and I often link up with other bright students through these referrals.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="e128635838">
            <titleStmt>
               <title>Efficiency, Equality, and Labeling: An Experimental Investigation of Focal Points in Explicit Bargaining</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/aer.104.10.3256</idno>
                  <idno type="origin">10.1257%2Faer.104.10.3256</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The main treatment was run in June 2010 (for Set I) and November 2012 (for Set II) at the CBESS Experimental Laboratory at the University of East Anglia. Participants were recruited from the general student population using the <rs cert="1.0" resp="#annotator31" subtype="used" type="software" xml:id="e128635838-software-simple-0">ORSEE</rs> system <ref type="bibr">(Greiner 2004)</ref>, excluding individuals who participated in previous bargaining table experiments. There were 144 subjects in the Set I sessions and 168 in the Set II sessions; no one participated in both. The bargaining protocol was implemented using <rs cert="1.0" resp="#annotator31" subtype="used" type="software" xml:id="e128635838-software-simple-1">z-Tree</rs> <ref type="bibr">(Fischbacher 2007)</ref>. Subjects' understanding of this protocol and other key aspects of the experiment was checked using a computerized questionnaire (which can be found in the online Appendix). Sessions lasted between 60 and 80 minutes; earnings ranged between £5 and £25 with an average of £15.12, including a £5 show-up fee.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f4dd8eed51">
            <titleStmt>
               <title>The importance of family background and neighborhood effects as determinants of crime</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s00148-015-0566-8</idno>
                  <idno type="origin">10.1007%2Fs00148-015-0566-8</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>where we only observe C if = I (C * if &gt; 0). We estimate Eq. 6 using <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="f4dd8eed51-software-simple-0">STATA</rs>'s xtlogit command. The variance components σ 2 e and σ 2 a are estimated using maximum likelihood. The random family effect is estimated conditional on the random individual effect being logistically distributed with mean zero, variance σ 2 e = π 2 /3 and independent of a f . Inspired by the approach used in <ref type="bibr">Mazumder (2008)</ref> and <ref type="bibr">Björklund et al. (2010)</ref>, we then go on to include potentially important family-wide variables, either one at a time or simultaneously, in the x if matrix. For example, consider the inclusion of parental income and education in x if . These additional control variables should reduce the residual variation in the outcome variable and produce a lower estimate of the between-family variation, σ a 2 * , than the estimate produced without the added controls. Abstracting from measurement error, we can interpret the difference between these two estimates, σ a 2 − σ a 2 * , as an upper bound on the amount of the variance in the family component that can be explained by parental income and education. It is viewed as an upper bound since it includes other factors that are correlated with parental income and education. 10 9 <ref type="bibr">Björklund and Jäntti (2012)</ref> discuss this issue in great detail and provide quantitative examples of its importance. <ref type="bibr">10</ref> In the presence of measurement error, this difference, σ 2 a − σ 2 * a , is more correctly viewed as a downwardly biased estimate of the upper bound on the amount of the variance in the family component that can be accounted for by parental income and education. This experiment also produces a new sibling correlation ρ * . From what we know about the relationship between parents' income and education and children's crime (see, e.g., <ref type="bibr">Hjalmarsson and Lindquist 2012)</ref>, we expect this new sibling correlation to be lower but still substantial in magnitude.</p>
            <p>Once again, x in allows for the inclusion of multiple control variables. Note that the family component, a f , is now subsumed by the individual component, e in . To net out sorting into neighborhoods (i.e., to remove the covariance term), we include controls for parents' income, education and criminality, and family structure in x in . <ref type="bibr">11</ref> We also include gender and birth year dummies. 12 11 Sorting on unobservables may also occur. To the extent that these unobservable characteristics are uncorrelated with our control variables, they will bias our estimated neighborhood correlations upwards. <ref type="bibr">12</ref> In practice, we estimate neighborhood correlations after first randomly drawing one child from each family. We do this, so that large families living in small neighborhoods will not dominate the estimation. Alternatively, one could estimate neighborhood, family, and individual variance components simultaneously by applying <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="f4dd8eed51-software-simple-1">STATA</rs>'s xtmelogit to the whole sample and estimating a model with three levels. But due to our large sample, our use of control variables, and the large number of correlations that we need to calculate in this paper, this approach becomes infeasible; running time and convergence become problematic. In some cases, we have used both methods and the answer does not change. The fact that we randomly sample one child from each family does not seem to matter much either.</p>
            <p>The intensive margin correlations are computed using binary outcome variables. <ref type="bibr">21</ref> For crime, we have constructed binary outcome variables taking the value 1 if an individual has committed at least x crimes (where x is equal to 1, 2, 3, 4, 5, or 10 crimes). For prison sentence length, we have constructed binary outcome variables indicating if an individual has spent a total of at least x months in prison (where x isSibling correlations are estimated using <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="f4dd8eed51-software-simple-2">STATA</rs>'s xtlogit command. All outcome variables are dichotomous. Birth year and gender dummies are included. Standard errors are in parentheses. Singletons are included, except in the estimation of the mixed siblings. Estimates are not sensitive to the inclusion/exclusion of singletons equal to 0.5, 1, 2, 3, 4, 5, 6, or 12 months). We show the intensive margin correlations in <ref type="figure">Fig. 1</ref> along with approximate 95 % confidence intervals. The trend line in <ref type="figure">Fig. 1</ref> is upward sloping for sibling correlations in crime. This means that factors shared by siblings account for a larger share of the variation in crime at the intensive margin. For instance, for sisters, the share of the variation that can be attributed to factors shared by siblings is more than twice as high for "at least 10 crimes" than for "at least 1 crime."</p>
            <p>The results estimated from Eqs. 9 and 10 are reported in the first two columns of <ref type="table">Table 4</ref> for all crime. Having an older sibling who is convicted of at least one crime in year t − 1 raises the younger sibling's odds of receiving at least one conviction in 30 Schrøter Joensen and Skyt Nielsen (2015) report a causal influence of older male siblings' choice of academic curricula on their younger male siblings' choice of academic curricula. 31 None of the results in the section change if we, instead, use the number of convictions each year, that is if we instead create a panel of annual intensive margin variables.Odds ratios (OR) are estimated using logistic regressions. Standard errors clustered on individuals, i.e., the younger sibling, are reported in parentheses. Family background controls include parental income and education, parental criminality, and family structure (household size at age 15, unknown father, mother's age at first birth, and household type at age 15). Sibling correlations in columns <ref type="formula">(1)</ref> and <ref type="formula">(5)</ref> are estimated using <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="f4dd8eed51-software-simple-3">STATA</rs>'s xtlogit command. Sibling correlations reported in columns (2-4) are approximated using the following formula: ρ ≈ (OR φ − 1)/(OR φ + 1), where φ is calibrated using the odds ratios and sibling correlations estimated in columns <ref type="formula">(1)</ref> and <ref type="formula">(5)</ref>. In the first row of sibling correlations (involving C 1 t−1 ), φ is equal to 0.82. In the second row of sibling correlations (involving C 1 t ), φ is equal to 0.76 * * * p &lt; 0.01</p>
            <p>The highest correlations are the correlations for property crime for men and women, which are both 0.03. These correlations indicate that at most 3 % of the variation in property crime can be attributed to shared neighborhood factors. These 36 Most Swedish studies using observational data tend to find limited influences of neighborhoods on children's outcomes (e.g., <ref type="bibr">Brännström 2004 and Lindahl 2011)</ref>. However, several quasi-experimental studies have demonstrated that growing up in an ethnic enclave in Sweden does have an important effect on children's outcomes (e.g., <ref type="bibr">Edin et al. 2003 andÅslund et al. 2011)</ref>. <ref type="bibr">37</ref> We cannot get information on schools attended nor on the Small Area Marketing Statistics used to define neighborhoods in <ref type="bibr">Sariaslan et al. (2013)</ref>, since the cohorts we study are too old.Neighborhood correlations are estimated using <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="f4dd8eed51-software-simple-4">STATA</rs>'s xtlogit command after first randomly drawing one individual from each family. This random draw is done separately for each of the three sample used. All outcome variables are dichotomous. Birth year and gender dummies are included. Controls for parental education and income, parental criminality, and family structure are also included in order to net out sorting into neighborhoods. Standard errors are in parentheses should be contrasted with our brother and sister correlations in "at least one property crime," which are 0.38 and 0.34, respectively. Thus, neighborhood effects appear to be able to explain approximately 7 % of the sibling correlation in property crime at the extensive margin. Given that we have included all of the family-wide controls used in Section 5, these 7 % can be added on to the 27 % explained in the previous section. Thus, it appears that we can account for as much as one third of the sibling correlation at the extensive margin. The intensive margin neighborhood correlations are again computed using binary outcome variables. We show the neighborhood correlations for up to 10 crimes and up to 1 year in prison in <ref type="figure">Fig. 6</ref>. In <ref type="figure">Appendix Fig. 8</ref>, we show separate figures for each crime type.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d58f00e3ed">
            <titleStmt>
               <title>Manufacturing strategies and choices in cultural contexts</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3846/1611-1699.2009.10.279-289</idno>
                  <idno type="origin">10.3846%2F1611-1699.2009.10.279-289</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>In order to reduce number of variables and get more reliable indicators, principal component analysis followed by the exploratory factor analysis with varimax rotation was completed for the variables describing different aspects of manufacturing strategies and operations. Some of the original variables were excluded from the analysis due to the bad fi t with the resulting factor model. In the fi nal model 3 factors explain 66.6% of the total variance of 31 initial variables (see Appendix 2). The subscales for each factor were computed by the aid of the regression algorithm built into the factor analysis tool in the <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d58f00e3ed-software-simple-0">SPSS</rs> software. The internal reliability of our three subscales was confi rmed by theoretical fi t and also by the fact that the factor loadings of every initial variable were very high for one factor and relatively high proportion of the initial variance is described by the model.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c4a9d6179d">
            <titleStmt>
               <title>How To Count Citations If You Must</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/aer.20140850</idno>
                  <idno type="origin">10.1257%2Faer.20140850</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>22 Absent a compelling argument for choosing one over the other, one must conclude that this adjustment procedure is not well founded. <ref type="bibr">21</ref> This h -index adjustment is suggested by <ref type="bibr">Kaur, Radicchi, and Menczer (2013)</ref> and it is used by the online citation analysis tool <rs cert="0.8" resp="#annotator13" type="software" xml:id="c4a9d6179d-software-0">Scholarometer</rs> (<rs corresp="#c4a9d6179d-software-0" resp="#annotator13" type="url">http://scholarometer.indiana.edu</rs>), whose popularity for computing and comparing the h -indices of scholars across disciplines seems to be growing. <ref type="bibr">22</ref> Indeed, the ranking in (3.2) is impossible to obtain through any rescaling of the mathematicians' and the biol-</p>
            <p>We conclude by providing the details of the two empirical exercises described in the introduction. <ref type="bibr">23</ref> In both exercises we rely on a dataset constructed by Glen Ellison who graciously made it available to us. <ref type="bibr">Ellison (2013, p. 68)</ref> describes his data as follows: "The dataset includes citation records of almost all faculty at the top 50 economics departments in the 1995 NRC ranking. Faculty lists were taken from the departmental websites in the fall of 2011. Citation data for each economist were collected from <rs cert="0.7" resp="#annotator13" subtype="used" type="software" xml:id="c4a9d6179d-software-simple-2">Google Scholar</rs>. Economists were classified into one or more of 15 subfields primarily by mapping keywords appearing in descriptions of research interests on departmental or individual websites. Slightly less than one-half of economists are classified as working in a single field."</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c000ee65d2">
            <titleStmt>
               <title>Finding the Right Mix: How Do Contextual Factors Affect Collaborative Mental Health Care in Ontario?</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3138/h400-0370-l1p4-k804</idno>
                  <idno type="origin">10.3138%2Fh400-0370-l1p4-k804</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Analysis of the findings was ongoing throughout the data-collection period. This allowed emergent issues to be probed during subsequent interviews and to inform the sampling of additional informants.Sampling continued until saturation with regard to the contextual factors was reached. Each transcript was then thoroughly reviewed by the first author to reveal common themes. These themes were coded using <rs cert="1.0" resp="#annotator31" subtype="used" type="software" xml:id="c000ee65d2-software-simple-0">NVivo</rs> qualitative analysis software. Some of the themes were directly related to the interview questions and others emerged from the interviews. The richness of the diversity of factors raised by respondents and their concurrence about the importance of these factors prompted a return to the literature to further assess the salience of these issues. It also led directly to thinking about how to place the diversity of factors into a coherent framework that would be helpful to policymakers and health-care providers interested in developing successful interdisciplinary collaborative mental healthcare programs. The conceptual framework presented here should be considered a starting point for further development. <ref type="figure">Figure 1</ref> presents the conceptual framework that emerged from this analysis. The meaning of each factor within the framework is detailed in <ref type="table">Table 1</ref>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c9e90f19c2">
            <titleStmt>
               <title>Network Externalities and Compatibility Among Standards: A Replicator Dynamics and Simulation Analysis</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s10614-017-9706-4</idno>
                  <idno type="origin">10.1007%2Fs10614-017-9706-4</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>While the literature does not offer any previous models on strategic use of network externalities and market power in the case of tied standards, there is a small literature tradition on product bundling <ref type="bibr">(Choi 2004; Nalebuff 2004; Miao 2010; Eisenmann et al.</ref> <ref type="bibr">Luksha (2008)</ref> recounts several cases of cooptation of organizational networks (supplier networks, user networks) by a dominant firm for its own purposes. His examples include Microsoft as the dominant player in the PC operating systems' segment forcing cooperation between the two major competitors in the PC processor segment (which is clearly tied to PC operating systems), Intel and AMD. Luksha also identifies firms that actively shape and coordinate their user networks, namely Sun (then the vendor of a wide array of IT products including <rs cert="1.0" resp="#annotator5" type="software" xml:id="c9e90f19c2-software-simple-0">Java</rs>, <rs cert="1.0" resp="#annotator5" type="software" xml:id="c9e90f19c2-software-simple-1">StarOffice</rs>, <rs cert="1.0" resp="#annotator5" type="software" xml:id="c9e90f19c2-software-simple-2">MySQL</rs>,<rs cert="1.0" resp="#annotator5" type="software" xml:id="c9e90f19c2-software-simple-3">Solaris</rs>), Google, and Intel. Luksha does not go into detail on this, but it is clear that this activity is targeted on entrenching and extending the dominant position and the usage shares of the various products. This can be done by bundling products (across tied segments), particularly, in case of firms that offer a wide variety of related products like Sun (at the time), Google, Apple, and Microsoft. It can also be accomplished by acquiring more well-connected users (say, university students and faculty) and perhaps by coercing the help of prominent institutions (say, by offering special deals to universities)-in fact, aggressive marketing to otherwise privileged user groups is rather common. The case of Sun's Java offers a prime example of a huge marketingcampaign that was run to position the new standard in the mid 1990s, probably in full awareness of the intricacies of network externalities <ref type="bibr">(Garud et al. 2002)</ref>. Further, many successful commercial ICT standards started out by securing a temporary bundling or at least a licensing cooperation with one of the major players in a tied subsector: Microsoft's success in the 1980s came after (and as a direct consequence of) its alliance with IBM in the 1970s; one of the first strategic actions of <rs corresp="#c9e90f19c2-software-4" resp="#curator" type="publisher">Sun</rs>'s deployment of <rs cert="1.0" resp="#curator" type="software" xml:id="c9e90f19c2-software-4">Java</rs> in 1995 was a licensing agreement with Microsoft <ref type="bibr">(Garud et al. 2002)</ref>; the three successful mobile operating systems (<rs resp="#curator" type="software" xml:id="c9e90f19c2-software-simple-6">Android</rs>, <rs resp="#curator" type="software" xml:id="c9e90f19c2-software-simple-7">iOS</rs>, <rs resp="#curator" type="software" xml:id="c9e90f19c2-software-simple-8">Windows Mobile</rs>) were established by major vendors of other (tied) standards, the <rs resp="#curator" type="software" xml:id="c9e90f19c2-software-simple-9">Google search engine</rs> (in combination with other Google online services) in case of <rs resp="#curator" type="software" xml:id="c9e90f19c2-software-simple-10">Android</rs> and PC operating systems in case of <rs corresp="#c9e90f19c2-software-5" resp="#curator" type="publisher">Apple</rs>'s <rs resp="#curator" type="software" xml:id="c9e90f19c2-software-5">iOS</rs> and <rs corresp="#c9e90f19c2-software-6" resp="#curator" type="publisher">Microsoft</rs>'s <rs resp="#curator" type="software" xml:id="c9e90f19c2-software-6">Windows Mobile</rs>. Sure enough, they swiftly displaced the early mobile operating systems by less well-positioned vendors, such as Blackberry and <rs resp="#curator" type="software" xml:id="c9e90f19c2-software-simple-15">Symbian</rs> <ref type="bibr">(West and Mace 2010). 30</ref> More recently there has been speculation about which segment is likely to determine the future development of the ongoing mobile device platform competition with some scholars arguing that attracting third-party developers is the most important aspect <ref type="bibr">(Ghazawneh and Henfridsson 2013)</ref> while others contend that mobile online services (app stores, integration with social networks) play a crucial role compared to more traditional (software and hardware) segments <ref type="bibr">(Kenney and Pon 2011)</ref>. Specifically, the platforms that emerged as superior, <rs corresp="#c9e90f19c2-software-7" resp="#curator" type="publisher">Apple</rs> <rs resp="#curator" type="software" xml:id="c9e90f19c2-software-7">iOS</rs>, <rs corresp="#c9e90f19c2-software-8" resp="#curator" type="publisher">Google</rs> <rs resp="#curator" type="software" xml:id="c9e90f19c2-software-8">Android</rs>, and <rs resp="#curator" type="software" xml:id="c9e90f19c2-software-simple-20">Windows Mobile</rs> were identified as those able to generate revenue in those services. An interesting strategy may be that of Google, which generates its revenue and the network externalities crucial in keeping the platform competitive in entirely different segments <ref type="bibr">(Kenney and Pon 2011)</ref>. Note, however, that this particular analysis in <ref type="bibr">Kenney and Pon (2011)</ref> does not place too much value on the integration of and compatibility among the segments; as the simulations above suggest, this may be another crucial effect, which may be at the bottom of the success of the tightly closed Apple platform.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d688dc178c">
            <titleStmt>
               <title>AN EVALUATION OF TOTAL PROJECT RISK BASED ON FUZZY LOGIC</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3846/btp.2015.534</idno>
                  <idno type="origin">10.3846%2Fbtp.2016.534</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The case study presents the use of fuzzy logic at evaluation of total project risk base on RIPRAN method. The Fuzzy logic Toolbox of the <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d688dc178c-software-simple-0">MATLAB</rs> software was used for the creating of the decision making model. At first is it necessary to design the variables, their attributes and their membership functions.</p>
            <p>For implementation of fuzzy model in <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d688dc178c-software-simple-1">MATLAB</rs> software was created executable file called "M-file". This file contains the following sequence of commands <ref type="figure">(Fig. 10)</ref>. This file is used to enter the input values (NSR, TVSR) and automatically evaluate the total value of project risk (TVPR).</p>
            <p>The advantage of this fuzzy model is the ability to transform the input variables The Number od SUb-Risks (NSR) and The Total Value of Sub-Risk (TVSR) to linguistic variables, as well linguistic evaluated The Total Value Project RIsk (TVPR) -output variable. With this approach it is possible to simulate an uncertainty that is always associated with projects. After the fuzzy model is constructed, it is necessary to tune it (to set up the inputs on known values, evaluate the results and to change the rules or weights, if necessary) when the model was built. If the fuzzy model is tuned, it is possible to use it in practice. For implement the fuzzy model in <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d688dc178c-software-simple-2">MATLAB</rs> can also created an executable file called M-File. M-file is used to enter the input values and automatically evaluate the total risk of the project.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="afead5ae63">
            <titleStmt>
               <title>Simulating World Trade in the Decades Ahead: Driving Forces and Policy Implications</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/twec.12479</idno>
                  <idno type="origin">10.1111%2Ftwec.12479</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>This paper combines CGE modelling (<rs cert="1.0" resp="#annotator3" subtype="used" type="software" xml:id="afead5ae63-software-simple-0">MIRAGE</rs>) with the macroeconometric model<rs resp="#curator" type="software" xml:id="afead5ae63-software-simple-1">MaGE 1</rs> to address the sectoral implications of the envisaged growth trajectory of national economies. <ref type="bibr">2</ref> The driving forces of the changes modelled with <rs cert="1.0" resp="#curator" subtype="used" type="software" xml:id="afead5ae63-software-simple-2">MIRAGE</rs> are the change in countries' GDP per capita, the (sectorally differentiated) gains in total factor productivity (TFP) by these countries, their changing comparative advantage and evolving country sizes. On the demand size, country size and GDP per capita determine the size of markets and demand composition. With the reshaping of the world economy, the patterns of international demand will be affected profoundly. Meanwhile, the capabilities of countries will change, as developing economies accumulate human capital, invest in infrastructure and new production capacities and progressively catch up in terms of efficiency. Finally, markets will change as well as exporters, leading to a profound revamping of world trade patterns. <rs cert="1.0" resp="#curator" subtype="used" type="software" xml:id="afead5ae63-software-simple-3">MIRAGE</rs> is providing a detailed description of these changes for a given classification of countries and regions and a given sectoral aggregation of traded goods and services. The transformations of bilateral and sectoral trade flows are driving changes in the sectoral value added in each country, which leads to a reallocation of resources and ultimately changes in factor prices. Last but not least, these reference trajectories will be 'bent' by the shocks to trade costs or the migratory movements simulated in various scenarios.</p>
            <p>To track these repercussions, CGE modelling is essential. The even bigger advantage of CGE analysis is its internal consistency: markets are cleared; what is produced somewhere is bought elsewhere; available production factors bind the production capacities of countries; sectors must invest to produce; excess demand will be covered by net imports; the external account will be balanced by capital flows when needed or else the real exchange rate (country competitiveness) will have to adjust. Such intrinsic consistency is precious when it comes to addressing issues of an extremely high dimensionality, with many countries growing at different speeds, trading with each other in various sectors ranging from agriculture to business services. In our mind, this intrinsic consistency in the representation of the changing patterns of the world economy, when all factors of growth are combined at the country level, is an enormous asset to conduct large-scale and medium-term foresight activities. Still, it has limits which have to be kept in mind when interpreting the results. These limits are either common to any CGE model of this type, or specific to the version of <rs cert="1.0" resp="#annotator3" subtype="used" type="software" xml:id="afead5ae63-software-simple-4">MIRAGE</rs> used here.</p>
            <p>Common to all CGE approaches are the simplicity of the functional forms describing agent behaviour and the rather aggregated representation of the world economy, both imposed by computational constraints. The downside of such aggregation is that models do not track what is happening within a given sector in a given region. Similarly, simple functional forms (e.g. the absence of intertemporal optimisation by individual agents) imply that the dynamics of the model are rather elementary -meaning sequential. The model is solved sequentially, on a yearly step, and transition from one period to the next relies on changes in capital depletion, the sectoral allocation of new capital as well as changes in the labour force, TFP and energy efficiency. Also, the parameterisation of key behavioural parameters is rather crude and hardly addresses individual countries' idiosyncrasies. As sectors are aggregated, models do not fully reflect what could happen within a sector affected by, for example, a tariff cut on one product. Changes in tariffs, as any obstacles to trade, are modelled at the sectoral level, even if shocked at the most detailed level before aggregation. This implies that the impact of changes in trade costs (which depends on the variance of the trade costs on the top of the mean) is not measured precisely and that the creation of new flows of exports, the so-called extensive margin of trade, is not accounted for. In a nutshell, a country that does not export a category of goods will not begin to export these goods, when trade costs in the sector come down, as the trade elasticity is applied to a zero flow. Last but not least, CGE models generally are not confronted with historical data for validation. Partial remedies were considered in the current exercise, notably by making sure that <rs cert="1.0" resp="#annotator3" subtype="used" type="software" xml:id="afead5ae63-software-simple-5">MIRAGE</rs> reproduces the observed past response of trade to changes in world income. More specifically, some of the literature addressing the recent slowdown in the growth of international trade have pointed to the slowing pace of international vertical specialisation <ref type="bibr">(Constantinescu et al., 2015; Crozet et al., 2015)</ref> besides slow GDP growth as a possible explanation. Therefore, at the global level in the reference case, <rs cert="1.0" resp="#annotator3" subtype="used" type="software" xml:id="afead5ae63-software-simple-6">MIRAGE</rs> is set to reproduce a conservative elasticity of world trade to income observed in the long run, with the exception of the 1990s, a transitory period characterised by the expansion of global value chains and the surge of new big traders.</p>
            <p>Not all limitations could be addressed in this framework: for instance, if decisions were to depend on the future state of the economy, and uncertainty to be taken into account, a different class of models (dynamic stochastic general equilibrium models) would need to be used, which hardly allow for any sectoral or regional decomposition. Regarding the extensive margin of trade, or the more detailed representation of trade responses to shocks in trade costs, the large-scale ambition of this exercise did not allow us to use modelling advances sometimes implemented in smaller scale models. <ref type="bibr">3</ref> We also faced specific computational or data constraints in our present endeavour that might be circumvented in other simulations. The first issue concerns the representation of competition among firms. Many CGE models fit imperfect competition, and this can also be done in<rs cert="1.0" resp="#annotator3" subtype="used" type="software" xml:id="afead5ae63-software-simple-7">MIRAGE</rs>. However, given the long-term horizon of the exercise and the structural changes of the economies at stake, we were neither confident that the initial parameterisation of the model would fit well the development of these economies, nor were we convinced that the same parameterisation of market structures could apply in different economies. Combined with additional computing complexity, this led us to work with perfect competition. One important consequence of this choice is that our estimation of the gains from trade (via consumer demand or via producers' purchases of intermediate goods) is rather conservative, as it does not reflect variety gains, scale economies or firm selection. For the present endeavour, this is of lesser importance, as we do not focus on the gains from trade and specialisation, but on trade patterns.</p>
            <p>The next limitation relates to trade in services. Not all modes of supply (as defined by the WTO's General Agreement on Trade in Services (GATS) Art. I:2) are modelled in our exercise. We model services exported under mode 1 (cross-border trade) and mode 2 (consumption abroad), but modes 4 (presence of natural persons) and 3 (commercial presence) are absent. Quantitatively, this is not a major problem regarding mode 4, which is currently still small, although this might change substantially in the future. More importantly, the absence of services supply via commercial presence abroad is due to our decision not to activate the foreign direct investment (FDI) module in <rs cert="1.0" resp="#annotator3" subtype="used" type="software" xml:id="afead5ae63-software-simple-8">MIRAGE</rs>. We made this choice as a result of the lack of reliable information on future movements of FDI (although a snapshot of the current situation, which is usable for CGE modelling purposes, is provided by <ref type="bibr">Guimbard et al., 2012)</ref>. This limitation is linked to the more general problem regarding foreign presence. We do not have reliable information on the activity of foreign affiliates, with some exceptions (United States and, quite recently, EU Member States). The reason for this lack of information is that FATS (Foreign Affiliate Trade in Statistics) are not collected systematically and that FDI stocks (when available) are a poor proxy for the activity of foreign affiliates <ref type="bibr">(Fukui and Lakatos, 2012)</ref>.</p>
            <p>Besides these intrinsic limitations of CGE models and the specific choices made for this modelling exercise, CGE models are said not to take into account an important development of international trade patterns in recent times, namely the expansion of global value chains (GVCs), exemplified by 'Factory Asia' (WTO-IDE-JETRO, 2011). The development of GVCs changes many aspects of trade patterns, how to analyse them and the way in which technology, services, investment and trade interplay <ref type="bibr">(Elms and Low, 2013)</ref>. For example, it can be shown that services are increasingly also traded indirectly through trade in goods <ref type="bibr">(Stehrer, 2012)</ref>. Baldwin (2011) provides a theoretical framework explaining why the fragmentation of production occurs. In order to understand the implications of global production networks for CGE modelling, it is important to recall that CGE modelling is in line with input-output studies a la Leontief, and, in addition, puts emphasis on prices, market clearing and substitution. Inter-industry relationships are instrumental for the construction of CGE models, and the origin (domestic or imported) of intermediate consumption is traced. As the results of changing trade patterns can be expressed in terms of value added (and this is what we do here), it is possible to say that GVCs are intrinsically embodied in CGE models, even if trade is not reported in value-added terms in the simulations. <ref type="bibr">4</ref> This way of addressing the international fragmentation of production is still far from perfect. In <rs cert="1.0" resp="#annotator3" subtype="used" type="software" xml:id="afead5ae63-software-simple-9">MIRAGE</rs>, the geographic composition of the basket of intermediates applies overall and is not differentiated by importing sector. We can, for instance, track the various origins of imported steel in a country, but we cannot know the origin of steel imported by the car industry. We simply assume that the geographic origins of steel imports in that sector are similar to the overall distribution in the economy. Hence, GVCs de facto are present in CGE models, such as <rs cert="1.0" resp="#annotator3" subtype="used" type="software" xml:id="afead5ae63-software-simple-10">MIRAGE</rs>, but are not modelled as such owing to this proportionality assumption. Also, the activity of foreign affiliates is present in the database, but not identified as such, and the composition by country of origin of each imported intermediate product is present, but not at the level of the importing industry. Although trade is expressed in gross terms, changes in the value added of the sector mirror the reaction of industries to trade shocks and take into account international input-output relations, albeit with less detail than in dedicated GVC studies.</p>
            <p>The direction, composition and nature of international trade have changed substantially over the past couple of decades, and many of these trends may be related to the proliferation of GVCs. Emerging economies have gained a larger share in global trade and increasingly trade with one another. At the sectoral level, developing countries have diversified their exports and obtained a higher overall share in manufacturing trade. The importance of services trade has also risen. At the same time, much of 'global' trade is actually concentrated at the regional level. In analysing country, sector and factor outcomes of our integrated <rs cert="1.0" resp="#annotator3" subtype="used" type="software" xml:id="afead5ae63-software-simple-11">MaGE</rs> and <rs cert="1.0" resp="#annotator3" subtype="used" type="software" xml:id="afead5ae63-software-simple-12">MIRAGE</rs> simulations in more detail, we will now examine to what extent these trends in trade can be expected to continue in the future. <ref type="table">Table 2</ref> gives an overview of the combined macroeconomic and trade simulations in terms of projected average annual growth rates of GDP and exports up to 2035. Exports of goods are likely to vary more than GDP in the 'high' scenario despite the very conservative elasticity used in our model, as noted in Section 2. In the 'low' scenario, by contrast, trade would grow much slower and even less than GDP. Whether a future scenario of key economic and policy variables is rather optimistic or pessimistic matters a lot more for developing than for developed countries, with the former being characterised by a much larger variation in outcomes than the latter. While growth rates are generally lower in a gloomy economic and trade environment across both groups of countries, the difference is barely three-quarters of a per cent for developed countries' average GDP growth, but over 4 per cent for developing countries. Growth in GDP and services trade in developing countries is widely superior to growth rates in developed countries in the optimistic scenario, but much less so in a pessimistic scenario, where growth in goods trade could advance even more slowly than in developed countries. Examining specifically the trade cost component, it turns out that growth in developing countries would be about half a per cent higher/lower in an open/restrictive trade policy environment, while growth in developed countries would hardly be affected.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="db7802c1e8">
            <titleStmt>
               <title>Genotype–environment interactions and their translational implications</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.2217/pme.10.75</idno>
                  <idno type="PMC">PMC3108095</idno>
                  <idno type="PMID">21660115</idno>
                  <idno type="origin">10.1111%2Fjems.12141</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Existing studies of sponsored search are typically based on a modest number of search terms and the corresponding number of paid clicks received by a single retailer. Our research complements these studies by focusing on the organic clicks that 759 retail sites received from more than 12,000 search terms. There is considerable cross-sectional variation in our data: It includes Web-only retailers as well as traditional retailers and covers 15 different retail segments including apparel, electronics, and mass merchants. For each of these search terms, we observe which retail sites received organic clicks as well as the number of clicks. We also obtained data from the first five pages of search results on <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-0">Google</rs> and <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-1">Bing</rs> for each search term, and this ultimately permits us to quantify the impact on organic clicks of a site's rank (position) in the search results. Our data also include several different measures of the accumulated brand equity of online retailers. These data allow us to determine whether consumers are more likely to click the link of a retailer who is perceived to operate a high-quality site (as a result of the retailer's current and past investments in advertising, the depth and breadth of offerings, secure payments, one-click purchases, returns policies, and so on). Ultimately, this permits us to quantify the benefits of SEO strategies that attempt to gain traffic by improving a retailer's rank in organic search results, versus gaining traffic by improving the quality and brand awareness of a site.</p>
            <p>The remainder of this section provides an overview of SEO and the related literature. Section 2 discusses our data and describes the econometric methodology underlying our analysis. Section 3 presents our empirical findings, whereas Section 4 provides robustness checks and some additional results. Finally, we conclude in Section 5 with some additional managerial implications of our findings for SEO. <ref type="figure">Figure 1</ref> highlights the avenues that retailers have for gaining traffic through search engines. This screenshot shows the search results that appear following a search for "shoes online" using <rs cert="1.0" resp="#annotator20" type="software" xml:id="db7802c1e8-software-simple-2">Google Search</rs>. In this particular example, three different types of links appear: top ads, side ads, and organic results. <ref type="bibr">3</ref> The top ads (marked by the red box in <ref type="figure">Figure 1</ref>), if any, are the highest listed search results and appear against a yellow background. For this particular search there are three top ads; the maximum number of top ads that may be displayed is four. The organic results (marked by the blue box) are listed below the top ads. Up to ten organic results can appear on a search result page. Finally, the side ads (purple box) appear on the right-hand side of the screen; <rs cert="1.0" resp="#curator" subtype="used" type="software" xml:id="db7802c1e8-software-simple-3">Google</rs> allows for up to eight side ads to be shown on a result page.</p>
            <p>One way retailers obtain traffic is through the paid links that appear in top or side ads. Unlike organic links, retailers can directly influence the position of ads, which are displayed and ranked according to the results of an auction that is run in real time. Retailers identify keywords they want to bid on and specify how much they are willing to spend. <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-4">Google</rs> determines the ad rank using a site's maximum bid specified for the keyword and a quality score, which includes factors like click-through rates and relevance. Advertisers only pay when the link is clicked; the cost per click is equal to the minimum amount needed to get a specific position (generalized second-price auction mechanism). There is an extensive literature (discussed below) examining this avenue for obtaining clicks.</p>
            <p>A second way retailers obtain traffic through the search engine channel is through clicks on organic results, and this is the focus of our analysis. 4 A site's position in <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-5">Google</rs>'s organic search results depends on the site's relevance to a given search term. The exact algorithm that <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-6">Google</rs> uses to determine a site's ranking is proprietary; according to <rs cert="1.0" resp="#curator" subtype="used" type="software" xml:id="db7802c1e8-software-simple-7">Google</rs>, it depends on thousands of factors. <ref type="bibr">5</ref> Although the goal of SEO is to optimize the organic traffic a retailer receives through product searches on search engines, the ultimate goal of retailers is presumably to maximize their profits. One of the initial steps in this optimization process is identifying the benefits and costs of different strategies for increasing traffic. <ref type="bibr">6</ref> Our paper represents a first attempt to examine the benefits side of the ledger, and in particular, to quantify the drivers of retailers' organic clicks.</p>
            <p>Our main dataset is assembled using data from <rs corresp="#db7802c1e8-software-3" resp="#curator" type="publisher">comScore</rs> <rs cert="0.8" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-3">Search Planner</rs> and contains information on the number of organic clicks web sites received for search terms and phrases entered at main search engines (i.e., <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-10">Google</rs>, <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-11">Bing</rs>, <rs cert="1.0" resp="#annotator20" type="software" xml:id="db7802c1e8-software-simple-12">Yahoo</rs>, <rs cert="0.9" resp="#annotator20" type="software" xml:id="db7802c1e8-software-simple-13">AOL</rs>, and <rs cert="0.9" resp="#annotator20" type="software" xml:id="db7802c1e8-software-simple-14">ASK</rs>) during August 2012. <rs cert="0.8" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-15">Search Planner</rs> uses the comScore panel, which contains all online browsing activity of around two million U.S. users. Because our goal is to analyze the drivers of organic traffic following product searches, we restrict our sample to only include web sites that are Internet retailers. For this we make use of Internet Retailer's Top 500 Guide, which contains a ranking of North America's 500 largest e-retailers based on annual Web sales. Although not all of these retailers appeared in the <rs corresp="#db7802c1e8-software-30" resp="#curator" type="publisher">comScore</rs> <rs cert="0.7" resp="#curator" subtype="used" type="software" xml:id="db7802c1e8-software-30">Search Planner</rs> database, some e-retailers (e.g., Amazon) operate multiple web sites (e.g., Amazon.com and Zappos.com), resulting in a total of 759 retail sites for which we have click-through data. For each of these 759 retailers, we used <rs cert="0.7" resp="#curator" subtype="used" type="software" xml:id="db7802c1e8-software-simple-18">Search Planner</rs> to identify all search terms that generated traffic from <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-19">Google</rs> to the retailer. There is some overlap in search terms: as shown in <ref type="figure">Figure 1</ref>, Onlineshoes.com as well as Zappos.com appear relatively high in the organic results for the term "shoes online," which means that for both retailers this term is part of the set of search terms that generated traffic from <rs cert="1.0" resp="#curator" subtype="used" type="software" xml:id="db7802c1e8-software-simple-20">Google</rs>. In total we end up with 12,184 distinct search terms that led users to the 759 online retailers. The third dataset we use contains all the links that appeared on the first five search results pages on <rs cert="1.0" resp="#annotator20" type="software" xml:id="db7802c1e8-software-simple-21">Google Search</rs> and <rs resp="#curator" type="software" xml:id="db7802c1e8-software-simple-22">Bing Search</rs> for each of the 12,184 search phrases. We collected these data using a scraper written in Java; the data contain organic search results as well as paid links. <ref type="bibr">10</ref> Not all 759 online retailers are relevant for each of the search terms in our data. For instance, Best Buy is not relevant for individuals searching for shoes and is therefore 9. Aaker (1991) defines brand equity as "a set of brand assets and liabilities linked to a brand, its name and symbol, that add to or subtract from the value provided by a product or service to a firm and/or to that firm's customers." Brand equity is based on factors like brand loyalty, name awareness, and quality. See <ref type="bibr">Keller and Lehmann (2006)</ref> for a recent survey of the branding literature.</p>
            <p>Our measure of a retail site's brand equity is based on the methodology developed in <ref type="bibr">Baye et al. (2012)</ref> to overcome challenges in measuring the "prominence" of online retailers' names. These authors point out that the standard approach, which uses historical advertising data to measure accumulated brand equity and the strength of a firm's name, is not useful in the case of online retailers. Among other things, many online retailers are privately held and do not disclose advertising expenditures; the parent companies of publically traded online retailers do not typically report monthly advertising expenditures at the URL level. They further argue that the number of product searches on <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-23">Google</rs> (or <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-24">Bing</rs>) that include the retailer's name or URL in the search query-navigational searches, in industry parlance-is a useful measure of a retailer's brand equity. Intuitively, the inclusion of "Amazon" in a product search indicates the consumer recognizes the company and can recall its name. This may be the result of past advertising campaigns, recommendations from friends, knowledge of Amazon's pricing practices, product depth, return policies, and so on. And in contrast to advertising expenditures, it is possible to use data from comScore to measure the number of navigational searches Amazon received in a given month.</p>
            <p>For these reasons, we use navigational searches to measure a firm's brand equity. Navigational searches are essentially a shortcut for typing in the URL of a specific retailer and then searching its site. Thus, our measure of brand equity is the total number of organic clicks a particular retailer received in August 2012 from searchers who navigated 11. Indeed, Best Buy did not show up in at least the first 30 pages of search results on <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-25">Google</rs> for the search term "shoes online" (checked on February 26, 2013).</p>
            <p><rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-26">Google</rs> continuously updates its rankings of search results to generate the most relevant search results, which means that our rank variable will depend on past clicks. It is therefore likely that rank is correlated with the error term and thus endogenous. A similar effect may be at work for the ads variable: Ad positions are based on the outcome of a second-price auction that takes the relevance of the bidder with respect to the search term into account, again making it likely that ad positions are based on past clicking behavior on <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-27">Google</rs>.</p>
            <p>The standard approach in the literature on clicks at platforms (e.g., clicks at price comparison sites or sponsored clicks at search engines) is to assume that such positions are exogenous. Using the Wu-Hausman test for endogeneity, however, we reject the hypothesis that rank and ad positions are exogenous in our data ( p = 0.0023 and p = 0.0116, respectively). To account for the potential endogeneity of these variables, we use information about rank and ads on <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-28">Bing</rs> as instruments. These instruments are correlated with the endogenous regressors, but are unlikely to be correlated with the error term, because <rs cert="0.9" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-29">Bing</rs>'s decisions on search result rankings and ad positions are not based on past clicks on <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-30">Google</rs>. Indeed, using the Sargan test for overidentifying restrictions, we cannot reject the hypothesis that these are valid instruments for rank and ad positions on <rs cert="1.0" resp="#curator" subtype="used" type="software" xml:id="db7802c1e8-software-simple-31">Google</rs> ( p = 0.3795).</p>
            <p>As we explained in Section 2.1, a retail site is included as an observation if it appears on the first five pages of the <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-32">Google</rs> search result page for a specific search term, independent 15. Retail segment fixed effects control for systematic differences in clicks across, for example, mass merchants who may receive many clicks owing to product breadth effects and specialty retailers who receive fewer clicks. Although this specification assumes the marginal impact of brand equity and rank is identical across retail segments, we show in Section 4 that the results are similar when one excludes mass merchants such as Amazon and Walmart.</p>
            <p>16. The p-value for this test, which uses navigational searches on <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-33">Bing</rs> as an instrument, is p = 0.1692. of whether the retailer received organic clicks according to <rs cert="0.7" resp="#curator" subtype="used" type="software" xml:id="db7802c1e8-software-simple-34">Search Planner</rs>. Complicating matters, <rs cert="0.7" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-35">Search Planner</rs> only reports the number of organic clicks if those clicks exceed a certain threshold, which means we do not know whether sites receiving zero organic clicks according to <rs cert="0.7" resp="#curator" subtype="used" type="software" xml:id="db7802c1e8-software-simple-36">Search Planner</rs> really received no click-throughs for the search term in question or whether they were censored. What makes our setting different from a standard censoring environment is that the selection rule depends on total clicks (including paid clicks) rather than just organic clicks. This means that a different probability mechanism generates both the zero clicks and the positive clicks, and this cannot be captured by a standard Tobit censoring model. For this reason, we estimate a Heckman-type selection model. As we argued in the previous subsection, endogeneity is likely to be important in our data, so we allow for endogenous explanatory variables. Estimation of the model consists of two stages. In the first stage we regress a dummy for having positive clicks on all exogenous variables (including instruments) z. Here, it is important to include at least one more instrument than is necessary for dealing with the endogeneity problem (otherwise identification is purely based on the parametric form of the inverse Mills ratio). This additional exclusion restriction should relate to the probability of observing positive organic clicks. Because this probability relates to the number of total clicks, we use additional variables in the selection equation that are important for getting paid clicks: We add dummies for whether a sponsored link was displayed on each of pages 2 through 5 in the <rs cert="0.9" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-37">Bing</rs> search results. We obtain the inverse Mills ratio, given byλ = λ(zδ) = φ(zδ)/ (zδ) from the selection equation, and add this to the second stage to obtain</p>
            <p>The second specification in <ref type="table">Table III</ref> uses navigational searches from <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-38">Bing</rs> rather than <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-39">Google</rs> to measure brand equity. Because <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-40">Bing</rs> has a different population of users and employs a different algorithm for returning search results, it is unlikely that unobserved factors that affect clicks on <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-41">Google</rs> are correlated with this measure of brand equity based on navigational searches on <rs cert="1.0" resp="#annotator20" subtype="used" type="software" xml:id="db7802c1e8-software-simple-42">Bing</rs>. The results in column <ref type="formula">(2)</ref> show that our findings are robust to using this alternative measure of brand equity.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="dc71d2adaf">
            <titleStmt>
               <title>The formation of a good safety culture at enterprise</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3846/1611-1699.2009.10.169-180</idno>
                  <idno type="origin">10.3846%2F1611-1699.2009.10.169-180</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The data were collected during 2002-2004. The objective of the questionnaire was to gather information about the workers' knowledge of the occupational risks connected with their occupation, possible work-related occupational hazards, and attitudes towards using personal protective equipment and safety routines. Questions about the workers' opinions and their perceptions about the exposure to occupational risks (noise, chemicals, vibration, bad microclimate, insuffi cient lighting, and ergonomics), experienced health symptoms and their awareness of the Occupational Health and Safety Act were also asked. The workers were randomly selected from textile, printing, mechanical, plastic, food processing industries and offi ces. A questionnaire was distributed to 412 workers in investigated industries. The average age was 43.6 years. Confi dentiality of information was always maintained and only coded information was used in the analysis. Data collected on the questionnaire study was coded and then entered into the statistical software programme (<rs cert="1.0" resp="#annotator7" subtype="used" type="software" xml:id="dc71d2adaf-software-simple-0">Excel</rs>) for analyses. The investigation gave results as follows: 1. 65% of workers considered excessive exposure to noise, 60% bad microclimate (too cold workrooms), 54% bad lighting at workplaces; not-effective ventilation was the main occupational risk factor (75% of workplaces). 2. Only some of the workers (10%) answered that they know which information about the dangerousness of chemicals may be obtained from the chemical labels. Nobody of respondents pointed at the chemical safety data sheets as one of the important source of information. 3. Most of the workers (80%) reported that personal protective equipment (PPE) was accessible; however, the questionnaire did not show the attitudes towards using PPE. Some of the respondents (36%) pointed that the PPE disturbs the work; others mentioned (46%) that the quality of PPE is not very good. 4. In the present study, 63% of the workers reported that they were aware of the person who was the working environment specialist in their company. At the same time, only 20% of workers knew who was elected to be their working environment representative. 5. Different opinions were given on safety instructions and guidelines. In the present study, 25% of the workers reported that they had not received safety instructions and guidelines. The assessment of the safety instructions was made on a 6-point scale (from 0 to 5, where 0 was bad and 5 -very good). According to workers' opinion, the quality of accessed safety instruction was satisfactory (2.8 points). 6. The questionnaire revealed that only 25% were generally aware of the safety legislation.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="b2b5ec53b5">
            <titleStmt>
               <title>PanelWhiz and the Australian Longitudinal Data Infrastructure in Economics</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/j.1467-8462.2013.12010.x</idno>
                  <idno type="origin">10.1111%2Fj.1467-8462.2013.12010.x</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>One software solution to the many of the issues raised here is a tool called '<rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-4">PanelWhiz</rs>' &lt;<rs corresp="#b2b5ec53b5-software-4" resp="#annotator2" type="url">http://www.panelwhiz.com</rs>&gt;, which is written in and for the statistical package called <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-5">Stata</rs> &lt;<rs corresp="#b2b5ec53b5-software-5" resp="#annotator2" type="url">http://www.stata.com</rs>&gt;. For the German SOEP, its functionality is summarised in Haisken-DeNew and Hahn (2010). <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-4">PanelWhiz</rs> was initially developed to deal specifically with the complexity of the German SOEP structure on variable naming, but has since expanded to cover all of the world's major household panel datasets.</p>
            <p><rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-5">PanelWhiz</rs> is a graphical user interface that allows users to extract data from known datasets in a controlled manner, without having to write data extraction code oneself. The structure of the data is in fact embedded into the data itself and is called up and read by<rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-6">PanelWhiz</rs> to allow it to use this structure automatically, without the user having to delve further in the data structure. In terms of the amount of <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-7">Stata</rs> programming code, it is the largest-ever user-written collection of add-ons for <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-8">Stata</rs>. <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-9">PanelWhiz</rs> provides in <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-10">Stata</rs> an analogous platform to what the Integrated Public Use Microdata Series (for more information, see Minnesota Population Center 2001) does at the browser level. This article outlines the features of <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-11">PanelWhiz</rs> in conjunction with Australian datasets that are listed in <ref type="table">Table 1</ref>.</p>
            <p>The obvious advantage of using <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-12">PanelWhiz</rs> is the power of the common interface. Every survey supported by <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-13">PanelWhiz</rs>, regardless of original structure, is integrated into the system in a similar manner. The entire dataset is scanned for critical pieces of information: (i) which physical data files belong together in a data year; (ii) variable names and how they correspond to each other over time; and (iii) keywords associated with variable labels.</p>
            <p>In contrast to standard statistical software, one does not open a data file and select one single variable, but rather one can select an entire vector of variables belonging to each other. As all words used in variable labels are scanned, one can compile a total list of all used words from the labels and create an index of keywords or dictionary, linking each known keyword to a particular variable, which in turn can be related to an entire vector of variables which belong together over time.°C To start off, one selects the dataset of interest (in this case, HILDA Survey). Assuming that one were to start a data project from scratch, as in <ref type="figure">Figure 1</ref>, one would first be interested in opening either a specific wave or a keyword list. Here, we will examine a specific wave. The HILDA Survey contains information from 2001 to 2011, and correspondingly, there are clickable links for each year's wave. A browse page is created when clicking on a year. However, many datasets contain thousands of variables, making the creation of browse pages a relatively slow process. <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-14">PanelWhiz</rs> creates a browse page once when needed, and if the page already exists, <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-15">PanelWhiz</rs> simply uses it. The browse page is valid for a given distribution of data. It may become necessary to delete and recreate the browse pages if, for instance, there is a new release or revised version of the data.</p>
            <p>After having selected several items and having stored the <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-16">PanelWhiz</rs> project under a name, such as 'myproject', the Project Page would look something like in <ref type="figure">Figure 5</ref>.</p>
            <p>The items selected would be sorted by their categories, such as 'AN' for 'Ancestry'. Adding any new items to the project would create additional entries in the Project Page. Here, in this particular view of the Project Page, we seeBy clicking on 'Retrieval', the data retrieval is started. File by file, the data files are opened, the selected variables are kept and possibly renamed, sub-sets of the files are stored in temporary files and then they are pieced together to create one rectangularised file in<rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-17">Stata</rs> 'long' format (observations stacked on each other, year by year).</p>
            <p><rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-18">PanelWhiz</rs> is a tool written entirely in <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-19">Stata</rs>'s own programming language, which facilitates easy data extraction using a graphical user interface. Users are not required to write data retrieval programs, as <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-20">PanelWhiz</rs> generates and runs extraction code on the fly, based on the variables selected. Due to its common interface and implementation for many datasets, it is useful for international comparative work and allows for an easy introduction to a new dataset for first-time users. As the data retrievals are standardised and used by many researchers, the chance of mistakes in the initial data-creation phase of any research is virtually reduced to zero. Updates to <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-21">PanelWhiz</rs> are pushed to users and are installed in the same manner as <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-22">Stata</rs> itself. <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-23">PanelWhiz</rs> currently has more than 300 users and currently supports international datasets such as the German SOEP, the British BHPS (or its successor, the UK Understanding Society study), the American PSID and Current Population Survey-National Bureau of Economic Research, the American Survey of Income and Program Participation, the South African National Income Dynamics Study, in addition to the HILDA Survey, MABEL, LSAC, LSIC and CASiE. <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-simple-24">PanelWhiz</rs> runs in <rs resp="#curator" type="software" xml:id="b2b5ec53b5-software-simple-25">Windows</rs>, Mac and <rs resp="#curator" type="software" xml:id="b2b5ec53b5-software-simple-26">Linux</rs>, but needs at least <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="b2b5ec53b5-software-28">Stata SE/MP</rs> <rs corresp="#b2b5ec53b5-software-28" resp="#annotator2" type="version">11</rs> or later to run.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f2ceada032">
            <titleStmt>
               <title>Regional Trade Agreements in East Asia: Will They Be Sustainable?</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/j.1467-8381.2009.02008.x</idno>
                  <idno type="origin">10.1111%2Fj.1467-8381.2009.02008.x</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>14 The model has three economic agents: producers, representative consumers (private and public) and trading partners. The GTAP6inGAMS model is a traditional static Arrow-Debreu general equilibrium model in which the zero profit condition and market clearance define the equilibrium. The GTAP6inGAMS is a modified version of the GTAP model version 6 developed for <rs cert="0.8" resp="#curator" subtype="used" type="software" xml:id="f2ceada032-software-simple-0">GAMS</rs> users. Most of the model specifications are the same as the GTAP model. The differences between the GTAP model and <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="f2ceada032-software-simple-1">GAMS</rs> version of the model are as follows. First, the GTAP model is based on a constant difference elasticity demand system, but the <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="f2ceada032-software-simple-2">GAMS</rs> model uses Cobb-Douglas preferences. Second, the GTAP model assumes that global capital is endogenously allocated by regional rates of return. However, for 12. They, however, propose to achieve the bigger trade blocs by utilizing ASEAN as the hub for the integration (e.g. the three sets of ASEAN+1 FTA), which is not desirable based on the empirical analysis in the present paper.</p>
            <p>13. An income terms of a trade measure, the purchasing power of exports in terms of imports (the ratio of the value of exports to the price of imports), can be a better measure of income or welfare effect of price changes in international trade, especially for developing countries, relative to the net barter terms of trade. See <ref type="bibr">Chacholiades (1990, pp. 136-7)</ref>. 14. The explanation of the CGE model used in this paper is derived from <ref type="bibr">Park (2006)</ref>. simplicity, the GTAP6inGAMS model exogenously fixes the global capital flows. The model described in the present paper uses a classification consisting of 7 sectors and 25 economies, as in <ref type="bibr">Park (2006)</ref>. <ref type="bibr">15</ref> The model solution has been calibrated, with 2001 as the base year, using global trade, assistance and production data from the GTAP 6 Database. <ref type="bibr">16</ref> The model has been implemented using the <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="f2ceada032-software-simple-3">GAMS MPSGE</rs>. <ref type="bibr">17</ref> The GTAP6inGAMS model is adopted because the CGE model is in line with the commonly used GTAP model and the solver, <rs cert="0.8" resp="#annotator5" subtype="used" type="software" xml:id="f2ceada032-software-simple-4">GAMS MPSGE</rs>, is simple and easy to handle.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="be2eaef0d4">
            <titleStmt>
               <title>The Australian Data-Driven Urban Research Platform: Systems Paper</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/1467-8462.12152</idno>
                  <idno type="origin">10.1111%2F1467-8462.12152</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Across Australia (and indeed in most other countries), a huge array of organisations collects and holds datasets that are fundamental to tackling the current and future challenges facing cities. On the one hand, some of these data providers have data that are directly accessible on the Web. For example, the Australian Bureau of Statistics (ABS, &lt;http://www.abs.gov.au&gt;) has over 1 million web pages through which a huge array of past and present Census data can be accessed, as well as an extensive array of surveys and reports related to the Australian population. On the other hand, many other national and statebased agencies do not offer access to their 'data', preferring instead to release data in aggregated forms; for example, as written reports. Being able to access and combine distributed data from multiple independent and autonomous organisations through a unified environment would greatly simplify the life of many urban researchers and allow major urban research challenges to be systematically tackled. Ideally, such a solution would offer live (programmatic) access to in situ remote data resources from the definitive data providers and avoid web scraping or other ad hoc data collection mechanisms that would break when websites and their content change. The solution should provide tools to support collection and enrichment of provider-specific metadata to enable complex data analytics scenarios using this diverse ecosystem of data. The <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-0">AURIN</rs> platform has delivered such an integrated, data-rich research environment. <rs cert="0.7" resp="#curator" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-1">AURIN</rs> is not unique in this endeavour and a range of efforts is currently ongoing-albeit on a much more focused domain; for example, with a fixed or known set of data providers.</p>
            <p>It is important to note that <rs cert="0.7" resp="#curator" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-2">AURIN</rs> offers single sign-on to all of the datasets, servicesSource: <ref type="bibr">Sinnott et al. (2012)</ref>.°C and tools offered by all providers through federated authentication to the <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-3">AURIN</rs> portal, available at &lt;<rs corresp="#be2eaef0d4-software-3" resp="#annotator14" type="url">https://portal.aurin.org.au</rs>&gt;; that is, users authenticate at their (potentially remote) home organisation and are subsequently able to access all data without further challenge or response demands from any data provider. It is important to note that single sign-on here does not imply that all data and tools are accessible to all users at all times. Several data providers place specific demands on who can access their data and tools and what they can subsequently do with these data. The Public Sector Mapping Agency (PSMA, &lt;http://www.psma.com.au&gt;) is one example of a commercial partner that requires restricted usage to their data. It should also be pointed out that through the federated authentication model, any academic across Australia (that is, those with an email address ending in '.edu.au') is able to access the portal directly. Other non-academics (for example, those from industry and government) are able to access the portal through targeted accounts that are set up through the access federation virtual home organisation.</p>
            <p>There are at present (December 2015) 1,661 datasets that are accessible through the <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-5">AURIN</rs> platform from 66 separate organisations. These data providers and the nature and number of data they make available are summarised in <ref type="table">Table 1</ref>. As seen, these datasets cross federal government, state government, local government, industry and academia. The data shown in <ref type="table">Table 1</ref> represent the total data that are currently available across Australia. However, it is often (typically) the case that researchers are interested in data related to a particular regional setting; for example, transport issues in Melbourne or property prices in Sydney. The <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-6">AURIN</rs> platform provides a user-driven interface that allows researchers to access data through selecting the area of interest through a map-based interface, as shown in <ref type="figure">Figure 2</ref>, or through selection of data based upon the geospatial aggregation level, as shown in <ref type="figure">Figure 3</ref>.</p>
            <p>It is important to note that the <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-7">AURIN</rs> platform provides an extensive variety of geospatial coordinate systems that capture changes over time. Thus, <ref type="figure">Figure 2</ref> shows the postal areas as defined by the ABS in 2011. A different set of polygons for those portal areas existed historically-this is the case for the many other flavours of geospatial classification that exists or has existed across Australia: local government areas, statistical local areas, working zones, statistical areas (SA4 down to SA1) as shown in <ref type="figure">Figure 3</ref>, labour force regions, greater capital city areas, statistical divisions, urban centre localities, suburbs, Commonwealth electronic divisions amongst many others. Capturing these changes over time is essential for researchers so that they can compare meaningful data in meaningful contexts. This is especially the case for those interested in longitudinal studies of cities.</p>
            <p>The datasets cover an extensive variety of information related to Australian urban settlements. As an example of the diversity of the information available through the <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-8">AURIN</rs> platform, searching across a range of keywords returns extensive data, as shown in <ref type="table">Table 2</ref>. It is noted that the platform supports a range of fuzzy search capabilities; for example, searching for employ will return matching datasets with data or metadata including terms such as unemployed and employment.</p>
            <p>The <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-9">AURIN</rs> platform allows much more targeted searching for specific data that might be available. Thus, transport researchers might be interested in terms such as train (34), tram (21), car (79), station (12), bus stop <ref type="formula">(2)</ref>, accident (31), bicycle (10), pedestrian (4) and commute <ref type="formula">(7)</ref>, where the numbers in parentheses represent the number of datasets that include the italicised keywords.</p>
            <p>It is essential that the <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-10">AURIN</rs> platform supports the access to, and integration of, diverse datasets. In supporting access to data, it was recognised that each remote data provider is autonomous and can make its data available through technologies that it deems appropriate; that is, <rs cert="0.7" resp="#curator" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-11">AURIN</rs> cannot demand that the provider install a particular software stack. A rich variety of technical solutions has been developed and delivered to meet the°C <ref type="bibr">2016</ref> The University of Melbourne, Melbourne Institute of Applied Economic and Social ResearchTo support the integration of data, the platform utilises next-generation Cloud-based noSQL systems (<rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-12">CouchDB</rs>) and associated data translation systems. We demonstrate the data integration capabilities through a representative case study.</p>
            <p>Socio-Economic Indexes for Areas (SEIFA) is a product developed by the ABS that ranks areas in Australia according to relative socioeconomic advantage and disadvantage. The indices are based on information from the 5 yearly Census. SEIFA 2011 is (at present) the latest version of this product and consists of four indices. The Index of Relative Socio-Economic Advantage and Disadvantage (IRSAD) summarises information about the economic and social conditions of people and households within an area, including both relative advantage and disadvantage measures. The <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-13">AURIN</rs> platform provides access to the IRSAD data from the ABSspecifically, the data classified across the SA2s across Australia.</p>
            <p>One typical scenario in using the IRSAD data is to identify regions facing particular challenges regarding disadvantage. Creation of a choropleth map through the <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-14">AURIN</rs> environment allows visualisation of the levels of advantage and disadvantage for all SA2s of Australia, as shown in <ref type="figure">Figure 4</ref>. As seen from <ref type="figure">Figure 4</ref>, there are areas of particular disadvantage in the Northern Territory, South Australia and northern Queensland. Here, the paler colour represents a lower SEIFA IRSAD score and the darker colour represents areas of higher scores. The <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-15">AURIN</rs> platform allows arbitrary selection of the classification scales. As seen from <ref type="figure">Figure 4</ref>, the table (lower left) shows the actual numeric values associated with the SEIFA IRSAD score.</p>
            <p>Understanding the issues surrounding SA2s with a lower IRSAD score can be explored through the <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-16">AURIN</rs> platform. Here, we consider the number of children in welfaredependent or low income families (earning less that $20,799), which is available from UA_PHIDU. Combining (merging) the SEIFA dataset and the UA_PHIDU dataset allows for a range of statistical analyses to be undertaken. Through selecting data at the SA2 level, these datasets can be combined directly (utilising the unique codes associated with the SA2s across Australia). Once merged, a range of analyses can be undertaken. Here, we consider a basic linear regression, although we note that the<rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-17">AURIN</rs> platform contains over 100 analytical tools that are available for richer geospatial analytics. These tools are outlined in <ref type="bibr">Sinnott and Voorsluys (2015)</ref>. <ref type="figure">Figure 5</ref> shows the correlation between the SEIFA IRSAD score and the number of lower income or welfare-dependent families in the SA2s across Australia. It is noted that the<rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-18">AURIN</rs> platform enforces intelligent decisionmaking where it can; for example, by checking that the datasets that are joined are at the same geospatial aggregation level (SA2). Outliers in these correlations (for example, areas with low SEIFA scores or areas with large numbers of children in welfare-dependent or low income families) can be seen directly. What is key here is that these are independent datasets from independent organisations that hitherto have not allowed direct comparison. Instead, researchers typically have to source these datasets themselves through ad hoc means and undertake such analysis using a range of tools.A further analysis that might be considered is the SEIFA IRSAD score and the percentage of 16 year olds in secondary college; that is, do we observe more people leaving school earlier in more disadvantaged areas? This dataset is also available from UA_PHIDU. Using the<rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-19">AURIN</rs> tools to merge the datasets and analyse the data, we see the correlation shown in <ref type="figure">Figure 6</ref>. The data points in the lower-left portion of the scatter plot represent those SA2s with a low SEIFA IRSAD score and lower school-leaving age. <ref type="figure">Figure 4</ref> shows that many of the disadvantages arise in areas with increased numbers of Indigenous persons. A further dimension to this analysis is to explore the correlation between the poor health of the Indigenous communities and the SEIFA IRSAD score.</p>
            <p>Information on the health of the Indigenous population is available from UC_NATSEM. The correlation between the SEIFA IRSAD score and poor-health information of the Indigenous population of Australia is shown in <ref type="figure">Figure 7</ref>. <ref type="figure">Figure 7</ref> also shows the interactive, user-oriented nature of the <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-20">AURIN</rs> portal. Thus, the outlier in the scatter plot is shown (in red) along with the location (East Arnhem) on the map (top). This interactivity of the underlying data is key to helping support the advanced visual analytics of the urban data landscape.</p>
            <p>The <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-21">AURIN</rs> platform supports rich spatial data analytics capabilities. One scenario of interest is whether areas of advantage and/or disadvantage tend to cluster together geospatially. As one example of a spatial <ref type="bibr">(Sinnott 2015)</ref>. Here, similarity of location is based upon the sharing of a boundary with a given SA2; that is, here we seek to explore whether SA2s that share a boundary with other SA2s share similar SEIFA scores.To support these analyses, it is necessary to spatialise (add the geometry) to the SA2s and calculate their contiguous spatial weights; that is, count SA2s if they share a border or a corner with other SA2s. The range of possible Moran's I values is between -1 and 1. An estimate of 0 implies no spatial autocorrelation exists. For a significant estimate, the closer it gets to 1, the greater the degree of positive spatial autocorrelation, while the closer it is to -1, the stronger the negative spatial autocorrelation.</p>
            <p>Applying the <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-22">AURIN</rs> statistical analysis tools to the 2011 SEIFA IRSAD scores for the SA2s of Australia produces the spatial autocorrelation scores shown in <ref type="figure">Figure 8</ref>. As shown, there is a slight correlation between the spatial correlation (0.1922) of the 2011 SEIFA IRSAD scores and the SA2s of Australia.</p>
            <p>Moran's I is just one of the more advanced spatial statistical correlations that are available in the <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-23">AURIN</rs> platform. These tools represent best practice in statistical analyses. Detailed documentation on all of these tools is available through the <rs cert="0.7" resp="#curator" subtype="used" type="software" xml:id="be2eaef0d4-software-5">AURIN</rs> website (&lt;<rs corresp="#be2eaef0d4-software-5" resp="#curator" type="url">http://docs.aurin. org.au</rs>&gt;), together with a range of advanced tutorials covering which analyses should beapplied in which context. Such capabilities allow more advanced analyses to be made available to non-spatial statistics experts.</p>
            <p>It is noted that whilst many researchers derive great benefit from the rich variety of tools that is available within the portal, some researchers prefer to simply have access to the <rs cert="0.7" resp="#curator" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-26">AURIN</rs> datasets and download them to their own desktop to undertake their own desktopbased analytics; for example, using tools such as <rs cert="1.0" resp="#annotator14" type="software" xml:id="be2eaef0d4-software-simple-27">ArcGIS</rs>, <rs cert="1.0" resp="#annotator14" type="software" xml:id="be2eaef0d4-software-simple-28">STATA</rs> or <rs cert="1.0" resp="#annotator14" type="software" xml:id="be2eaef0d4-software-simple-29">SPSS</rs>. The <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-30">AURIN</rs> portal supports the download of data in comma-separated-variable format, Javascript Object Notation and as Shapefiles. The platform also supports the notion of projects and a range of separate experiments can be supported.</p>
            <p>The platform also allows researchers to upload their own data and compare or analyse their datasets with other datasets that are made available through the <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-31">AURIN</rs> environment.</p>
            <p>In this article, we described the <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-32">AURIN</rs> e-infrastructure and its support for diverse data use/re-use and integration scenarios. The system currently supports seamless and secure access to data from, at present, 66 independent and typically definitive and autonomous data organisations across Australia, with over 1,661 unique datasets currently accessible. These cover unit-level datasets through to aggregated data across a multitude of scales (both temporal and spatial). These systems are driven by metadata with tools to support data harvesting and subsequent enrichment by data providers. The <rs cert="0.7" resp="#curator" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-33">AURIN</rs> model of data access, use/re-use and integration has already demonstrated its flexibility and scaleability, with new data sources being continually added with minimal impact on the underlying infrastructure.</p>
            <p>The work described here has primarily focused on the technical aspects of the e-infrastructure and exemplar uses. It should be noted that the systems have gained significant traction across the urban research landscape, and interestingly, in government, with many government agencies now accessing and using the systems for their own needs. There have been over 50,000 user sessions in accessing the <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-34">AURIN</rs> platform, with major acceptance by academic and non-academic users. There are over 3,000 regular users of the<rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-35">AURIN</rs> platform.</p>
            <p>The <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-36">AURIN</rs> platform is very much focused on urban and built environment research challenges, but it is broadly applicable to other domains of application. For example, it is now used as the basis for a range of other new projects including the Department of the Environment's $8.8 million Clean Air and Urban Landscapes initiative.</p>
            <p>This article has focused upon the openly accessible aggregate data; however, many urban researchers face considerable challenges in accessing disaggregated (unit-level) datasets. The <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-37">AURIN</rs> platform has capabilities for secure data access and usage. These are described in more detail in <ref type="bibr">Ma (2015)</ref>. Furthermore, many researchers require access to real-time information on the state of cities. Social media is one example of data that can capture what is happening in cities at any given time. Future work on <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-38">AURIN</rs> will include data from organisations such as Twitter.</p>
            <p>Future work on <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-39">AURIN</rs> involves the extension of the platform to include more datasets, and importantly, more recent datasets; for example, the 2016 Census. Work is also ongoing in supporting an open application programming interface for direct programmatic access to the data and tools within the platform. Initial prototyping of mobile applications has already commenced in both the <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-40">iOS</rs> and <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-41">Android</rs> platforms <ref type="bibr">(Moran 1950; Zhang 2015)</ref>.</p>
            <p>One of the challenges facing <rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-42">AURIN</rs> is its longer term sustainability. In 2015, the project faced challenges due to the economic uncertainty of the national funding landscape. As a result, certain commercial (fee-for-use) datasets, such as the Fairfax Australian Property Monitor housing data, had to be withdrawn due to lack of funds. These datasets were amongst the most widely accessed and used within the<rs cert="0.7" resp="#annotator14" subtype="used" type="software" xml:id="be2eaef0d4-software-simple-43">AURIN</rs> platform. Despite the obvious uptake of the platform by the research community and technical sophistication of the solution, such°C</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="b51c288efd">
            <titleStmt>
               <title>Mystery Shopping − The Tool of Employee Communication Skills Evaluation</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3846/btp.2011.05</idno>
                  <idno type="origin">10.3846%2Fbtp.2011.05</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Mystery shopping started back in the 1940s with financial institutions. A shopper would enter the banking establishment, make a cash deposit, and state that a receipt was not needed. Observations of how the transaction was processed or not processed were made. This type of assignment is called an integrity assignment and mainly deals with the potential act of theft. From this type of assignment, mystery shopping has developed into a rather large and unique industry covering areas from auto repair to fast food. In fact, some fast-food restaurants are mystery-shopped three times a day and shoppers are rotated <ref type="bibr">(Newhouse 2004</ref>: 2). Mystery shopping got its start as a way to check on employee integrity and minimize theft primarily in the financial services industry. In the 1990's, fuelled by the internet, the mystery shopping industry experienced rapid growth and acceptance. Into the 2000's, the creation of software packages such as <rs cert="1.0" resp="#annotator14" type="software" xml:id="b51c288efd-software-simple-0">SASSIE</rs> and <rs cert="1.0" resp="#annotator14" type="software" xml:id="b51c288efd-software-simple-1">Prophet</rs> have revolutionized the industry <ref type="bibr">(Michelson 1997, 2004: 6)</ref>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c333f36eeb">
            <titleStmt>
               <title>Formal and informal external linkages and firms’ innovative strategies. A cross-country comparison</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s00191-010-0188-y</idno>
                  <idno type="origin">10.1007%2Fs00191-010-0188-y</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The estimation method is a Multivariate Probit maximum likelihood in which the decisions to engage in formal and informal linkages with a set of external organizations are estimated simultaneously. 5 Formal or informal linkages data are binary but they have as many 'dimensions' as the number of external partner/ sources. The choices are not mutually exclusive. This method allows the simultaneous estimation of more than one binary probit equation with correlated disturbances. By allowing disturbances across equations to be freely correlated, the method allows to test for the correlation between dependent variables conditional on a certain number of common explanatory variables <ref type="bibr">(Galia and Legros 2004, p. 1193)</ref>, thus providing insights into the extent of complementary of substitution between them. A positive (negative) correlation of the error terms between two equations is taken as an indication of complementarity (substitution) between the two dependent variables.The data used in this paper come from the Community Innovation Survey 3 (CIS 3) that investigates the process of innovation development by firms in the period 1998-2000, in four European countries: The Netherlands, Norway, Sweden and the UK. The CIS survey asks firms about the type of innovation introduced in the 3 years preceding the survey, the sources of information they drew upon, their formal collaborative arrangements in order to innovate, as well as their expenditures and investments in several types of innovation activities. The process of innovation in services and in manufacturing has been 4 Fundamental-process activities include chemicals, plastic and rubber industries. Complexproducts include transport equipment. Product-engineering include machinery and equipment industries. Science-based activities include represented pharmaceutical and electrical and optical industries. Continuous-process includes all the other manufacturing activities. This is also the reference category. 5 More specifically, the estimation method employed here is based on the 'recursive conditioning simulator' implemented for <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="c333f36eeb-software-simple-0">STATA</rs> by <ref type="bibr">Cappellari and Jenkins (2003)</ref>. <ref type="bibr">6</ref> As a robustness check we have also performed for each country 12 separate Probit regressions one for each type of linkage. Results in terms of sign and significance of the estimators of multivariate and binary Probit are very similar. These results are available upon request from the corresponding author.Firm's largest market is international found to be quite different <ref type="bibr">(Miozzo and Soete 2001; Cainelli et al. 2006)</ref>. Therefore, the analysis undertaken in this paper focuses on manufacturing firms with more than nine employees. As the CIS survey does not collect data on the sources of information and collaborative arrangements for noninnovators, we cannot proceed, as we wished, with the analysis for the noninnovative firms. Thus all the firms in our sample are innovative in the sense that they have introduced at least one innovation in the period covered by the survey. Our sample includes a total of 3963 firms (1633 firms for The Netherlands, 1005 firms for the UK, 559 for Sweden, and 766 for Norway).</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="e776aa9e78">
            <titleStmt>
               <title>"The United States Congress and IMF financing, 1944-2009"</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s11558-011-9108-7</idno>
                  <idno type="origin">10.1007%2Fs11558-011-9108-7</idno>
               </bibl>
            </sourceDesc>
            <biblStruct/>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Model 6.3 moves away from the experiment to assess whether senators' length of tenure affects their voting behavior. Controlling for tenure length is important because senators may be more supportive of the IMF because they serve longer terms than representatives, rather than because they have larger constituencies. I assess this alternative institutional argument via variation among senators with respect to TIME TO ELECTION. This variable measures the number of years between an IMF roll-call vote and a senator's next election. It ranges from 0 to 5, with 0 indicating that a senator is up for reelection later in the same year as the roll-call vote, and 5 indicates that the senator has 5 years remaining until her next election. <ref type="bibr">19</ref> The evidence from Model 6.3 supports the "larger constituency" hypothesis: the point estimate for TIME TO ELECTION is <ref type="bibr">18</ref> Interpreting interaction terms in non-linear models is not straightforward. For example, the direction and significance of the effect may be different for different observations <ref type="bibr">(Ai and Norton 2003)</ref>. Using the inteff command in <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="e776aa9e78-software-simple-0">STATA</rs>, I calculated the interaction effect correctly <ref type="bibr">(Norton et al. 2004)</ref>. This revealed that the interaction term SENATE * REP-AT-LARGE is not statistically significant at any level but positive for all observations. 19 Senators are divided into three classes for purposes of elections and every 2 years the members of one class-approximately one-third of the Senate-face election or reelection. Data on the class and election dates of senators are from Swift et al., Database of Congressional Historical Statistics.Bill and vote information obtained from the "Voteview" and the Library of Congress "Thomas" websites</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="efb5f079e4">
            <titleStmt>
               <title>MIT Press Journals 2018 catalog</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1162/qjec.2005.120.1.87</idno>
                  <idno type="origin">10.1162%2Fqjec.2005.120.1.87</idno>
               </bibl>
            </sourceDesc>
            <biblStruct/>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The journal is hosted on an open-access, open-review, rapid publication platform called <rs cert="0.9" resp="#annotator3" type="software" xml:id="efb5f079e4-software-3">PubPub</rs>, created by students at the <rs corresp="#efb5f079e4-software-3" resp="#curator" type="publisher">Media Lab</rs>. <rs cert="0.9" resp="#annotator3" type="software" xml:id="efb5f079e4-software-simple-2">PubPub</rs> is a collaborative publication environment with rich commenting features, and powerful, intuitive authoring tools. JoDS articles are authored directly within the <rs cert="0.9" resp="#annotator3" type="software" xml:id="efb5f079e4-software-simple-3">PubPub</rs> environment, which provides support for multimedia, image integration, and large data sets.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="ddf560d942">
            <titleStmt>
               <title>Policy Watch: Developments in Antitrust Economics</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/jep.13.1.181</idno>
                  <idno type="origin">10.1257%2Fjep.13.1.181</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>In the 1995 Microsoft case, the Justice Department was seeking to protect competition in innovation as much as price competition. Protecting incentives for innovation is a common recent theme at the antitrust enforcement agencies. Another example occurred when two pharmaceutical producers, Ciba-Geigy and Sandoz, sought to merge in 1996. The FTC obtained a consent settlement requiring the merged firm (Novartis) to sell off one of the competing research and development efforts in genetic engineering to a third firm to protect competition in innovation. Moreover, in a new round of Microsoft litigation, the Justice Department is currently asking a court to determine whether Microsoft's integration of Internet browser software into <rs resp="#curator" type="software" xml:id="ddf560d942-software-simple-0">Windows</rs> harms innovation and tends to create or maintain monopoly by excluding rivals selling browsers such as <rs cert="1.0" resp="#annotator14" type="software" xml:id="ddf560d942-software-simple-1">Netscape</rs> (DOJ's view), or promotes innovation and competition by improving software design (Microsoft's position). <ref type="bibr">6</ref> The widely discussed possibility of network externalities-the benefit that users obtain when other customers use the same product, as with much computer software-raises the stakes for antitrust enforcers in discriminating between harmful and beneficial exclusionary practices. <ref type="bibr">7</ref> The possibility of network effects increases the efficiency benefits of having a single standard in the marketplace, but it may also entrench a firm and so allow it to exercise market power in a more damaging way. In consequence, antitrust enforcement decisions may be ineffective unless they are made early in an industry's development, before a standard becomes so secure as to make an antitrust remedy impractical. Yet it may be hard to make reliable judgments before uncertainties about the consequences of the challenged conduct have been fully resolved in the marketplace.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="e21c7da718">
            <titleStmt>
               <title>Trading and enforcing patent rights</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/1756-2171.12020</idno>
                  <idno type="origin">10.1111%2F1756-2171.12020</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Litigation data. The patent litigation data set was compiled by <ref type="bibr">Lanjouw and Schankerman (2001, 2004)</ref>. This data set matches litigated patents identified from the Lit-Alert database with information on the progress or resolution of suits from the court database organized by the Federal Judicial Center. The data set contains 14,169 patent cases filed during the period 1975-2000. For each of these case filings, the data set reports detailed information on the main patent litigated, the patentee, the infringer, and the court dealing with the case. The data set contains information on patent cases filed in U.S. federal district courts (and not on appeal). For each patent in our data, we identify the suits in which the patent was involved and the year in which the case was filed. 12 litigation denoted by θ i . Moreover, patent transactions characterized by large enforcement gains will reallocate the patent to owners with large θ i . Thus, in a generalized version of our model, we would expect to find that ex ante infringement β i is less likely when ex post enforcement gains are larger. <ref type="bibr">10</ref> We also dropped records in which the buyer and seller are the same entity and in which the execution date is either before the application date or after patent expiration. For additional details on the procedure, see <ref type="bibr">Serrano (2010)</ref>. <ref type="bibr">11</ref> Specifically, for each transfer between a seller i and buyer j, we identified all the patents that list the seller i as the (primary) inventor and checked whether any of these patents was assigned to the buyer j at its grant date. We drop all such transactions. <ref type="bibr">12</ref> The use of reassignment data as a proxy for activity in the market for innovation can be problematic, because technology can be transferred through patent licensing without changes in ownership. This concern is less relevant in Tax data. Information on state and federal income and capital gains taxes is obtained from the NBER Tax Rates database. This contains marginal income tax rates by year and state for a representative household with $500,000 of wage income. <ref type="bibr">13</ref> The data set also reports maximum federal and state long-term capital gains tax rates by year and state, computed using the NBER <rs cert="0.7" resp="#annotator2" subtype="used" type="software" xml:id="e21c7da718-software-simple-0">TAXSIM</rs> model. We obtain information on the maximum federal and state corporate marginal tax rates, for each year and state, from two government publications: the Significant Features of Fiscal Federalism, available for the <ref type="bibr">period 1982-1995 (American Council on Intergovernmental Relations, 1982-1995)</ref> and the Book of the States, for the period <ref type="bibr">1996-2000 (Council of State Governments, 1996-2000)</ref>. For each assigned patent in our data set, we use the ordinary income and capital gains marginal tax rates in the state of the initial patent assignee. For unassigned patents, we used the state of the primary inventor as identified by the USPTO. To measure tax rates faced by potential corporate buyers, we construct a weighted average of state corporate taxes where state weights are determined by the fraction of state patent applications in the technology class of the patent. 14 Matching data on income and capital gains taxes to patents is meaningful as long as the patent is owned by an individual at the time of the transaction. To ensure this, we focus our analysis on the first transfer of a patent. Subsequent owners are generally not individuals and thus are not subject to either personal income or capital gains taxation on the patent transaction. Focusing on the first transfer involves dropping very few patent trades. Most of the traded patents in our data are traded only once (94.9%), and only 0.15% of traded patents are traded more than three times.</p>
            <p>In principle, exploiting the information contained in the USPTO assignment data, it is possible to recover the patenting activity of the buyers in our sample. Unfortunately, the names of the buyer and seller in the Patent Assignment database were never standardized by the USPTO. Therefore, to back out buyer patent portfolios, we need to match each buyer name manually with a unique assignee identifier required to identify the buyer's patents. Because of the large size of our sample (17,605 traded patents), we manually matched only patents that were both traded and litigated at least once in their lifetime (569 patents). In the empirical analysis below, we will focus on regression results for the entire data set (299,356 patents), but also show that the findings also our study that focuses on patent litigation, because typically it is the owner of the patent who brings patent infringement actions. Nonexclusive licensees do not have the right to sue for patent infringement <ref type="bibr">(Textile Prods. v. Mead Corp., 134 F.3d 1481, 1485 Fed. Cir. 1998</ref>). An exclusive licensee may have standing to bring such a suit, but only under some restrictive contract arrangements <ref type="bibr">(Resonant Sensors Inc. v. SRU Biosystems, Inc., no. 3:08-cv-1978-M)</ref>. <ref type="bibr">13</ref> For details, see the description of the <rs cert="0.7" resp="#annotator2" subtype="used" type="software" xml:id="e21c7da718-software-1">TAXSIM</rs> program in <ref type="bibr">Feenberg and Coutts (1993)</ref>. The simulation and the resulting data are available at <rs corresp="#e21c7da718-software-1" resp="#curator" type="url">www.nber.org/∼taxsim/state-rates</rs>/. 14 All our results are robust to dropping corporate tax rates or to using corporate tax rates in the state of the inventor, which assumes that trading of patents occurs only within states. <ref type="table">Table 1</ref> reports summary statistics for the key variables. <ref type="table">Table 1</ref>A shows the fraction of sample patents involved in trade or litigation at least once in their life. Of the total sample, 4.55% of patents are traded and 0.69% are involved in at least one suit. These rates are low, but it is worth noting that, for the later patents in the sample, data on trade and litigation are truncated, and this biases downward litigation and trade rates. <ref type="bibr">16</ref> Moreover, patents that are traded or litigated are much more valuable than those that are not (as measured by citations received). <ref type="bibr">17</ref> The striking fact from this table is the strong association between trading and litigation. Of patents that are traded, 4.2% are also litigated; for patents that are not traded, the litigation rate is only 0.51%. Of patents that are litigated, 27.9% are also traded; for patents that are not litigated, only 4.4% are traded.</p>
            <p>Step 1. Regress each of the variables in the vector of covariates X on P using local linear regression. In our setting, this involves running multiple regressions. The regressions were run in <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="e21c7da718-software-5">STATA</rs> <rs corresp="#e21c7da718-software-5" resp="#annotator2" type="version">10</rs> using the command lpoly.</p>
            <p>The second part of the procedure involves numerically differentiating E[L| P]. To do so, we divide observations into groups, based either on the deciles of the distribution of P or the absolute value of P. Recall that the variable component of E[L| P] with respect to P is ε( P). The mean of ε( P) was calculated for each of these groups. The derivative of ε( P) was obtained by finite differencing across neighboring groups. The confidence intervals of the marginal treatment effects were obtained using 50 bootstrap iterations (seed = 123 in <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="e21c7da718-software-6">STATA</rs> <rs corresp="#e21c7da718-software-6" resp="#annotator2" type="version">10</rs>).Note: Capital gains tax rates for the period 1982-2001 are obtained from the NBER <rs cert="0.7" resp="#annotator2" subtype="used" type="software" xml:id="e21c7da718-software-simple-7">TAXSIM</rs> data set. Changes in tax rates are equal to the number of policy changes in the capital gains tax rate for the period 1982-2001. Tax Change is equal to the average tax change in percentage points (or growth rates) among all the changes in the capital gains tax rate in a state. U.S. states with zero tax changes in the state-level capital gains tax rate are states with no state level capital gains tax rate (e.g., Florida). The row labeled "average" is the average across U.S. states. U.S. states are ranked according to the number of individually owned patents granted in the state.Note: All regressions include age, period, and patent fixed effects. Standard errors in parentheses are clustered at the patent level. Statistical significance: * 10%, * * 5%, * * * 1%. NewOwner = 1 when the patent changes ownership for the first time and remains equal to one for the remaining life of the patent. Time period <ref type="bibr">dummies: before 1986, 1986-1990, 1991-995, and after 1995</ref>. P is the estimated probability of not being owned by the original inventor.Note: Standard errors in parentheses are clustered at patent level. All regressions include age, period, and patent fixed effects. Statistical significance: * 10%, * * 5%, * * * 1%. Litigation dummy = 1 if the patent is involved in at least one case at that age; NewOwner = 1 when the patent changes ownership for the first time, and remains equal to one for the remaining life of the patent. Time period <ref type="bibr">dummies: before 1986, 1986-1990, 1991-1995, after 1995.</ref> In columns 2-4, LargeBuyer = 1 if the acquirer obtained more than eight patents in the 20 years before trade. In columns 1 and 2, TechFit = 1 if acquired patent belongs to technology subcategory in which buyer has more patents. In column 3, TechFit constructed using USPTO patent n classes. In column 4, TechFit = 1 if either the acquired patent cites one of the patents of the buyer or if the patents of the buyer cite the acquired patent. NewOwner and its interactions are instrumented by the probit estimates of the probability of not being owned by the original inventor.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f3f11dc867">
            <titleStmt>
               <title>PASS-ADIAB – Linked Survey and Administrative Data for Research on Unemployment and Poverty</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1515/jbnst-2018-0002</idno>
                  <idno type="origin">10.1515%2Fjbnst-2018-0002</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The administrative data in PASS-ADIAB, whose structure is identical to that of the Sample of Integrated Labour Market Biographies (SIAB, see <ref type="bibr">Antoni et al. 2016</ref>), consist of a number of files. The main file is the individual file that contains the longitudinal labour market biographies of all linked respondents. In order to use the linked administrative data they have to be merged to the individual interviews from the PASS SUF prior to analysis. To do so, one merges records that belong to the same person identifier pnr, which is contained in the survey datasets as well as in the administrative individual file. This is also shown in the User Guide: the procedure applicable to administrative data is similar to using PASS's own biography dataset (<rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="f3f11dc867-software-simple-0">Stata</rs>: ibid, Example 9.4; <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="f3f11dc867-software-simple-1">SPSS</rs>: <ref type="bibr">Fuchs et al. 2015, Example 1.4)</ref>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="b7d1e4ecc5">
            <titleStmt>
               <title>The tax reform agenda</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1057/s11369-017-0056-y</idno>
                  <idno type="origin">10.1057%2Fs11369-017-0056-y</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>An alternative to eliminating a few deductions and exclusions would be to put an overall limit on the amount that a taxpayer can benefit from such tax expenditures. That might be politically easier to enact since no group of taxpayers would feel that they were being singled out to lose their deductions. A few years ago, Dan Feenberg, Maya MacGuineas, and I (2011) proposed such a plan, allowing each individual to benefit from the full range of tax expenditures but limiting the resulting tax reduction to two percent of that taxpayer's adjusted gross income. Simulations using the <rs corresp="#b7d1e4ecc5-software-0" resp="#curator" type="publisher">NBER</rs> <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="b7d1e4ecc5-software-0">TAXSIM</rs> model projected that such a cap would have raised $278 billion in 2011. A two percent cap would also cause substantial simplification by inducing more than 35 million taxpayers to shift from itemizing their deductions to using the standard deduction.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f48d9576bf">
            <titleStmt>
               <title>The Tragedy of the Anticommons in Knowledge</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1177/0486613415586992</idno>
                  <idno type="origin">10.1177%2F0486613415586992</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>It seems that although in the commons/anticommons context there is still underuse under single ownership when pervasive externalities are minor, in the anticommons context alone collecting rights into single ownership may eliminate the tragedy of the anticommons and therefore improve the situation if pervasive externalities are minor. However, even if concerns about the adequacy of collecting rights into single ownership as a response to the underuse problems caused by an anticommons could be overcome, other problems remain. In practice "it is easier to fragment than to reconvene fragmented property" <ref type="bibr">(Parisi, Schulz, and Depoorter 2004; Heller 2008: 213)</ref>. As Heller pointed out, collecting rights from an anticommons into usable private property bundles is often brutal and slow, and both markets and governments may fail to do so <ref type="bibr">(Heller 1998; Heller and Eisenberg 1998)</ref>. For example, "[l]and is much easier to break up than to put back together." Eminent domain is one way to unlock the value frozen in gridlocked land, "but it's a crude solution" <ref type="bibr">(Heller 2008: 110, 111)</ref>. 13 Obviously, it is even harder to solve the cross-border anticommons tragedy at the international level resulting from fragmented sovereignty, as Adam Smith observed in 1776 in The Wealth of Nations: "The navigation of the Danube is of very little use to the different states of Bavaria, Austria and Hungary, in comparison of what it would be if any of them possessed the whole of its course till it falls into the Black Sea"(Smith 1789: book 1, chapter 3, paragraph 8; <ref type="bibr">Buchanan and Yoon 2000; Heller 2008: 182, 183)</ref>. <ref type="bibr">14</ref> The above points seem also true of the anticommons in knowledge. Both markets and governments may find it difficult to dissolve barriers to knowledge production assembled under privatization. For example, one program can infringe many different software patents at once. <rs cert="1.0" resp="#annotator13" type="software" xml:id="f48d9576bf-software-simple-0">Linux</rs>, the kernel of the GNU/Linux operating system, may have infringed as many as 283 different U.S. software patents <ref type="bibr">(Stallman 2005)</ref>. A company that developed a treatment for Alzheimer's disease found it could not bring it to market unless it bought access to dozens of patents and "[a]ny single patent owner could demand a huge payoff; some blocked the whole deal" <ref type="bibr">(Heller 2008: xiii)</ref>. A final point to remember is that in this emerging new era of technocapitalism, 15 even if these formidable obstacles to consolidation can be overcome, the intellectual monopoly created will likely be detrimental to the diversity and dynamics of knowledge development, replacing the "tragedy" of the anticommons with the "Sargasso Sea" of monopoly.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="bd0016b986">
            <titleStmt>
               <title>Banned from the sharing economy: an agent-based model of a peer-to-peer marketplace for consumer goods and services</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s00191-017-0548-y</idno>
                  <idno type="PMC">PMC6096715</idno>
                  <idno type="PMID">30147242</idno>
                  <idno type="origin">10.1007%2Fs00191-017-0548-y</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Data availability statement The datasets generated during and/or analyzed during the current study are available from the corresponding author upon request. The simulation model is also available and can be run using <rs cert="1.0" resp="#annotator14" type="software" xml:id="bd0016b986-software-27">NetLogo</rs> <ref type="bibr">(Wilensky 1999)</ref>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c420aa559b">
            <titleStmt>
               <title>How Social Preferences Shape Incentives in (Experimental) Markets for Credence Goods</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/ecoj.12284</idno>
                  <idno type="PMC">PMC5347901</idno>
                  <idno type="PMID">28344358</idno>
                  <idno type="origin">10.1111%2Fecoj.12284</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Overcharging OverchargingThe term undertreatment refers to providing q 0 when the consumer needs q 1 ; overtreatment refers to providing q 1 when the consumer needs q 0 ; and overcharging refers to charging p 1 when q 0 has been provided. <ref type="bibr">6</ref> Dulleck et al. (2011) have a total of 16 experimental treatments (on the role of liability, verifiability, competition and reputation) of which we discuss only two here (and add two new ones). The main difference in experimental design between the new treatments and those in <ref type="bibr">Dulleck et al. (2011)</ref> is our reliance on (carefully designed) exogenously given prices for different qualities of the good rather than letting sellers endogenously decide on prices. To emphasise this difference, we refer to the treatments B/N and B/V in <ref type="bibr">Dulleck et al. (2011)</ref> as treatments N-Endo and V-Endo here, while the new treatments have names ending in -Exo. of customers and sellers within each matching group after each period. All experimental sessions were run computerised using <rs cert="1.0" resp="#annotator7" subtype="used" type="software" xml:id="c420aa559b-software-2">zTree</rs> <ref type="bibr">(Fischbacher, 2007)</ref> and recruiting was done via <rs cert="1.0" resp="#annotator7" subtype="used" type="software" xml:id="c420aa559b-software-3">ORSEE</rs> <ref type="bibr">(Greiner, 2004)</ref>. A total of 184 subjects participated in treatments N-Endo and V-Endo.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f140b3d501">
            <titleStmt>
               <title>MDY volume 18 issue 1 Cover and Front matter</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1017/s136510051300076x</idno>
                  <idno type="origin">10.1017%2Fs136510051300076x</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>An electronic version of the journal is published just prior to the paper version at journals.cambridge.org/mdy. Access to the full-text articles of the electronic version will be limited to institutional subscribers of the hard copy version of the journal, with access available to all online users at that institution's registered domain. Tables of Contents, abstracts, searching and alerting services will remain free of charge. To view the full text of Macroeconomic Dynamics, you will need to use the <rs corresp="#f140b3d501-software-0" resp="#curator" type="publisher">Adobe</rs> <rs cert="1.0" resp="#annotator2" type="software" xml:id="f140b3d501-software-0">Acrobat Reader</rs> software. If you do not have a copy already, you can get it free of charge. Just follow the link from our Cambridge Journals Online home page, download it, and install it as a plug-in helper application for your browser.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="e99746b1fa">
            <titleStmt>
               <title>Raising the bar (1)</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1080/17421772.2015.1126966</idno>
                  <idno type="origin">10.1080%2F17421772.2015.1126966</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Finally, nobody appears to have made software code available. The conclusion must be that J-tests have rarely been used in empirical research up to now and that the main bottleneck for practitioners is the lack of freely downloadable software. Hence, besides further developing J-tests, and stating that J-tests are a useful tool to select the right spatial weights matrix and/or spatial econometric model specification, we suggest that the researchers who have been active in this field may have even more impact if they make their software available. This would follow LeSage <ref type="formula">(2015)</ref>, who has made available code written in <rs cert="0.8" resp="#annotator2" type="software" xml:id="e99746b1fa-software-simple-0">Matlab</rs> relating to his Bayesian comparison approach.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="ae9bb960ec">
            <titleStmt>
               <title>The impacts of fundraising periods and geographic distance on financing music production via crowdfunding in Brazil</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s10824-015-9248-3</idno>
                  <idno type="origin">10.1007%2Fs10824-015-9248-3</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The geographic distance between the entrepreneurs and the donors was obtained for the cities where they were located, and we used <rs resp="#curator" type="software" xml:id="ae9bb960ec-software-0">Google Maps APIs</rs> (<rs corresp="#ae9bb960ec-software-0" resp="#curator" type="url">http://code.google.com/apis/maps/</rs>) to identify the coordinates of each location. The distance was calculated using the procedure adopted by <ref type="bibr">Nichols (2003)</ref> with the tool <rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="ae9bb960ec-software-1">ArcGIS ArcMap</rs> <rs corresp="#ae9bb960ec-software-1" resp="#annotator4" type="version">10.0</rs> (<rs corresp="#ae9bb960ec-software-1" resp="#curator" type="publisher">ESRI</rs> 2010). The motivation for using these types of tools comes from the argument that when the data's spatial distribution is related to the phenomena occurring in the territory, one has a modern way to clarify questions related to various fields of knowledge <ref type="bibr">(Ebner et al. 2009</ref>). The use of these techniques in the business field is in an embryonic stage (Goodchild 2010), especially in studies of collaborative phenomena <ref type="bibr">(Sui et al. 2013)</ref>. In addition, geographical information systems (GISs) could be a tool toward providing a spatial explanation of governance, population and cultural assets <ref type="bibr">(Redaelli 2012)</ref>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="fadbc08608">
            <titleStmt>
               <title>Can interest rate spreads stabilize the euro area?</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1080/00036846.2015.1021547</idno>
                  <idno type="origin">10.1080%2F00036846.2015.1021547</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>In the next step, we explain the choice of our preferred model. As expected, OLS estimates of parameter ρ are somehow larger than those of FE. However, reported point estimates of parameters using bias-corrected FE (we employed <ref type="bibr">Bruno, 2005</ref> <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="fadbc08608-software-1">Stata</rs> routine) are almost the same as the FE estimates, which suggest that the Nickell bias seems negligible in our context (and that OLS estimates suffer from omitted variable bias). Also the significance of country dummies suggests that OLS estimates may not be the most reliable ones. Finally, it should be noted that the FE estimates have a relatively small variance, especially if compared to consistent estimators as GMM <ref type="bibr">(Mátyás and Sevestre, 2008)</ref> or bias-corrected FE. For these reasons, we choose the simple FE estimator as the preferred one. Regarding the specification tests, the Arellano-Bond test for autocorrelation shows that including the lagged dependent variable and year dummies reduces problems with autocorrelation. At no reasonable significance level, the null hypothesis (i.e. that there is no autocorrelation) should be rejected. To deal with heterogeneity we use robust SEs.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d1ad8ca2d7">
            <titleStmt>
               <title>INFORMATION ACQUISITION UNDER RISKY CONDITIONS ACROSS REAL AND HYPOTHETICAL SETTINGS</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/ecin.12386</idno>
                  <idno type="origin">10.1111%2Fecin.12386</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>I collect data on how subjects acquire information about risky choices in both real and hypothetical settings using process-tracing software called <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d1ad8ca2d7-software-simple-0">Mouselab</rs>. On average, there are no significant differences across settings in the amount of time subjects take to make a choice or the completeness of the information they acquire. Subjects also acquire information in sequences consistent with an integration model of decision-making, such as expected utility theory or prospect theory. I do not find significant differences in risk preferences across settings, on average, but I do find that subjects' risk preferences are related to the completeness of the information that they acquire and where they start their information acquisition. <ref type="bibr">(JEL C91, D80, D83)</ref> <ref type="bibr">(Levitt and List 2007)</ref> <ref type="bibr">Kahneman and Tversky (1979, 265)</ref> <ref type="bibr">(Battalio, Kagel, and Jiranyakul 1990; Harrison 2006; Holt and Laury 2002, 2005)</ref> <ref type="bibr">(Beattie and Loomes 1997; Kühberger, Schulte-Mecklenbeck, and Perner 2002; Wiseman and Levin 1996)</ref> <ref type="bibr">Taylor (2013)</ref> cognitive ability tend to make choices exhibiting hypothetical bias. <ref type="bibr">2</ref> These contradictory results have yet to be explained. <ref type="bibr">Camerer and Hogarth (1999)</ref> illustrate that real incentives are likely to alter behavior when the task involved depends on the subject's effort level. But the choices that are typically used to measure risk aversion are designed to measure preferences, not effort. Moreover, while it is clear how economic incentives can cause individuals to misrepresent their willingness to pay for clean air in a survey that could influence environmental policy, there is not an obvious economic motivation for why individuals would systematically misrepresent their risk preferences so that they appear more risk tolerant when they make choices with hypothetical consequences. <ref type="bibr">3</ref> This study uses a method of process tracing to explore whether individuals acquire information about the risky choices in the commonly used Holt and Laury multiple-price list (HL MPL) differently when the choices have hypothetical monetary consequences rather than real monetary consequences <ref type="bibr">(Holt and Laury 2002)</ref>. It is possible that individuals may attend to the information in hypothetical choices differently than real choices, and several studies have demonstrated that how individuals acquire information can be related to their decision-making. For instance, <ref type="bibr">Costa-Gomes and Crawford (2006)</ref> show that simplified searches are correlated with deviations from equilibrium in a two-person guessing game, <ref type="bibr">and Caplin, Dean, and Martin (2011)</ref> find that search patterns can affect subjects' decisions conditional on choice complexity.</p>
            <p>There is no doubt that subjects are willing to devote at least some time to the consideration of hypothetical choices. Psychologists frequently rely exclusively on hypothetical decisions to study behavior. Some of these studies have used <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d1ad8ca2d7-software-simple-1">Mouselab</rs> or other process-tracing techniques, and they demonstrate that individuals do spend time considering hypothetical choices <ref type="bibr">(Willemsen and Johnson 2010)</ref>. The question of interest here, however, is whether information 2. <ref type="bibr">Taylor (2013)</ref> focuses on the relationship between cognitive ability, risk aversion, and incentives using data from this same experiment.</p>
            <p>More recent studies that have explored this question employed tasks and choices more akin to the types of choices typically used by economists to measure risk preferences <ref type="bibr">(Arieli, Ben-Ami, and Rubinstein 2011; Glöckner and Herbold 2011; Johnson, Schulte-Mecklenbeck, and Willem- sen 2008)</ref>. <ref type="bibr">5</ref> Interestingly, these studies have reached contradictory conclusions. For example, <ref type="bibr">Johnson, Schulte-Mecklenbeck, and Willemsen (2008)</ref> and <ref type="bibr">Glöckner and Herbold (2011)</ref> find information acquisition patterns consistent with integration models, but Arieli, Ben-Ami, and Rubinstein (2011) do not. The conflicting results are not due to differences in how the process data were collected. All three studies used hypothetical payoffs, and <ref type="bibr">Glöckner and Herbold (2011)</ref> and <ref type="bibr">Arieli, Ben-Ami, and Rubinstein (2011)</ref> used eye-tracking technology, while <ref type="bibr">Johnson, Schulte-Mecklenbeck, and Willemsen (2008)</ref> used <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d1ad8ca2d7-software-simple-2">Mouselab</rs>.</p>
            <p>The following section briefly describes the method used to observe subject information acquisition, as well as provides a detailed explanation of the experimental design and the 5. <ref type="bibr">Glöckner and Betsch (2008)</ref> used a method that they call the "open <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d1ad8ca2d7-software-simple-3">mouselab</rs> method," which does not actually mask the information and, instead, asked a subject to move the mouse to the piece of information about which she is thinking. They find that information search is more consistent with an integrative model using this method.</p>
            <p>I use a software program called <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d1ad8ca2d7-software-simple-4">Mouselab</rs> to observe how subjects acquire information about risky choices. <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d1ad8ca2d7-software-simple-5">Mouselab</rs> is a method of processtracing used by psychologists for nearly three decades, and economists more recently, because it is lower-cost alternative to eye-tracking and provides analogous measures <ref type="bibr">(Costa-Gomes and Crawford 2006; Costa-Gomes, Crawford, and Broseta 2001; Gabaix et al. 2006; Johnson et al. 2002; Payne, Bettman, and Johnson 1988; Reutskaja et al. 2011)</ref>. The software enables the researcher to "mask" selected information so that a subject must move the computer pointer over the attribute to reveal it. In this experiment, the probabilities and payoffs (the attributes) of each gamble were masked and the individual was required to "unmask" the attributes by scrolling over the cell to view it. Once the mouse pointer leaves the cell, the attribute is hidden again. By requiring a subject to unmask the gambles' attributes and allowing them to view only one piece of information at a time, individuals' strategies for acquiring information about risky choices is observable in this stylized context. <ref type="bibr">6</ref> An example of how each pair of gambles was presented to subjects in the experiment is shown in <ref type="figure">Figure 1</ref>. 7</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="a5fd1edbe6">
            <titleStmt>
               <title>Are Online and Offline Prices Similar? Evidence from Large Multi-Channel Retailers</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/aer.20160542</idno>
                  <idno type="origin">10.1257%2Faer.20160542</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The data collection effort is unprecedented in scope and size, and was carried out as part of the BPP. I first selected the retailers to be sampled by focusing on the top 20 companies by market shares in each country that sell both online and offline (" multi-channel"), and have product barcodes that can be matched across samples. Next, I used crowdsourcing platforms such as <rs corresp="#a5fd1edbe6-software-1" resp="#curator" type="publisher">Amazon</rs> <rs cert="0.5" resp="#annotator14" subtype="used" type="software" xml:id="a5fd1edbe6-software-1">Mechanical Turk</rs>, <rs cert="0.5" resp="#annotator14" subtype="used" type="software" xml:id="a5fd1edbe6-software-simple-2">Elance</rs>, and <rs cert="0.5" resp="#annotator14" subtype="used" type="software" xml:id="a5fd1edbe6-software-simple-3">UpWork</rs> to hire 323 workers to collect the offline data. Each worker was assigned a simple task: to scan the barcodes and collect prices for a random set of 10 to 50 products in any physical store of a given retailer. In some cases they had to return to the same store multiple times to scan the same set of products. Using a special app for android phones developed to simplify and standardize the data collection process, these workers scanned each product's barcode, manually entered the price, took a photo of the price tag, and sent all the information via e-mail to the BPP servers, where it was automatically processed and cleaned. A scraping software then used the barcode numbers to look for the same product in the website of each retailer, and collected the online price within a period of seven days. The matched online-offline dataset contains prices for more than 24,000 products and 38,000 observations sampled between December 2014 and March 2016.</p>
            <p>Collecting prices offline is normally an expensive and complicated process. NSOs rely on a large number of trained data collectors to do it correctly. Unfortunately, the micro data collected by NSOs for CPI purposes cannot be used for my comparisons because the retailer and product details are confidential information. Lacking the budget for a traditional data collection effort, I looked for alternatives using new technologies. In particular, I relied on popular crowdsourcing platforms, such as <rs corresp="#a5fd1edbe6-software-7" resp="#curator" type="publisher">Amazon</rs> <rs cert="0.5" resp="#annotator14" subtype="used" type="software" xml:id="a5fd1edbe6-software-7">Mechanical Turk</rs>, <rs cert="0.5" resp="#annotator14" subtype="used" type="software" xml:id="a5fd1edbe6-software-simple-6">Elance</rs>, and <rs cert="0.5" resp="#annotator14" subtype="used" type="software" xml:id="a5fd1edbe6-software-simple-7">UpWork</rs>, to find people willing to do simple data collection tasks. To minimize the chance of data-entry errors, I developed a custom mobile phone app that simplified the data collection process.</p>
            <p>The mobile app was custom-built to simplify and standardize the data collection process. It is an app for android phones called "<rs cert="1.0" resp="#annotator14" type="software" xml:id="a5fd1edbe6-software-simple-8">BPP @ MIT</rs>," available for download at the Google Play Store. <ref type="bibr">6</ref> Every time a worker visits a store, she clicks on a button to open a new file. For the first product, she has to enter the store's name, zip code, and country. Then she scans the UPC barcode of the product (or the barcode on the price tag, depending on the particular retailer instructions provided), manually enters the price shown in the price tag next to the product (including all sales), marks the price as "regular" or "sale," and takes a photograph of the price tag (which is used to detect errors and validate the data). All products are scanned in a loop which makes the process quick and simple. When done, the worker taps an icon to e-mail the data to the BPP servers. A member of the BPP team verifies the submitted data and pays the worker. 5 I tried to conduct a similar large-scale offline data collection with MIT students in the Boston area in 2011, but most of them were asked to stop and leave the stores after some time. Collecting data this way appears to be easier now that more people use smartphones inside stores. Indeed, <ref type="bibr">Fitzgerald (2013)</ref> reports that the fear of showrooming has faded for many US retailers. See <ref type="bibr">Balakrishnan, Sundaresan, and Zhang (2013)</ref> for an economic analysis of showrooming practices. 6 See https://play.google.com/store/apps/details?id=com.mit.bpp. The app can be downloaded for free, but a "project code" must be requested from the BPP team. This code is used to separate the data from different projects. See http://bpp.mit.edu/ offline-data-collection/ for more details.Notes: These retailers satisfy three conditions. First, they are in the list of top 20 retailers by market share in their respective countries according to Euromonitor International. Second, they sell both online through a country-specific website and offline through physical stores. Third, there is a way to perfectly match products online and offline for the price comparison. See the online Appendix for more detailed characteristics and results.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d622f7bf55">
            <titleStmt>
               <title>The Size and Composition of Government Spending in Europe and Its Impact on Well-Being</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/j.1467-6435.2010.00478.x</idno>
                  <idno type="origin">10.1111%2Fj.1467-6435.2010.00478.x</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Secondly, the marginal effects that we are interested in refer to the interaction terms at the top of table 1. However, the calculation of marginal effects in the context of nonlinear estimations with interaction terms is much more difficult than assumed by many researchers. In this context, <ref type="bibr">Ai and Norton (2003)</ref> have identified 72 articles published between 1980 and 1999 in the economics journals listed on JSTOR that use interaction terms in nonlinear models. However, none of them provides a correct interpretation of the interaction term's marginal effect. In fact, the reported results often diverge strongly from the true results. As <ref type="bibr">Ai and Norton (2003)</ref> point out, these marginal effects are not calculated by standard statistical software packages such as <rs cert="1.0" resp="#annotator14" type="software" xml:id="d622f7bf55-software-simple-0">Stata</rs>. <ref type="bibr">15</ref> To conclude, hypothesis 3 stating that government size has a more positive impact on well-being with high expenditure decentralization and low corruption cannot be rejected. Moreover, the hypothesis that government size has a more positive effect on well-being for left-wing voters cannot be rejected (first 15. Ai and Norton have in the meantime made available the inteff module which does calculate these marginal effects for the binary case. For the ordered response case, no such module has become available yet <ref type="bibr">(Norton et al. 2004</ref>).</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c11972c006">
            <titleStmt>
               <title>Applied Spatial Econometrics: Raising the Bar</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1080/17421770903541772</idno>
                  <idno type="origin">10.1080%2F17421770903541772</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The opposite situation occurs when all cross-sectional units are assumed to be neighbours of each other and are given equal weights. In that case all off-diagonal elements of the spatial weights matrix are w ij 01. Since the row and column sums are N Á 1, these sums diverge to infinity as N goes to infinity. In contrast to the previous case, however, (N Á 1)/N01 instead of 0 as N goes to infinity. This implies that a spatial weights matrix that has equal weights and that is row normalized subsequently, w ij 01/(N(1), must be excluded for reasons of consistency. <ref type="figure">Figure 1</ref> summarizes a family of eight linear spatial econometric models, among which are the non-spatial model in (1) on the right-hand side and the Manski model in (2) on the left-hand side. Each model to the right of the Manski model can be obtained from that model by imposing restrictions on one or more of its parameters. The kinds of restrictions are reported alongside the arrows in <ref type="figure">Figure 1</ref>.Some of the models recorded in <ref type="figure">Figure 1</ref> are well known and frequently used in applied research, while other models are not. LeSage &amp; Pace (p. 32) denote the model with a spatially lagged dependent variable (WY) and a spatially autocorrelated error term (Wo) by the term SAC, though without pointing out what this acronym is standing for. Since <ref type="bibr">Kelejian &amp; Prucha (1998)</ref> have been the main advocates of this model, it is therefore renamed the KelejianÁPrucha model in this paper. The model with a spatially lagged dependent variable (WY) and spatially lagged independent variables (WX) has been introduced by <ref type="bibr">Anselin (1988)</ref> and is labelled the spatial Durbin model. A model with spatially lagged independent variables (WX) and a spatially autocorrelated error term has hardly been used in the literature. LeSage &amp; Pace (pp. 41Á42) label it the spatial Durbin error model. <ref type="figure">Figure 1</ref> seems to indicate that the best strategy to test for spatial interaction effects is to start with the most general model, i.e. the model that includes a spatially lagged dependent variable, spatially lagged independent variables, and a spatially autocorrelated error term simultaneously. However, as <ref type="bibr">Manski (1993)</ref> notes, at least one of the K'2 interaction effects must be excluded, because otherwise the parameters are unidentified. To verify this, I carried out a simple Monte Carlo experiment generating Y by (2a) and (2b), one X variable drawn from a uniform distribution on the interval [Á1,1], r 0 a 0 b 0 u 0 l 0 0:25; s 2 00.01, N060, and a spatial weights matrix W corresponding to the corners of the seams in a soccer ball (in <rs cert="1.0" resp="#annotator3" type="software" xml:id="c11972c006-software-simple-0">Matlab</rs> known as the Bucky Ball). Based on 1,000 repetitions, I found biases in the parameter estimates of the endogenous and exogenous interaction effects (r and u, respectively) that may be as great as 0.0423 and, related to that, standard deviations that may be as great as 0.2677. These results corroborate <ref type="bibr">Manski's (1993, p. 534)</ref> finding that there are no technical obstacles to estimating a model with interaction effects among the dependent variable, the independent variables and the disturbance terms, but that the parameter estimates cannot be interpreted in a meaningful way since the endogenous and exogenous effects cannot be distinguished from each other.</p>
            <p>One of the merits of LeSage &amp; Pace's book is that they offer another criterion to select models, namely the Bayesian posterior model probability. Whereas tests for significant differences between log-likelihood function values, such as the LRtest, can formally not be used if models are non-nested (i.e. based on different spatial weights matrices), Bayesian posterior model probabilities do not require nested models to carry out these comparisons (p. 162). The basic idea is to set prior probabilities equal to 1/S, making each model equally likely a priori, to estimate each model by Bayesian methods, and then to compute posterior probabilities based on the data and the estimation results of this set of S models. Although the mathematics of this approach might deter potential users (Chs 5 and 6), my experience with this approach is positive. First, posterior model probabilities may differ widely even if the estimation results appear to be quite robust to different specifications of the spatial weights matrix. In a study I did on cross-country differences in governance <ref type="bibr">(Seldadyo et al., 2010)</ref>, the posterior model probability of a 10 nearest-neighbour matrix appeared to be more than six times as large as that of an inverse distance matrix, more than three times as large as that of a five nearestneighbour matrix, and more than twice as large as an inverse distance matrix with a cut-off point. Second, since <rs cert="1.0" resp="#annotator3" type="software" xml:id="c11972c006-software-simple-1">Matlab</rs> <rs resp="#curator" type="software" xml:id="c11972c006-software-0">routines</rs> applying Bayesian methods to the spatial lag, spatial error and spatial Durbin models are made downloadable for free on <rs corresp="#c11972c006-software-0" resp="#curator" subtype="person" type="publisher">LeSage</rs>'s website (<rs corresp="#c11972c006-software-0" resp="#annotator3" type="url">www.spatial-econometrics.com</rs>), these kinds of comparisons can be carried out relatively easily. Furthermore, since <rs corresp="#c11972c006-software-1" resp="#curator" subtype="person" type="publisher">LeSage</rs> also provides <rs cert="1.0" resp="#annotator3" type="software" xml:id="c11972c006-software-simple-6">Matlab</rs> <rs resp="#curator" type="software" xml:id="c11972c006-software-1">routines</rs> applying the Bayesian method to the spatial lag, spatial error and spatial Durbin models of limited dependent variables, similar types of selection procedures as discussed in this and the previous sections can be used for empirical problems requiring a probit or tobit approach (Ch. 10).</p>
            <p>One possible way to calculate the dispersion of the direct and indirect effects is to apply formulae for the sum, the difference, the product and the quotient of random variables (see, among others, <ref type="bibr">Mood et al., 1974, pp. 178Á181)</ref>. However, owing to the complexity of the matrix of partial derivatives [see <ref type="bibr">(6)</ref>] and because every empirical application will have its own unique number of observations (N) and spatial weights matrix (W), it is almost impossible to derive one general approach that can be applied under all circumstances. In order to draw inferences regarding the statistical significance of the direct and indirect effects, LeSage &amp; Pace (p. 39) therefore suggest simulating the distribution of the direct and indirect effects using the varianceÁcovariance matrix implied by the maximum likelihood estimates. <ref type="bibr">Elhorst &amp; Fréret (2009)</ref> Using the <rs cert="1.0" resp="#annotator3" type="software" xml:id="c11972c006-software-simple-8">Matlab</rs> <rs resp="#curator" type="software" xml:id="c11972c006-software-2">routine 'sar'</rs> posted on <rs corresp="#c11972c006-software-2" resp="#curator" subtype="person" type="publisher">LeSage</rs>'s website (<rs corresp="#c11972c006-software-2" resp="#curator" type="url">www.spatial- econometrics.com</rs>), one particular parameter combination drawn from this varianceÁcovariance matrix (indexed by d ) can be obtained by:</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c42c67a440">
            <titleStmt>
               <title>The Establishment History Panel – Redesign and Update 2016</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1515/jbnst-2016-1001</idno>
                  <idno type="origin">10.1515%2Fjbnst-2016-1001</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Access to the BHP is only possible via on-site use at the FDZ and remote data processing. For on-site use, the FDZ provides several workplaces for visiting researchers within a secure computing environment at different locations in Germany and abroad <ref type="bibr">(Bender/Heining 2011). 6</ref> For remote data processing, researchers submit scripts to the FDZ that execute commands for data preparation and analysis. The scripts are processed on secure servers within the FDZ and the results are transferred to the researchers after a disclosure review. Remote data processing at the FDZ is conducted via the <rs resp="#curator" type="software" xml:id="c42c67a440-software-1">Job Submission Application (JoSuA)</rs> developed by the <rs corresp="#c42c67a440-software-1" resp="#curator" type="publisher">Institute for the Study of Labour (IZA)</rs>, which allows users to submit jobs and access their results via a custom-built web interface (see <ref type="bibr">Eberle et al. forthcoming)</ref>. Researchers intending to use the BHP are required to apply for data access via on-site use and remote data processing at the FDZ. Access can be granted to non-commercial research institutions conducting research projects in the fields of employment research.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="fc5393437f">
            <titleStmt>
               <title>The Bootstrap and Multiple Imputations: Harnessing Increased Computing Power for Improved Statistical Tests</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/jep.15.4.129</idno>
                  <idno type="origin">10.1257%2Fjep.15.4.129</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Despite these caveats and inherent uncertainty about the superiority of the bootstrap in specific settings, the availability of powerful desktop computers makes widespread use of the bootstrap very tempting. Bootstrap estimates-especially based on the paired approach-are trivial to obtain once the estimation model is specified. The statistical package <rs cert="1.0" resp="#annotator13" type="software" xml:id="fc5393437f-software-simple-0">Stata</rs> has several bootstrap procedures and subroutines available in its core program, and a researcher who has never read a single <ref type="bibr">5</ref> Other useful cross-section applications, surveyed in Brownstone and Kazimi (2000), include quantile regression, discrete choice and hazard models, and frontier production function estimates. <ref type="bibr">6</ref> <ref type="bibr">Bickel and Freedman (1983)</ref> provided an early example of model-based bootstrapping for time series. Their approach has been generalized to autoregressive moving average models (Berkowitz and <ref type="bibr">Kilian, 2000; Li and Maddala, 1996)</ref> and cointegrated regression models <ref type="bibr">(Li and Maddala, 1997)</ref>. Kilian (1998a, b) considers bootstrapping vector autoregression models and applies this technique to examine international effects of monetary policy. word about the bootstrap could produce bootstrapped estimates in <rs cert="0.9" resp="#annotator13" type="software" xml:id="fc5393437f-software-simple-1">Stata</rs> with little difficulty. Researchers should be cautious about such casual application, because results regarding the reliability and accuracy of bootstrapped sampling distributions are not yet available for many classes of econometric models. However, while we await further results, the bootstrap can be very useful for simple applications such as estimating the sampling distributions of nonparametric statistics (for example, quantiles and percentiles of univariate distributions) or statistics with illdefined distributions (see Valletta, 1993, footnote 13, for an example).</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="e46553a13f">
            <titleStmt>
               <title>MIT Press Journals 2018 catalog</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1162/qjec.121.4.1383</idno>
                  <idno type="origin">10.1162%2Fqjec.121.4.1383</idno>
               </bibl>
            </sourceDesc>
            <biblStruct/>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>• In-depth articles on cutting-edge research and developments in technology, methods, and aesthetics of computer music • Reports on products of interest, such as new audio and <rs cert="0.6" resp="#annotator2" type="software" xml:id="e46553a13f-software-simple-0">MIDI</rs> software and hardware • Interviews with leading composers of computer music • Announcements of and reports on conferences and courses in the United States and abroad • Publication, event, and recording reviews • Tutorials, letters, and editorials • Numerous graphics, photographs, scores, algorithms, and other illustrations. MIT Press Journals offers CMJ and the annual publication, Leonardo Music Journal (see p. 22), at a special subscription price; see details below.</p>
            <p>The journal is hosted on an open-access, open-review, rapid publication platform called <rs cert="0.7" resp="#annotator2" type="software" xml:id="e46553a13f-software-1">PubPub</rs>, created by students at the <rs corresp="#e46553a13f-software-1" resp="#curator" type="publisher">Media Lab</rs>. <rs cert="0.7" resp="#annotator2" type="software" xml:id="e46553a13f-software-simple-3">PubPub</rs> is a collaborative publication environment with rich commenting features, and powerful, intuitive authoring tools. JoDS articles are authored directly within the <rs cert="0.7" resp="#annotator2" type="software" xml:id="e46553a13f-software-simple-4">PubPub</rs> environment, which provides support for multimedia, image integration, and large data sets.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d736202c66">
            <titleStmt>
               <title>Women's employment, children and transition An empirical analysis for Poland1</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/j.1468-0351.2009.00346.x</idno>
                  <idno type="origin">10.1111%2Fj.1468-0351.2009.00346.x</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Estimation by maximum likelihood of both models can be automatically implemented in <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d736202c66-software-simple-0">STATA</rs>, although the maximization of the likelihood function of the second model requires the evaluation of an integral by Gaussian Quadrature <ref type="bibr">(Butler and Moffitt, 1982</ref>). The first model ignores individual heterogeneity and does not exploit the repeated observation available on the same woman. The inference requires the use of a robust covariance matrix to take into account the error serial correlation induced by the unobserved individual component not varying with time.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="fc5c36172d">
            <titleStmt>
               <title>Liar Liar: Experimental Evidence of the Effect of Confirmation-Reports on Dishonesty</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1002/soej.12244</idno>
                  <idno type="origin">10.1002%2Fsoej.12244</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Prepopulated tax forms are used in Finland, Denmark, Sweden, Norway. The United Kingdom is currently considering the adoption of prepopulated tax forms. Although there has been discussions about the merits of a prepopulated tax form in the United States, the State of California is the only U.S. government to have implemented them under their ready-return initiative. <rs cert="1.0" resp="#curator" type="software" xml:id="fc5c36172d-software-simple-0">ReadyReturn</rs> has since been cancelled, but its primary feature-prepopulating tax forms with pre-existing information-is still available with <rs cert="1.0" resp="#annotator14" type="software" xml:id="fc5c36172d-software-simple-1">CalFile</rs>. 7 Of course, this strategy does not preclude the possibility of increased noncompliance among taxpayers who have only verifiable income in the current period, but earn income from nonverifiable sources in the future. Confirmation-reports could also increase noncompliance by inducing taxpayers to reduce their verifiable income and increase their nonverifiable income. 8 Notice that the payment for rolling a six is zero as in <ref type="bibr">Fischbacher and F€ ollmi-Heusi (2013)</ref>. Because the pay-off for rolling a six is zero, we recode the outcome six to be zero. This adjustment is made to the data for all tables and figures reported in the article. This design conforms with the previous literature; for example, <ref type="bibr">Fischbacher and F€ ollmi- Heusi (2013)</ref>; <ref type="bibr">Ruffle and Tobol (2014)</ref>. The main difference between our design and that of the existing literature is in the way subjects are asked to report the outcome of the die roll. The previous literature simply asked subjects to self-report the outcome of the die roll. We add to this by also asking some subjects to provide confirmation-reports. Subjects are randomly assigned to one of seven groups; one self-report treatment and six confirmation-report treatments. The only difference between the self-report treatment and the confirmation-report treatment is in step two where subjects are asked to report the die roll outcome.</p>
            <p>10 Following the survey, subjects are asked to complete a bonus task, which involves rolling a six-sided die and reporting the outcome. Subjects receive task instructions shown in Figures 4 and 5. They are told that the amount of the bonus payment is determined by the reported die roll according to <ref type="table">Table 1</ref>. A link to the website random.org, which allows people to roll virtual die, is embedded into our website. Subjects are instructed to click on the link in order to roll the die, and to return to our website to report the outcome of the die roll. We make it explicitly clear that our website is not affiliated with random.org, so theSubjects took an average of nine minutes to complete both stages of the experiment, and 50% of subjects completed the experiment in less than 5 minutes and 20 seconds. <ref type="bibr">10</ref> We created the external website for the sole purpose of hosting the survey and bonus task. The survey includes questions about road mileage user-fees, number of miles driven, age, gender, race, and education. Subjects are paid a flat participation fee for completing the survey. The experiment was done in December 2015 and January of 2016. actual outcome of the die roll is not known to us. Subjects are told that we can only observe the number that they report on our website. Subjects are shown an ID code after submitting their reported die roll, and are told to report this code on the <rs cert="1.0" resp="#annotator14" type="software" xml:id="fc5c36172d-software-simple-2">Mturk</rs> website in order for us to process their bonus payment.</p>
            <p><rs cert="1.0" resp="#annotator14" type="software" xml:id="fc5c36172d-software-simple-3">Mturk</rs> is especially advantageous for our study because subjects can be assured that we have no way of telling whether they are lying or not. Subjects complete the tasks in their own environment immune to the influence from other participants or the experimenters. Therefore, the design does not involve any social interactions. Additionally, the experiment does not include a compliance mechanism; that is, no audit, no penalty, and no opportunityfor the experimenter to observe a subjects actions. This implies that the actual and expected extrinsic costs of lying are zero. We argue that these two characteristics of the experimental design implies that the decision to be dishonest in our experiment depends only on a tradeoff between the external monetary benefits and the internal costs of the dishonest act.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="b24681d555">
            <titleStmt>
               <title>Foreign Direct Investment and Democracy: A Robust Fixed Effects Approach to a Complex Relationship</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/1468-0106.12204</idno>
                  <idno type="origin">10.1111%2F1468-0106.12204</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>According to <rs resp="#curator" type="software" xml:id="b24681d555-software-simple-0">Google Scholar</rs>, <ref type="bibr">Jensen (2003)</ref> is the most cited empirical study on the relationship between FDI and democracy. It has been cited 650 times, and, as shown in <ref type="figure">Figure 3</ref>, it remains a key reference today. In this section, we investigate the robustness of this study's findings to outliers. <ref type="bibr">2</ref> The sample covers 112 countries over the period 1970-1997, the dependent variable is FDI inflows as a share of GDP, and the proxy for democracy is Polity IV score (see <ref type="table">Table 5</ref> for a description). We focus on the key regression of the paper: model 10 in <ref type="table">Table 4</ref>. The estimates suggest that a fully autocratic government (score of 0) becoming fully democratic (score of 20) can be expected to attract 0.4% more FDI flows as a percentage of GDP). Given that the average level of FDI flows in the sample is 1.3% of GDP, this is a sizeable amount. <ref type="table">Table 4</ref> presents our results. Column (1) replicates perfectly the estimates found in <ref type="bibr">Jensen (2003)</ref>. In column (2), we remove the observations identified as outliers by the Cook distance, which is a scaled measure of the distance between the coefficient vectors when a given observation is omitted. Five percent of the observations are deemed to be outliers. Their omission has no qualitative impact on the results. In column (3) we use an M-estimator, which gives a lower weight to observations with large standardized residuals but is not robust to leverage points. The estimated impact of democracy falls by half. In column (4), the M-estimator is used again, but the sample now only includes observations that have not been identified as outliers, and, therefore, potential leverage points, in column (2). This is essentially what the <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="b24681d555-software-simple-1">Stata</rs> command -rreg-is doing.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d8d0b10833">
            <titleStmt>
               <title>Firm size and entrepreneurial characteristics: Evidence from the SME sector in Argentina</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3846/jbem.2010.13</idno>
                  <idno type="origin">10.3846%2Fjbem.2010.13</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>There are also disadvantages associated with the RQ estimator. Probably the main problem is that only asymptotic of the estimators are known, which raises the issue of how parameters behave in finite samples. This may not be too problematic in our case since we have a large sample. Though estimating quantile regressions is computationally demanding, this problem was less so because of the availability of powerful computers and software programs such as <rs cert="1.0" resp="#annotator31" subtype="used" type="software" xml:id="d8d0b10833-software-0">Stata</rs> <rs corresp="#d8d0b10833-software-0" resp="#annotator31" type="version">9.0</rs> which allow estimates to be performed relatively easily.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f9d7252337">
            <titleStmt>
               <title>Areeda–Turner in Two-Sided Markets</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s11151-015-9460-5</idno>
                  <idno type="origin">10.1007%2Fs11151-015-9460-5</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The difference in pricing strategies that characterizes two-sided markets warrants in many instances a different antitrust treatment. A recent review discussing these issues is <ref type="bibr">Evans and Schmalensee (2015)</ref>. <ref type="bibr">Parker and Alstyne (2005)</ref> were among the first to present an economic model that highlights that in two-sided markets pricing below marginal cost on one side may be a (nonstrategic) profit-maximising strategy. Indeed, by pricing below marginal cost on one side of the market a firm increases sales on that side, thus boosting demand and profits on the other side. <ref type="bibr">Wright (2004)</ref> included the claim that ''price below marginal cost on one side of the market is a sign of predation'' among the eight fallacies that derive from applying a one-sided logic to two-sided markets. <ref type="bibr">8</ref> Despite the warnings of the economics literature, competition authorities and courts tend to analyse predatory claims with a one-sided logic. In the recent case Bottin Cartographes versus Google, 9 for instance, the Commercial Court of Paris 6 One can also express the price level in terms of advertising pages. In that case it is the sum of the advertising price and the circulation revenues per-advertising-page. 7 Note that such a ratio is equivalent to the ratio between the revenues from the two sides. In the newspapers' business the ratio of circulation revenues to advertising revenues (or vice versa) is sometimes called the 'financing mix'. 8 To our knowledge, no paper discusses the conditions for predation in prices to take place in a two-sided market. A partial exception is <ref type="bibr">Motta and Vasconcelos (2012)</ref> who present a model of platform competition in which below-cost pricing can be used to deter the entry of a more efficient rival. However, the result in <ref type="bibr">Motta and Vasconcelos (2012)</ref> found Google guilty of the abuse of a dominant position in the market for online maps that allow stores' geolocation. <ref type="bibr">10</ref> The Court reached its decision by simply considering that the price of <rs cert="1.0" resp="#annotator14" type="software" xml:id="f9d7252337-software-simple-0">Google Maps API</rs>, being equal to 0 €, was necessarily lower than the production costs of the service. Interestingly, the Court stopped just short of recognizing the implications for competition policy of the two-sided business strategy of Google, as it recognized that <rs corresp="#f9d7252337-software-1" resp="#curator" type="publisher">Google</rs>, according to the contracts, would be able to insert advertising in its <rs cert="1.0" resp="#annotator14" type="software" xml:id="f9d7252337-software-1">Google Maps API</rs> service and therefore sell targeted advertising.</p>
            <p>Footnote 9 continued that provide stores' geolocation on firms' websites. Bottin is a multimedia mapping company that provides, among other things, online map applications that allow users to locate addresses and create itineraries online, which compete in France with the equivalent service of <rs cert="1.0" resp="#annotator14" type="software" xml:id="f9d7252337-software-simple-3">Google Maps API</rs> (Application Programming Interface). While Bottin offers its service in exchange for an annual fee and an ex post compensation based on actual consumption, the ordinary version of <rs cert="1.0" resp="#annotator14" type="software" xml:id="f9d7252337-software-3">Google Maps API</rs> is provided to its customers on a free basis. Bottin claimed that this had to be considered a predatory pricing and that by doing so Google aimed to extend its dominant position in the market of online search to the connected relevant market. 10 More precisely, with its decision of 31 January 2012 the Commercial Court of Paris found Google guilty of the abuse of a dominant position pursuant to Article L-420-2 paragraph 1 of the French Commercial Code and, as a consequence, awarded Bottin 500,000 € damages and interests, in addition to ordering Google to publish the judgment at its expense in several French and international newspapers. 11 Bottin Cartographes v. Google France and Google Inc. Court of Appeal of Paris, 5th Pole, 5th Chamber, 20 November 2013, available on http://www.legalis.net/spip.php?page=jurisprudencedecision&amp;id_article=3942. <ref type="bibr">12</ref> The opinion of the Competition Authority has not been made public yet. In fact, it can be disclosed only after the ruling of the Appeal Court (art. L.462-3 of the French Code of Commerce), which has not been handed out yet. <ref type="bibr">Filistrucchi et al. (2013)</ref> point out that there exist two-types of two-sided markets. Two-sided non-transaction markets are characterized by the absence of a transaction between the two sides of the market and, even though an interaction is present, it is usually not observable, so that a per-transaction (or per-interaction) fee or a two-part tariff is not possible. Typical examples are media markets. Two-sided transaction markets are instead characterized by the presence and observability of a transaction between the two groups of platform users. As a result, the platform is able to charge not only a price for joining the platform, but also one for using it, i.e. it can ask a two-part tariff. Examples in this category include payment cards schemes, virtual marketplaces, auction houses and operating systems.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="a010c944ed">
            <titleStmt>
               <title>VERSLAS: TEORIJA IR PRAKTIKA BUSINESS: THEORY AND PRACTICE A MODEL OF CRITERIA SYSTEM FOR EVALUATION OF CONSTRUCTION CONTRACTION AGREEMENTS</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3846/btp.2007.31</idno>
                  <idno type="origin">10.3846%2Fbtp.2007.31</idno>
               </bibl>
            </sourceDesc>
            <biblStruct/>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="a010c944ed-software-simple-0">Expert Choice</rs> software was used to make pilot calculations. This software opens wide opportunities for its users. However, it has drawbacks as well. The main drawback of this software is that it is adjusted for use in stationary work places. The model of indicator hierarchy for construction contract evaluation developed this way is just an extra feature of this software. Meanwhile, users without special software cannot use the developed system of indicators. True, that this system provides a possibility to publish the hierarchical model in Internet; however, only <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="a010c944ed-software-simple-1">Expert Choice</rs> users can make full use of such a model. Besides, this software has other drawbacks characteristic of other stationary software: limited number of workstations, attachment to a specific workstation, little opportunities to share experience with other users, etc.</p>
            <p>One of the most important opportunities to develop decision support systems is an increase of their integration. First, a system must be developed inside the organisation, and such system with a simple user interface would provide access to and exchange of information among company's employees. A clear link between the decision support system and various data must be provided and it must facilitate installation of various means for resource allocation. In this case, accessibility of the system can be achieved by using standard graphic user interface. Such standardisation is the main reason why <rs corresp="#a010c944ed-software-8" resp="#curator" type="publisher">Microsoft</rs> <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="a010c944ed-software-8">Windows</rs> and related products have become widely distributed so fast. The main problem of decision support systems is an interface with additional means that is difficult to implement. From the perspective of a user of a decision support system, the main criterion when a system for work is being selected is the simplicity of its user interface.</p>
            <p>Another opportunity to improve decision support systems is improvement of accessibility to documents and their management both inside and outside of an organisation. New search and structuring technologies such as underlining of an idea, hypertext and multimedia have been rapidly developed both for scientific research and commercial purposes. One of the most successful examples thereof is groupware, such as <rs resp="#curator" type="software" xml:id="a010c944ed-software-11">Lotus Notes</rs>, which emerged recently and is growing fast. The world becomes a uniform connected whole, which is the basis for further development of decision support systems.</p>
            <p>3. Three CCAs have been selected for calculations and evaluation provided as an example. The evaluation has shown that the first variant is the most favourable to a client, and having analysed graphic information, it is possible to determine possibilities for improvement of agreements. Software <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="a010c944ed-software-simple-5">Expert Choice</rs>, which has been used in this process, is well adjusted to development of a good hierarchy, to determination of criteria significance, to evaluation and analysis of the evaluation results.</p>
            <p>4. The calculations provided as an example enable the following conclusion: although the aforementioned software has a number of advantages, several main disadvantages can also be distinguished. The software is designed for stationary workstations. Thus the developed hierarchical model of CCA evaluation criteria can be only an extramodule of the aforementioned software. Whereas users who do not have specialised software cannot use the developed system of criteria. Although the system foresees a possibility to place hierarchy models in the Internet, only <rs cert="1.0" resp="#annotator2" subtype="used" type="software" xml:id="a010c944ed-software-simple-6">Expert Choice</rs> software users can use them to full extent. Considering the results of the analysis, a specialised decision support system should be developed for CCA evaluation; the system would implement advantages of an Internet-based system.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c8894b50a1">
            <titleStmt>
               <title>Analysing the Impact of Regulation on Disruptive Innovations: The Case of Wireless Technology</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s10842-016-0243-y</idno>
                  <idno type="origin">10.1007%2Fs10842-016-0243-y</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>We can find some noticeable examples of disruptive innovations in the telecommunications industry: <rs cert="1.0" resp="#annotator14" type="software" xml:id="c8894b50a1-software-simple-0">Skype</rs> that has gained the benefits of the market of long-voice telephony, numerous Internet messengers, that partly substituted SMS services or even traditional voice services, or Internet video streaming services such as NetFlix. However, the majority of these examples shares one distinctive feature that, in general, does not entirely fit the concept of disruptive technologies: entering the telecommunications market from the outside they have not undermined the positions of the incumbents of the industry.</p>
            <p>Indeed, from this point of view these innovations of telecommunications services differ significantly from other examples of disruptiveness. The appearance of smartphones subverted the market of mobile phones and destroyed the business of the former leader of the market Nokia. The introduction of digital photo cameras ruined the business of Kodak. At the same time, the appearance of <rs cert="1.0" resp="#annotator14" type="software" xml:id="c8894b50a1-software-simple-1">Skype</rs>, despite of the undeniable significance of the innovation for the industry, has not destroyed the business of British Telecom, Deutsche Telecom or Telefonica. There are some concerns of the incumbents about Bcommodization^of their services <ref type="bibr">(Larouche 2007; Kushida 2015)</ref>, but it does not mean the loss of their places under the sun.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d4e8f08ccf">
            <titleStmt>
               <title>Weights and substitution degree in multidimensional well-being in China</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1108/jes-04-2014-0055</idno>
                  <idno type="origin">10.1108%2Fjes-04-2014-0055</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Next, we calculate the inequality based on the parameter sets we obtained above. Here we use the best and weighted average parameter sets in bootstrap to calculate individuals' welfare without discretizing them, and then we calculate Theil's first and second indices and generalized entropy index with γ ¼ −1/2 using <rs resp="#curator" type="software" xml:id="d4e8f08ccf-software-simple-0">R package "ineq."</rs> Here we use generalized entropy index with γ ¼ −1/2 because the metric entropy measure we use is the same entropy metric with γ ¼ −1/2. The results are in <ref type="table">Table VII</ref>. The second column is the inequality in reported happiness, where happiness is a discrete variable. The third and fourth columns are inequality of welfare S calculated through the best and weighted average parameter sets in the bootstrap, using metric entropy measure 1. The fifth and sixth columns are inequality of welfare S calculated through the best and weighted average parameter sets in the bootstrap, using metric entropy measure 2. The results for discrete individual welfare are in <ref type="table">Table VIII</ref>. When we analyze inequality results, we should focus more on results from metric entropy measure 2 because it focusses on the similarity of two distributions. It is interesting to note from Tables VII and VIII, that inequality in self-reported "happiness" is always higher than that in multidimensional welfare based on entropy metric 2. One possible explanation is that this is due to the</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="b006584631">
            <titleStmt>
               <title>Decision support in software supported negotiations</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3846/jbem.2010.28</idno>
                  <idno type="origin">10.3846%2Fjbem.2010.28</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Negotiation is an everyday task. Except for simple and typical situations like discussing the task shared in a family (i.e. walking the dog, cleaning the apartment, etc.) influencing someone to change their mind or dividing the scarce resources, it includes also the complex deliberations between the companies, political parties or nations. We can describe negotiation situation as the problem of making decision about parties' interdependent goals and objectives <ref type="bibr">(Lewicki et al. 1999)</ref>. Moreover, the parties are committed to peaceful means of solving the problem and there is no clear or established method of making the decision. Usually, while thinking of the negotiation we consider a regular face-to-face meeting where the parties are sitting at the negotiation table and solving the problem using some advisors, analysts, facilitators or mediators. But we live in the time of technological explosion, and nowadays it influences nearly all the activities undertaken by humans, including negotiation. Negotiations are consequently being conducted by means of the electronic media, starting with simple phone calls, through the videoconferences, online chatting, with the use of software negotiation support system (NSS), ending with the electronic negotiation system (ENS) -a software that employs Internet technologies and is deployed on the web for the purpose of facilitating, organizing, supporting and/or automating activities undertaken by the negotiators and/or the third party <ref type="bibr">(Kersten and Lai 2007)</ref>. More and more people and organizations are deciding to negotiate electronically, since it saves time and money and allows to develop and maintain the business contacts regardless of the distance of time and space between the counterparts. Therefore, the negotiation support tools are still being developed to make the negotiation process more fluent and bring negotiators closer to the most satisfying agreement. Usually, these systems implement the formal models deriving from operational research and decision science that are used to help negotiators define negotiation space, evaluate offers and determine efficient solutions. There are a lot of NSSs and ENSs that have been used for simulating, training and teaching negotiations or for research purposes like <rs cert="0.7" resp="#annotator2" type="software" xml:id="b006584631-software-simple-0">INSPIRE</rs> <ref type="bibr">(Kersten and Noronha 1999)</ref>, <rs cert="0.7" resp="#annotator2" type="software" xml:id="b006584631-software-simple-1">Negoisst</rs> <ref type="bibr">(Schoop et al. 2003)</ref>, <rs cert="0.7" resp="#annotator2" type="software" xml:id="b006584631-software-simple-2">NeGoGo</rs> <ref type="bibr">(Lai et al. 2007)</ref> or <rs cert="0.7" resp="#curator" type="software" xml:id="b006584631-software-14">NegoCalc</rs> <ref type="bibr">(Wachowicz 2008)</ref>. Lots of theoretical and practical solutions for electronic negotiation support are also proposed in the literature, that focuse on both the methodological and applicational aspects of electronic negotiations (see <ref type="bibr">Stroebel and Weinhardt 2003; Urbanavičienė et al. 2009a, b)</ref>. Unfortunately, there are only a few examples of using the formal models and systems in the large and complex real-world negotiations of business or political nature. One of them was introducing the Deep Ocean Mining Model into the United Nations' UNCLOS III negotiations <ref type="bibr">(Sebenius 1984)</ref> concerning the rights to exploit the deep sea. It allowed to make a consensus between the developed and developing countries according to the profits sharing. Another system, developed by <rs corresp="#b006584631-software-1" resp="#curator" type="publisher">International Institute for Applied System Analysis</rs>, is <rs resp="#curator" type="software" xml:id="b006584631-software-1">RAINS</rs> <ref type="bibr">(Hordijk 1991)</ref>, which was used in the international negotiations between the European countries on the air pollution limits. Some other systems are still being developed to support real-world problems like <rs cert="0.5" resp="#annotator2" type="software" xml:id="b006584631-software-2">Familly_Winner</rs> <ref type="bibr">(Bellucci and Zeleznikow 2005)</ref> designed on <rs corresp="#b006584631-software-2" resp="#curator" type="publisher">Victoria University</rs>, Australia, for supporting divorcing disputes or <rs cert="0.5" resp="#annotator2" type="software" xml:id="b006584631-software-17">Smartsettle</rs> <ref type="bibr">(Thiessen and Soberg 2003)</ref>, which is now being adapted to support negotiations between First Nations and the Government of Alberta Province in Canada <ref type="bibr">(Thiessen and Shakun 2009)</ref>.</p>
            <p>Having completed all four steps of the above procedure we obtain a full scoring system of the feasible offers. Negotiators can use this system to compare proposals submitted by their counterparts and analyze the differences between quality (satisfaction rate) of the subsequent offers (that have the interval scale interpretation). Negotiators can also observe how the score is determined while the options are added or changed within the offer they prepare. The system allows also to follow the negotiation progress by analyzing the concession paths -the shape of the graph built with scores of successive offers submitted both by the negotiator himself and his counterpart. Such a solution is commonly applied in the NSS, for example in <rs cert="0.7" resp="#annotator2" type="software" xml:id="b006584631-software-simple-9">Inspire</rs> <ref type="bibr">(Kersten and Noronha 1999)</ref>, <rs cert="0.5" resp="#annotator2" type="software" xml:id="b006584631-software-simple-10">Smartsettle</rs> <ref type="bibr">(Thiessen and Soberg 2003)</ref> or <rs cert="0.7" resp="#annotator2" type="software" xml:id="b006584631-software-simple-11">Negoisst</rs> <ref type="bibr">(Schoop et al. 2003</ref>). An example of scoring negotiation offers with additive scoring system in <rs cert="0.7" resp="#annotator2" type="software" xml:id="b006584631-software-simple-12">Inspire</rs> ENS is shown in <ref type="figure">Fig. 2</ref>.</p>
            <p>Even-swap based scoring system was applied in a simple spreadsheet based negotiation support tool called <rs cert="0.7" resp="#annotator2" type="software" xml:id="b006584631-software-22">NegoCalc</rs> <ref type="bibr">(Wachowicz 2008)</ref>. The key stages of building the negotiation offers' scoring system in <rs cert="0.7" resp="#curator" type="software" xml:id="b006584631-software-simple-14">NegoCalc</rs> are shown in <ref type="figure">Fig. 3</ref>. First the base issue has been selected as the price, and then, after identification of the best options for all remaining issues (building time and warranty time), the equivalent amounts for even swaps in terms of price have been declared.</p>
            <p>Both an additive scoring system and even-swap based scoring system can be used in the offer's formulation process for analyzing in details the trade-offs between issues and options. While building the offer (i.e. adding the propositions for reservation levels of successive issues) negotiator can observe how the overall score changes. He can immediately react if the score is lower than his aspiration level by balancing with different combination of options (increasing value of one issue, while decreasing it for another). The process of building negotiation offer in <rs cert="0.7" resp="#annotator2" type="software" xml:id="b006584631-software-simple-15">NegoCalc</rs> system is presented in <ref type="figure">Fig. 7</ref>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="ed18de82d7">
            <titleStmt>
               <title>Application of E‐technologies for regional development: The case of Vilnius city</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3846/jbem.2010.20</idno>
                  <idno type="origin">10.3846%2Fjbem.2010.20</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The project <rs cert="0.7" resp="#annotator14" type="software" xml:id="ed18de82d7-software-simple-0">E-VOICE</rs> (<ref type="bibr">E-Voice … 2005</ref>) intends to concentrate on e-democracy/egovernment in order to try and renew the political information, communication and interaction processes between elected politicians, the administration and the citizensincluding young people -on a local and/or regional level at various locations in the North Sea Region with the support of the 'new' media <ref type="bibr">(Internet, e-mail, sms, i-mode, etc.)</ref> in combination with the 'old' media (television, radio, (mobile) telephone, newspapers, etc). Some possible examples are: the organisational development of digital office hours -citizens get the opportunity to pose questions to mayor, aldermen and/ or council members by e-mail or by direct communication via the Internet and web-TV; online townhall (e.g. experimental broadcasts of the yearly local-council budgetary meeting); digital debates and online panel discussions for citizens; electronic neighbourhood groups (<ref type="bibr">E-Voice … 2005</ref>).</p>
            <p>As a result of the delegation of various functions to the local districts and the resulting increased focus on a flatter, less bureaucratic structure in relation to decision-making processes it became necessary to develop new methods for use in local government <ref type="bibr">(INTELCITY 2002)</ref>. <ref type="bibr">Xie (2003)</ref> describes a complex online decision support system, <rs cert="0.8" resp="#annotator14" type="software" xml:id="ed18de82d7-software-simple-1">WebPolis</rs>, in the context of eGovernment and eDemocracy. <rs cert="0.8" resp="#annotator14" type="software" xml:id="ed18de82d7-software-simple-2">WebPolis</rs> is designed to facilitate greater community involvement in decision-making and offers a series of public communication and community application modules useful to local governments through a common online interface for users (both citizens and officials). <rs cert="0.8" resp="#annotator14" type="software" xml:id="ed18de82d7-software-simple-3">WebPolis</rs> communication modules include e-mail, community newsletters, discussion conferences, online resources metadata harvesting and searching engine, and online surveys. <rs cert="0.8" resp="#annotator14" type="software" xml:id="ed18de82d7-software-simple-4">WebPolis</rs> application toolsets contain Decision Action Process, Geographic Information Systems databases, landuse suitability analyst, infrastructure cost analyst, economic development analyst, and GASB34.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c377979b17">
            <titleStmt>
               <title>GARCH 101: The Use of ARCH/GARCH Models in Applied Econometrics</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/jep.15.4.157</idno>
                  <idno type="origin">10.1257%2Fjep.15.4.157</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>I turn now to the question of how the econometrician can possibly estimate an equation like the GARCH(1,1) when the only variable on which there are data is r t . The simple answer is to use maximum likelihood by substituting h t for 2 in the normal likelihood and then maximizing with respect to the parameters. An even simpler answer is to use software such as <rs cert="1.0" resp="#annotator14" type="software" xml:id="c377979b17-software-simple-0">EViews</rs>, <rs cert="1.0" resp="#annotator14" type="software" xml:id="c377979b17-software-simple-1">SAS</rs>, <rs cert="1.0" resp="#annotator14" type="software" xml:id="c377979b17-software-simple-2">GAUSS</rs>, <rs cert="1.0" resp="#annotator14" type="software" xml:id="c377979b17-software-simple-3">TSP</rs>, <rs cert="1.0" resp="#annotator14" type="software" xml:id="c377979b17-software-simple-4">Matlab</rs>, <rs cert="1.0" resp="#annotator14" type="software" xml:id="c377979b17-software-simple-5">RATS</rs> and many others where there exist already packaged programs to do this.</p>
            <p>First, we construct the hypothetical historical portfolio. (All calculations in this example were done with the <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="c377979b17-software-simple-6">EViews</rs> software program.) <ref type="figure">Figure 1</ref> shows the pattern of returns of the Nasdaq, Dow Jones, bonds and the composite portfolio leading up to the terminal date. Each of these series appears to show the signs of ARCH effects in that the amplitude of the returns varies over time. In the case of the equities, it is clear that this has increased substantially in the latter part of the sample period. Visually, Nasdaq is even more extreme. In <ref type="table">Table 1</ref>, we present some illustrative statistics for each of these three investments separately and for the portfolio as a whole in the final column. From the daily standard deviation, we see that the Nasdaq is the most volatile and interest rates the least volatile of the assets. The portfolio is less volatile than either of the equity series even though it is 80 percent equity-yet another illustration of the benefits of diversification. All the assets show evidence of fat tails, since the kurtosis exceeds 3, which is the normal value, and evidence of negative skewness, which means that the left tail is particularly extreme.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="ea7ea40413">
            <titleStmt>
               <title>The Past, Present and Future of the German Record Linkage Center (GRLC)</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1515/jbnst-2017-1004</idno>
                  <idno type="origin">10.1515%2Fjbnst-2017-1004</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>During the last decade, the research group of the second author has developed a Java program for record linkage called <rs resp="#curator" type="software" xml:id="ea7ea40413-software-6">Merge Toolbox (MTB</rs>, <ref type="bibr">Schnell et al., 2004)</ref>. GRLC has extended the capabilities of the program by including routines for privacy preserving record linkage <ref type="bibr">(Schnell et al., 2009)</ref>, special routines for self-generated-identification codes <ref type="bibr">(Schnell et al., 2010)</ref> and updated the input/output-options so that <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="ea7ea40413-software-7">MTB</rs> is able to read and write CSV files and native binary <rs resp="#curator" type="software" xml:id="ea7ea40413-software-1">Stata</rs>-<rs corresp="#ea7ea40413-software-1" resp="#curator" type="version">15</rs> files. <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="ea7ea40413-software-simple-4">MTB</rs> consists of different modules such as a data editor for record linkage and the main linkage module. Since 2012, <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="ea7ea40413-software-simple-5">MTB</rs> has been downloaded by 1104 researchers. <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="ea7ea40413-software-simple-6">MTB</rs> is discussed in the leading textbook on record linkage by <ref type="bibr">Christen (2012)</ref>. An implementation of multibit-trees for privacy preserving record linkage based on Bloomfilters <ref type="bibr">(Schnell, 2015)</ref> is provided as a C++ stand-alone program and as a library for R.</p>
            <p>Due to the research on privacy preserving record linkage within the GRLC, many new functions for encrypting linkage keys have been developed <ref type="bibr">(Schnell and Borgs, 2016)</ref>. These new functions have not been implemented in <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="ea7ea40413-software-simple-7">MTB</rs>, but in a new <rs resp="#curator" type="software" xml:id="ea7ea40413-software-simple-8">R</rs> library <rs cert="0.6" resp="#annotator14" type="software" xml:id="ea7ea40413-software-simple-9">PPRL</rs>. Since record linkage often involves large datasets with millions of records, most functions within <rs cert="0.6" resp="#annotator14" type="software" xml:id="ea7ea40413-software-simple-10">PPRL</rs> have been been optimized for speed using C++ as the main language. The <rs cert="0.6" resp="#annotator14" type="software" xml:id="ea7ea40413-software-simple-11">PPRL</rs> library will be released within 2017 as an open-source project.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c27a217dfd">
            <titleStmt>
               <title>Going for the Gold: The Economics of the Olympics</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/jep.30.2.201</idno>
                  <idno type="origin">10.1257%2Fjep.30.2.201</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Several tools can be used to potentially produce more precise economic impact estimates including the <rs resp="#curator" type="software" xml:id="c27a217dfd-software-0">Regional Input-Output Multiplier System (RIMS</rs> <rs corresp="#c27a217dfd-software-0" resp="#annotator3" type="version">II</rs>) provided by the <rs corresp="#c27a217dfd-software-0" resp="#curator" type="publisher">US Bureau of Economic Analysis</rs> and <rs resp="#curator" type="software" xml:id="c27a217dfd-software-simple-3">IMpact for PLANning (IMPLAN)</rs>, a commercially available software package. Both models use input-output tables for specific industries grounded in interindustry relationships within regions based upon an economic area's normal production patterns. But as <ref type="bibr">Matheson (2009)</ref> notes: "During an event like the Olympics, however, the economy within a region may be anything but normal, and therefore, these same inter-industry relationships may not hold. Since there is no reason to believe that the usual economic multipliers are the same during mega-events, any economic analyses based upon these multipliers may, therefore, be highly inaccurate."</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="cccc31fe7c">
            <titleStmt>
               <title>Risk attitude and wage growth: replicating Shaw (1996)</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s00181-012-0549-5</idno>
                  <idno type="origin">10.1007%2Fs00181-012-0549-5</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>As noted in Online Appendix A, the variable Asset has a very high proportion of zero's. This would imply that the individual's relative risk aversion is infinite, a rather extreme assumption. We tested the sensitivity to this extreme value by adding a dummy to distinguish zero and positive values; thus, the regression equation included <ref type="bibr">17</ref> In Footnote 14 Shaw also notes that she dropped interaction of the intercept with the risk attitude dummies. We decided not to follow her and stick to the full model. <ref type="bibr">18</ref> Since comparing estimates involves combined coefficients (e.g. β ·a xi ), we need to take into account the standard deviation of such combination. This is done by using the "nonlinear combinations of estimators" option in <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="cccc31fe7c-software-simple-0">STATA</rs>. <ref type="bibr">19</ref> For Italy the null hypothesis is not rejected for the interaction between ARA and years of schooling.a dummy for having any risky financial assets at all and the share of risky financial assets. In the SHIW data for Italy, with the highest proportion of zero's, including the dummy has no effect on the results for the other variables; the significance level for the share of risky assets does not change in any relevant way. 20</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d0872990c0">
            <titleStmt>
               <title>Monetary Policy Under Uncertainty in Micro-Founded Macroeconometric Models</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3386/w11523</idno>
                  <idno type="origin">10.1086%2Fma.20.3585423</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#multiple_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The optimal policy under commitment can be computed by formulating an infinite-horizon Lagrangian problem in which the central bank maximizes conditional expected social welfare subject to the full set of nonlinear constraints implied by the private sector's behavioral equations and the market-clearing conditions of the model economy.33 The first-order conditions of this problem are obtained by differentiating the Lagrangian with respect to each of the endogenous variables (including the policy instrument) and setting these derivatives to 0. Of course, performing these derivations by hand would be extremely tedious; thus, we utilize the symbolic <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d0872990c0-software-simple-0">Matlab</rs> procedures developed by <ref type="bibr">Levin and Lopez-Salido (2004)</ref> <ref type="bibr">34</ref> We then proceed to analyze the behavior of the economy under optimal policy by combining the central bank's first-order conditions together with the private sector's behavioral equations and the marketclearing conditions. Thus, the size of the model is much larger under the optimal policy because these first-order conditions take the place of a single interest rate reaction function, while the set of Lagrange multipliers is added to the list of model variables. Nevertheless, it should be emphasized that no new parameters have been added to the model because the central bank's first-order conditions involve the same structural parameters as in the behavioral equations and marketclearing conditions.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="bb53f0a4b5">
            <titleStmt>
               <title>Sources of Business Cycle Fluctuations</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.3386/w2589</idno>
                  <idno type="origin">10.1086%2F654078</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Since D(1) is the lower triangular, the long-run multipliers from Aot, AYt, Art7, and it -n to Aht are zero, so the coefficients of their lags each sum to zero. Imposing these constraints yieldsThe variables considered in our model are total hours worked (ht), output (yt), inflation (rr,), the nominal interest rate (it), and real oil prices (or). The Appendix gives the details of the sources of the data. Estimates reported in this paper are based on quarterly U.S. data from 1951:1. Data before 1951 are used as initial conditions in autoregressions. The end of the sample period is discussed below. The data for labor hours, output, and price are for the nonfarm private economy, excluding housing. We choose output for the nonfarm, non-housing private sector, rather than the whole economy because there are serious conceptual difficulties in relating the output to the inputs of housing, government, and farms. Housing and government are imputed in the national accounts. Farmers are largely 12. In the <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="bb53f0a4b5-software-simple-0">RATS</rs> packages, the equations can be estimated without including the disturbances and then transformed via the standard Cholesky decomposition. This decomposition picks out a different linear combination of the aggregate demand shocks, but since only their joint effect is identified, this difference is inessential. 13. <ref type="bibr">Blanchard and Quah (1988)</ref> use a different technique to estimate models subject to these long-run Wold causal orderings. They estimate the unrestricted vector autoregression for X, and then transform the system by post-multiplying the VAR by a matrix that imposes the necessary restrictions on the long-run multipliers and the residual covariance matrix.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="ecd07e900a">
            <titleStmt>
               <title>Social Media and Fake News in the 2016 Election</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/jep.31.2.211</idno>
                  <idno type="origin">10.1257%2Fjep.31.2.211</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>During the week of November 28, 2016, we conducted an online survey of 1208 US adults aged 18 and over using the <rs cert="0.8" resp="#annotator14" subtype="used" type="software" xml:id="ecd07e900a-software-simple-0">SurveyMonkey</rs> platform. The sample was drawn from <rs cert="0.8" resp="#curator" subtype="used" type="software" xml:id="ecd07e900a-software-simple-1">SurveyMonkey</rs>'s Audience Panel, an opt-in panel recruited from the more than 30 million people who complete <rs cert="0.8" resp="#curator" subtype="used" type="software" xml:id="ecd07e900a-software-simple-2">SurveyMonkey</rs> surveys every month (as described in more detail at https://www.surveymonkey.com/mp/audience/).</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f8f8887979">
            <titleStmt>
               <title>North-South foreign direct investment and bilateral investment treaties</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/twec.12539</idno>
                  <idno type="origin">10.1111%2Ftwec.12539</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Given our focus on aggregate data, our hypothesis and argumentation is somewhat different. To the extent that BITs act as a means of substituting for weak domestic institutions in the host, then we would expect that multinational firms concerned about weak domestic institutions would not invest (or would withdraw previous investments) in that country without the protection of a BIT. <ref type="bibr">15</ref> For more information on the difference between these estimators, see <ref type="bibr">Wooldridge (2002)</ref> and the <rs cert="1.0" resp="#annotator14" type="software" xml:id="f8f8887979-software-simple-0">Stata</rs> help file for psmatch2. <ref type="bibr">16</ref> In particular, this gives an estimate of the average treatment effect on the treated (ATT), which in our case gives the expected impact of new-BIT membership on FDI flows for a randomly drawn country-pair in the subsample of countrypairs that actually entered into a BIT agreement. Note that the ATT can be obtained from a weighted least squares regression, where the weights for the untreated observations correspond to the weights used in the construction of the control group. <ref type="bibr">17</ref> Myburgh and Paniagua (2016) use a similar approach and find a BIT reduces (raises) FDI levels below (above) the median.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="a71efb9ac8">
            <titleStmt>
               <title>Patents, Thickets and the Financing of Early-Stage Firms: Evidence from the Software Industry</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/j.1530-9134.2009.00228.x</idno>
                  <idno type="origin">10.1111%2Fj.1530-9134.2009.00228.x</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Data such as these suggest that costs associated with patentssearching prior art, building patent portfolios, and defending against the threat of patent litigation-have grown very substantially, and may now be having a significantly negative impact on the pace of innovation in the industry. To cite just two authoritative observers, <rs corresp="#a71efb9ac8-software-0" resp="#curator" subtype="person" type="publisher">Donald Knuth</rs>, author of The Art of Computer Programming and inventor of <rs cert="0.6" resp="#annotator14" type="software" xml:id="a71efb9ac8-software-0">TeX</rs>, has stated that "I don't think I would have been able to create  <rs cert="0.6" resp="#curator" type="software" xml:id="a71efb9ac8-software-simple-2">TeX</rs> if the present [patent] climate had existed in the 1970s," while erstwhile entrepreneur Bill Gates has opined that "If people had understood how patents would be granted when most of today's ideas were invented and had taken out patents, the industry would be at a complete standstill today."</p>
            <p>Turning to the impact of patent thickets on financing, the first three columns of <ref type="table">Table III</ref> present results from Poisson regressions on the number of new ventures receiving their first round of financing from external investors. As before, standard errors are clustered by market, and all regressions have market and year fixed effects. The dependent variable is the number of firms in market j that have not previously received external financing and which obtain such investment for the first time in year t. (The following section describes in greater detail how this indicator is constructed, and performs a firm-level analysis of the probability that a firm receives initial investment by a given year. Here we present a preview of these findings by examining the broad market-level association between in initial financing episodes and patenting in the market.) The regressions are run on a balanced panel of observations on our 27 markets between 1994 and 2002, for a total of 243 observations. The number of firms in a market receiving investment for the first time in year t will depend on the number of firms "at risk" (i.e., the number of early-stage firms that have not previously received investment). We therefore include in the regression a count of the total number of new ventures identified as being present in the previous sample year. Additional controls include the stage of development of firms in the market (the average age of the firms), and the same variables as in the entry regressions to control for demand and market structure, which affect the anticipated profitability of entrants. In column (1) the coefficient on the number of patents in force is large and negative, implying that software ventures that enter markets with larger patent thickets see are less likely to receive funding from outside investors. In this regression the estimated coefficient is negative and significant at the 10% level. 37 36. These effects were obtained by calculating the pre-and post-regime change difference in the partial derivative with respect to the log of patents in the market, using <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a71efb9ac8-software-simple-3">Stata</rs>'s predictnl command. This command computes standard errors via the delta method. See <ref type="bibr">Ai and Norton (2003)</ref> for more on interaction effects in nonlinear models.</p>
            <p>These results show the importance of controlling for market characteristics, and in the hazard models, our explanatory variables include year and product market fixed effects and the modal citation lag for patents in the market. Other market-level controls relating to profitability in the firm's target market include the number of 40. These graphs were created using <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="a71efb9ac8-software-simple-4">Stata</rs>'s sts command, which estimates separate Cox regressions for each percentile group. Adjusting for the number of incumbents means that the number of incumbents and the number of incumbents squared were included as covariates in the Cox regression. incumbents in the market and the number of incumbents squared, and the growth of sales in the market. <ref type="bibr">41</ref> Variables relating to the patent landscape include the log of the patent stock in the market and the Herfindahl over assignees of citations made by patents in the market. <ref type="bibr">42</ref> Firm-level explanatory variables include the aforementioned dummies for firm size range, implicitly the age of the firm (because the "duration" variable is years since the birth of the firm), and the number of each firm's patents granted and pending. 43 <ref type="table">Table IV</ref> gives results from these regressions, reported as hazard ratios. Estimates larger than one indicating a positive effect on the hazard of receiving funding, and estimates less than one indicating a negative effect. In general the hazard of funding is nonlinearly related to the number of competitors in the market, with an initial increasepresumably reflecting an increase in expected profits due to a reduction in the market power of incumbents or a reduction of barriers to entry created by network effects-then falling as the number of incumbents increases, which could reflect the fact that large numbers of incumbents indicate more mature, more crowded, and less attractive markets. The dummies for the range of firm size (not reported, but available upon request) display a concave relationship between firm size and the hazard of initial funding. The standard errors in these regressions are somewhat sensitive to the selection of control variables, but adding or dropping control variables has little effect on the estimated coefficients on the patent variables.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d2ddee9af5">
            <titleStmt>
               <title>THE IMPACT OF TAX PRICE CHANGES ON CHARITABLE CONTRIBUTIONS TO THE NEEDY</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/coep.12177</idno>
                  <idno type="origin">10.1111%2Fcoep.12177</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The dependent variables are donations to organizations who help people in need of food, shelter, or other basic necessities (needy), to CP&amp;IA/peace organizations, and to all other charities (all other). These data were taken directly or calculated from the COPPS database which identifies monetary donations to charitable organizations by purpose. 8 Federal and state marginal tax rates are estimated using Internet <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d2ddee9af5-software-1">TAXSIM</rs> Version <rs corresp="#d2ddee9af5-software-1" resp="#annotator14" type="version">9</rs>, the <rs corresp="#d2ddee9af5-software-1" resp="#curator" type="publisher">NBER</rs>'s FORTRAN program for calculating liabilities under U.S. federal and state income tax laws from individual data. 9 These rates are estimated presuming zero charitable contributions so that they represent "first dollar" tax rates. 10 This ameliorates the potential endogeneity problem with actual marginal tax rates that arises because large donation amounts 8. The all other categories include religious organizations, health and medical research, education, neighborhood and community improvement, youth and family services, the arts, the environment, and other organizations.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c877cb6726">
            <titleStmt>
               <title>The DataLab of the Australian Bureau of Statistics</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/1467-8462.12246</idno>
                  <idno type="origin">10.1111%2F1467-8462.12246</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The DataLab was designed in late 2015 to improve access to ABS microdata, enhance user experience and advance research outcomes to inform policy design and delivery, as well as increase the use of statistics within the research community. The development areas to be addressed included: the costs and security risks associated with the maintenance of ageing technologies (for example, RADL); the substantial changes in user requirements and the increased sophistication of researcher skills (for example, programming skills associated with <rs cert="1.0" resp="#curator" subtype="used" type="software" xml:id="c877cb6726-software-simple-0">SAS</rs> as opposed to improved 'point and click' functionality, and open source coding packages such as <rs cert="1.0" resp="#curator" subtype="used" type="software" xml:id="c877cb6726-software-simple-1">R</rs> and Python); and importantly the increased risk of re-identification posed by making more detailed as well as integrated microdata available.</p>
            <p>While the RADL allows analysis of microdata in a secure and controlled environment, it does not allow 'remote' access as implied by its name. Instead it provides a remote execution service, whereby authorised users submit queries in <rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="c877cb6726-software-5">SAS</rs> <rs corresp="#c877cb6726-software-5" resp="#annotator4" type="version">9.1</rs>, <rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="c877cb6726-software-6">SPSS</rs> <rs corresp="#c877cb6726-software-6" resp="#annotator4" type="version">11.5</rs> or <rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="c877cb6726-software-7">Stata</rs> <rs corresp="#c877cb6726-software-7" resp="#annotator4" type="version">10</rs> analytical languages against expanded CURFs (see <ref type="figure">Figure 1</ref>). A range of automated processes monitor data requests and either release or withhold results where there is a possible confidentiality/disclosure risk. The number of unit records that can be seen at any one point in time is restricted in the RADL.</p>
            <p>(iii) Researchers must be experienced in the use of one or more of the analytical software languages available within the DataLab. This is important as the DataLab is a self-service system and the ABS does not provide training or assistance to users for coding and methodological issues related to their research. Software available in the DataLab include <rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="c877cb6726-software-8">SAS Enterprise Guide</rs> <rs corresp="#c877cb6726-software-8" resp="#annotator4" type="version">7.1</rs>, <rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="c877cb6726-software-9">Stata</rs> <rs corresp="#c877cb6726-software-9" resp="#annotator4" type="version">13</rs>, <rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="c877cb6726-software-10">SPSS</rs> <rs corresp="#c877cb6726-software-10" resp="#curator" type="version">24, R 3.1.2</rs>, <rs corresp="#c877cb6726-software-11" resp="#curator" type="publisher">Microsoft</rs> <rs resp="#curator" type="software" xml:id="c877cb6726-software-11">Word</rs> <rs corresp="#c877cb6726-software-11" resp="#annotator4" type="version">2010</rs>, <rs corresp="#c877cb6726-software-12" resp="#curator" type="publisher">Microsoft</rs> <rs resp="#curator" type="software" xml:id="c877cb6726-software-12">Excel</rs> <rs corresp="#c877cb6726-software-12" resp="#annotator4" type="version">2010</rs>, <rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="c877cb6726-software-simple-20">Ultraedit</rs> and <rs corresp="#c877cb6726-software-13" resp="#curator" type="publisher">Adobe</rs> <rs resp="#curator" type="software" xml:id="c877cb6726-software-13">Acrobat Reader DC</rs> (PDF). Updates to these statistical packages will be made available as they are released. The addition of further packages is open to discussion as the DataLab is envisioned to be a product that evolves and adapts to user needs and requirements.</p>
            <p>In relation to package add-ons, researchers can request for CaMD to have these loaded on their behalf. For example, a researcher who is using <rs cert="1.0" resp="#annotator4" subtype="used" type="software" xml:id="c877cb6726-software-simple-23">STATA</rs> may need an add-on, such as 'svr', that is not available in the ABS licensed package within the DataLab. A request can be made to CaMD to have this added into the DataLab environment and once loaded it will be available for all DataLab users via this drive.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="ecb42a8c5d">
            <titleStmt>
               <title>The Digital Disruption and its Societal Impacts</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s10842-014-0187-z</idno>
                  <idno type="origin">10.1007%2Fs10842-014-0187-z</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Huberty <ref type="formula">(2014)</ref> focuses on what resides in the cloud, big data-large volumes of rapidly accumulating data of various kinds that registers information about individuals' behavior online. He suggests that big data is in its infancy and in order for it to live up to its commercial promise, certain shortcomings have to be first acknowledged and then addressed. He defines and challenges four previously unstated assumptions underlying big data: (1) all the individuals one cares about are included in the data, (2) one is able to identify the individuals of interest, (3) online behavior is consistent with offline behavior, and (4) individuals' representation of themselves are consistent over time. Strictly speaking, none of these four assumptions is true. Huberty defines three orders of viable big data business models. The first two of these are already well-known: the third-order business model is in essence online advertising; the second-order business model is online retailing. Huberty argues that the real changes will come from the first-order business models that are deeply embedded in, and reliant upon, data generated from and around real-world phenomena. Here, he cites products such as Nest thermostats that utilize direct home appliance data to make valuable decisions or the <rs cert="1.0" resp="#annotator14" type="software" xml:id="ecb42a8c5d-software-simple-0">Google Translate</rs> function that provides a big data service directly. He believes that these types of applications are where the true value of big data can be realized. <ref type="bibr">Kushida (2014)</ref> concentrates on the politics of ICT commoditization, i.e., he argues that national-level political and socio-economical dynamics influenced the rapid disruptions experienced by global ICT industries. His analysis suggests that local regulatory and technology choices played critical roles in shaping globally disruptive business models driving commoditization. Thus, global industry disruptions are shaped by a set of local political bargains particularly in the early stages of technological trajectories. In the context of mobile telecommunications, operators won the early political fights in Japan and equipment manufacturers in Europe, but eventually US-based computing-centric leaders emerged and ultimately commoditized all other players, both domestically and internationally. Even though-in the context of ICT-technological opportunities have been largely symmetric and we are often compelled to study variations in business strategies in an effort to explain competitive outcomes, Kushida's observations suggest that the underlying trajectories of global competition were shaped by national political dynamics.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c9408978f5">
            <titleStmt>
               <title>The NAIRU, Unemployment and the Rate of Inflation in Brazil *</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1590/s0034-71402003000400011</idno>
                  <idno type="origin">10.1590%2Fs0034-71402003000400011</idno>
               </bibl>
            </sourceDesc>
            <biblStruct/>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>The two models were estimated using computer programs developed with the help of the "<rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="c9408978f5-software-simple-0">Matlab</rs>" software. The log likelihood for the two models, and for a variety of alternative hypothesis, is presented in table 1. <ref type="bibr">6</ref> The parameter of β * t do not enter explicitly in the numerical optimization routine. They are concentrated out by the Kim filter.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f55d17f09f">
            <titleStmt>
               <title>STEM Training and Early Career Outcomes of Female and Male Graduate Students: Evidence from UMETRICS Data Linked to the 2010 Census</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/aer.p20161124</idno>
                  <idno type="PMC">PMC4876811</idno>
                  <idno type="PMID">27231399</idno>
                  <idno type="origin">10.1257%2Faer.p20161124</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>We link the UMETRICS data to the 2010 census via a person identifier, used internally by the Census Bureau, called the Protected Identification Key (PIK). PIKs are assigned through the <rs resp="#curator" type="software" xml:id="f55d17f09f-software-simple-0">Person Identification Validation System (PVS)</rs>, which uses probability record linkage techniques and personal information such as name, date of birth, and residential location <ref type="bibr">(Wagner and Layne 2014)</ref>. Once a PIK is assigned to a record, the Personally Identifiable Information is removed so analysts can anonymously link individuals across files for statistical and research purposes. Person records in the census contain date of birth, gender, race, ethnicity, and relationship to the head of household (HH), the last of which permits inference about certain relationships within a household. Marital status is modeled for graduate students who are either the HH or listed as a spouse or unmarried partner of the HH. Individuals are characterized as having children when they are, or are married to, the HH and there are (step) children of the HH present. <ref type="bibr">1</ref> 1 This approach provides valuable information but has two main limitations. First, we can only infer marital status The PIK is used to link to W-2 earnings, which cover total annual wages, tips, and other compensation from the job with the highest earnings in each year from 2005 and 2012. Linking to the LEHD provides establishment identifiers, and linking on those establishment identifiers to the BR, LBD, and ILBD provides sector of employment.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="da8a45d63e">
            <titleStmt>
               <title>Stock Prices and Fundamentals</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1162/08893360052471455</idno>
                  <idno type="origin">10.2307%2F3585311</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>As in most exercises of this type, the equity-premium puzzle remains a serious problem. For income and dividend processes and participation rates based on historical data, the model predicts an unrealistically small equity premium. We have increased the assumed volatility of aggregate income to increase the predicted premium, but want to emphasize that this may not be a neutral adjustment with respect to the other quantities of interest.10 4.2.1 Income and Preferences Let y(t) -log[Ya(t)/Ya(t -1)] be the growth rate of aggregate nonmarketed income at time t. Then the aggregate state of the economy is given by z(t) = [y(t) d(t)]', which is assumed to be generated by a Markov chain. To calibrate a process for z(t) we assume that a period corresponds to 25 years. The first period roughly corresponds to the working years between age 40 and retirement, and the second period is the time in retirement. Over the period 1889 to 1985, the average annual (log) growth rate in real aggregate consumption was 1.7% with a standard deviation of 3.5%. So that the model will produce a nonnegligible equity premium, we assume that the standard deviation of the aggregate growth rate in the model is 1.5 times the historical standard deviation of aggregate 9. The <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="da8a45d63e-software-simple-0">Matlab</rs> code is available upon request. 10. Recently <ref type="bibr">Campbell and Cochrane (1998)</ref> suggested that time-varying habit provides a higher estimate of the equity premium in a model based on aggregate consumption. However, <ref type="bibr">Cochrane (1997)</ref> claims that this preference specification cannot account for the recent run-up in stock prices.consumption. For the same reason, we assume that annual income growth is independently and identically distributed over time, although in fact it is slightly negatively autocorrelated. This implies a 25-year average (log) growth rate of 42.5% with a standard deviation of 17.5%. This distribution is discretized by assuming that y takes on the values 0.16 and 0.69 with equal conditional probability. The capital share in total income averages approximately 30%. Consistent with the aggregate statistics reported in <ref type="bibr">Heaton and Lucas (1998)</ref>, we assume that only half of this capital income is actually tradable. The nontradable portion, generated by private business holdings, is accounted for in nonmarketed income. Since dividends in the model are scaled relative to nonmarketed income, this means that we require d(t) to average 18%. In most of the calculations d(t) is fixed at 18%. In other experiments described below, we assume a more volatile dividend process to proxy for a lack of diversification.11</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c3d6b56ad4">
            <titleStmt>
               <title>ACCESS TO TECHNOLOGY AND THE TRANSFER FUNCTION OF COMMUNITY COLLEGES: EVIDENCE FROM A FIELD EXPERIMENT</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/ecin.12086</idno>
                  <idno type="origin">10.1111%2Fecin.12086</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>19. Power calculations can be performed in <rs cert="1.0" resp="#annotator31" subtype="used" type="software" xml:id="c3d6b56ad4-software-simple-0">STATA</rs> using sampsi or <rs cert="1.0" resp="#annotator31" subtype="used" type="software" xml:id="c3d6b56ad4-software-simple-1">SAS</rs> using proc power. many participants would clearly be prohibitively expensive. <ref type="bibr">20</ref> To detect a treatment-control difference of 0.02, which is 10% of the control group mean, the sample size would have to be 6,510 observations. The effect size would have to be 0.10 (or 50% of the control group mean) to be statistically detectable with sample sizes in the range of what is available in this experiment.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="b9f980c417">
            <titleStmt>
               <title>Context matters</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s10683-017-9546-z</idno>
                  <idno type="origin">10.1007%2Fs10683-017-9546-z</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Our econometric methodology is to fit, for each of RREU, RRRD, AREU and ARRD, the models to the decisions of the subjects, for each of the four elicitation methods, and hence obtain estimates of the risk aversion index (and also the other parameters). We do this by Maximum Likelihood, using <rs cert="0.8" resp="#annotator2" subtype="used" type="software" xml:id="b9f980c417-software-2">Matlab</rs> (the program is available in <rs corresp="#b9f980c417-software-2" resp="#curator" type="url">https://www.york.ac.uk/economics/exec/research/zhouandhey1</rs>/). To do this we need to make assumptions about the stochastic nature of the data. This arises from errors made by the subjects. We largely follow convention.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="f22e17f325">
            <titleStmt>
               <title>Semiparametric Censored Regression Models</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1257/jep.15.4.29</idno>
                  <idno type="origin">10.1257%2Fjep.15.4.29</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>We use several approaches to estimate the interval censoring model. The dependent variable in each case is the natural logarithm of annual taxable earnings, and the explanatory variables are race, level of education, age and agesquared. <ref type="table">Table 1</ref> presents the estimation results for the race and education coefficients based on the various estimators. The first column, headed OLS1, contains the ordinary least squares estimates based on all of the data. The second column, headed OLS2, presents the least squares results using only the observations that are not censored. The third column contains the Tobit maximum likelihood estimates under the assumption that the errors are normally distributed and homoskedastic. The remaining columns present the results for the three semiparametric estimators: CLAD, SCLS and ICLAD. The Tobit, CLAD and SCLS estimators were implemented using the <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="f22e17f325-software-simple-0">Stata</rs> software package, while the ICLAD estimator was calculated using the <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="f22e17f325-software-simple-1">Gauss</rs> package. For each estimator, we have created <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="f22e17f325-software-0">Stata</rs> "ado" files that are available at ͗http://elsa.berkeley.edu/ϳkenchay͘. <ref type="bibr">8</ref> It is clear from <ref type="table">Table 1</ref> that the least squares and maximum likelihood estimates of the black-white log-earnings gap and the returns to education are extremely biased when compared to the semiparametric estimators. We think of the CLAD estimator as the natural benchmark, since it is consistent under the normality of errors assumption justifying the maximum likelihood estimator, under the independence of errors assumption justifying the ICLAD estimator, and under the conditional symmetry of errors assumption justifying the SCLS estimator. When compared to the CLAD benchmark, the least squares estimator based on all of the data (OLS1) actually does better than the maximum likelihood estimator. For this application, it appears that misspecifying the errors as being normally distributed and using maximum likelihood estimation results in more biased estimates than ignoring the censoring problem entirely and using least squares estimation. A more formal test of the normality assumption also suggests that it is violated for the log-earnings model. <ref type="bibr">9</ref> There are sizeable differences in the estimated effects of education on earnings across the three semiparametric estimators. While the ICLAD and SCLS estimators of the education premium are similar, they are always greater than the CLAD estimator. These differences are significant given the precision of the estimates and range from 17 percent to 43 percent for the ICLAD estimator and <ref type="bibr">9</ref> Chay and Honoré (1998) calculate the test statistics for nonnormality and for heteroskedasticity in censored regression models discussed by Chesher and Irish <ref type="bibr">(1987)</ref>. The test statistic for detecting nonnormality ranges from 900.47 to 1200.68. Under the null, this statistic has an (asymptotic) <ref type="bibr">2</ref> (2) distribution with a 1 percent critical value of 9.21. Therefore, we easily reject the hypothesis that the errors are normally distributed. The test for heteroskedasticity yields statistics between 84.71 and 90.85. Under the null, these have (asymptotic) 2 (12) distributions with a 1 percent critical value of 26.22. We therefore also reject the null of no heteroskedasticity.20 percent to 33 percent for the SCLS estimator. The differences in the estimates of the black-white earnings gap are smaller, with the CLAD estimator about 11 percent to 28 percent and 4 percent to 18 percent smaller in magnitude than the ICLAD and SCLS estimators, respectively. Strikingly, the semiparametric approaches all result in more precise estimates of the race coefficient than maximum likelihood estimation. For example, the standard errors of the CLAD estimator are 25 percent to 55 percent smaller than the standard errors of the Tobit estimator. <ref type="bibr">10</ref> The differences in the coefficient estimates across the various estimators can be used as a sort of specification check, similar in spirit to the Newey (1987) specification analysis mentioned earlier. For the education coefficient, the large differences between the maximum likelihood and semiparametric estimates suggest that nonnormal errors are an important source of bias in the Tobit estimator. Further, the significant differences among the semiparametric estimates imply that heteroskedasticity and asymmetry of the errors are also sources of misspecification in the maximum likelihood estimator of the education premium. Conversely, for the black-white earnings gap, the smaller differences among the semiparametric estimates suggest that nonnormality is the biggest source of bias in the Tobit estimator, with heteroskedasticity and asymmetry playing smaller roles.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c6d11cfed3">
            <titleStmt>
               <title>Exploring regional and gender disparities in Beninese primary school attendance: a multilevel approach</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1080/09645292.2018.1426732</idno>
                  <idno type="origin">10.1080%2F09645292.2018.1426732</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>is the log-odds that child i in household j in commune k is attending school. b 0 is the intercept shared by all individuals, households and communes. m k is the effect of commune k, w jk . The effect of household j and 1 ijk is the child level residual error. The models here are computedsing second-order penalized quasi-likelihood (PQL2) in MLwiN <ref type="bibr">(Rasbash et al. 2015)</ref> via the <rs cert="1.0" resp="#curator" subtype="used" type="software" xml:id="c6d11cfed3-software-simple-0">Stata</rs> module runmlwin <ref type="bibr">(Leckie and Charlton 2013)</ref>. When selecting the appropriate means by which to estimate multilevel equations, it is necessary to choose a method at is both the most unbiased, but also computationally feasible. Simulations in <ref type="bibr">Rodriguez and Goldman (2001)</ref> show that 2nd order Penalised Quasi Likelihood (PQL) estimation provided the closest approximation to maximum likelihood estimation (MLE), out of the choice of 1st and 2nd order marginal quasi-likelihood (MQL) and 1st and 2nd order PQL. Whilst, ideally, MLE would be used to obtain all the estimates, this is computationally very intensive for models beyond the null; <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="c6d11cfed3-software-simple-1">Stata</rs>'s commands such as xtmelogit often take many hours or days to converge, if at all. Appendix B displays estimates of the variance of the random effects in equation <ref type="formula">(3)</ref> using MQL1, MQL2, PQL1, PQL2 and MLE; Whilst there is still a downward bias in the estimates of s 2 m and s 2 w compared to MLE, PQL2 performs substantially better than the other quasi-likelihood estimators. Given that MLE estimates for models beyond the null are computationally very difficult to obtain, PQL2 is the preferred method here.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c844840b93">
            <titleStmt>
               <title>MIT Press Journals 2018 catalog</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1162/qjec.2005.120.1.345</idno>
                  <idno type="origin">10.1162%2Fqjec.2005.120.1.345</idno>
               </bibl>
            </sourceDesc>
            <biblStruct/>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>• In-depth articles on cutting-edge research and developments in technology, methods, and aesthetics of computer music • Reports on products of interest, such as new audio and <rs cert="0.6" resp="#annotator2" type="software" xml:id="c844840b93-software-simple-0">MIDI</rs> software and hardware • Interviews with leading composers of computer music • Announcements of and reports on conferences and courses in the United States and abroad • Publication, event, and recording reviews • Tutorials, letters, and editorials • Numerous graphics, photographs, scores, algorithms, and other illustrations. MIT Press Journals offers CMJ and the annual publication, Leonardo Music Journal (see p. 22), at a special subscription price; see details below.</p>
            <p>The journal is hosted on an open-access, open-review, rapid publication platform called <rs cert="0.7" resp="#annotator2" type="software" xml:id="c844840b93-software-1">PubPub</rs>, created by students at the <rs corresp="#c844840b93-software-1" resp="#curator" type="publisher">Media Lab</rs>. <rs cert="0.7" resp="#annotator2" type="software" xml:id="c844840b93-software-simple-3">PubPub</rs> is a collaborative publication environment with rich commenting features, and powerful, intuitive authoring tools. JoDS articles are authored directly within the <rs cert="0.7" resp="#annotator2" type="software" xml:id="c844840b93-software-simple-4">PubPub</rs> environment, which provides support for multimedia, image integration, and large data sets.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="fcae93e89f">
            <titleStmt>
               <title>Antitrust and Vertical Integration in “New Economy” Industries with Application to Broadband Access</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s11151-011-9291-y</idno>
                  <idno type="origin">10.1007%2Fs11151-011-9291-y</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>More recently, in the <ref type="bibr">Microsoft (2002)</ref> antitrust litigation, the U.S. Department of Justice targeted <rs corresp="#fcae93e89f-software-0" resp="#curator" type="publisher">Microsoft</rs>'s vertical integration of its <rs cert="1.0" resp="#annotator14" type="software" xml:id="fcae93e89f-software-0">Windows</rs> operating system software with <rs corresp="#fcae93e89f-software-1" resp="#curator" type="publisher">Microsoft</rs> <rs cert="1.0" resp="#annotator14" type="software" xml:id="fcae93e89f-software-1">Internet Explorer</rs>, a web browser, among other alleged abuses. This controversial and complex case was characterized by Microsoft as an unwise attack on its success in developing new technology. This view builds in part on Joseph <ref type="bibr">Schumpeter's (1950)</ref> idea that an important form of competition involves innovation-stimulating struggles for market dominance, success at which generates economic rewards that incentivize investment in research and development of successive generations of technology <ref type="bibr">(Evans and Schmalensee 2002; Schmalensee 2000)</ref>. From a Schumpeterian perspective, Microsoft's monopoly of operating system software, far from being regrettable, was a necessary and temporary stimulant of continued technological progress. <ref type="bibr">4</ref> Hahn and Passel (2010) put the point succinctly in a recent online posting: Regulators need to recognize that in markets driven by rapidly changing technology and huge economies of scale, it's natural for one firm or another to be king of the hill-albeit temporarily. So market share alone is no indicator of anticompetitive behavior or of the difficulty a newcomer with a better idea would have in competing for the business.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="ded11ad45c">
            <titleStmt>
               <title>Dynamic linkages between stock markets: the effects of crises and globalization</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1007/s10258-013-0091-1</idno>
                  <idno type="origin">10.1007%2Fs10258-013-0091-1</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>where ρ(j ) is Spearman's rho for a copula applicable in regime j. The predicted values of the lower and upper tail dependence coefficients are defined analogously. It follows from (8) that the ρ t is Spearman's rho for the mixture of copulas governingthe dependence in regimes 1 and 2, with the weights equal to the predicted probabilities P (S t = 1| t −1 ) and P (S t = 2| t −1 ). Formulas <ref type="formula">(12)</ref> and <ref type="formula">(13)</ref> imply that an analogous property holds for the lower and upper tail dependence coefficients. Our comparison of the mean values of the mentioned dependence measures taken in the subperiods of equal length uses the model confidence set (MCS) methodology. The ranking of the periods according to the mean value of Spearman's rho is obtained with the MCS procedure applied sequentially-after each run, the series constituting the MCS are excluded. The higher score means the stronger dependence. In the case where the MCS consists of at least two periods, the arithmetic mean of the corresponding scores is assigned to each of them. The same procedure is conducted for the series of the predicted values of tail dependence coefficients. All MCSs (with the significance level α = 0.1) have been estimated using the package <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="ded11ad45c-software-3">MulCom</rs> of <ref type="bibr">Hansen and Lunde (2010)</ref> written in Ox <ref type="bibr">(Doornik 2006)</ref>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="fed1954681">
            <titleStmt>
               <title>Educational Attainment and Wage Inequality in Turkey</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/labr.12083</idno>
                  <idno type="origin">10.1111%2Flabr.12083</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>In our empirical application we use the <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="fed1954681-software-simple-0">STATA</rs> ado file '<rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="fed1954681-software-1">RIFREG</rs>' written by <ref type="bibr">Firpo et al. (2009)</ref>, downloaded from: <rs corresp="#fed1954681-software-1" resp="#annotator14" type="url">http://faculty.arts.ubc.ca/nfortin/datahead.html</rs>.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="c1acaa810f">
            <titleStmt>
               <title>PROVIDING GLOBAL PUBLIC GOODS: ELECTORAL DELEGATION AND COOPERATION</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/ecin.12482</idno>
                  <idno type="origin">10.1111%2Fecin.12482</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>After the election, everyone within a group is informed about the IDs of the three delegates who make contribution choices on behalf of their group members for the next three periods. At the end of each single period in Part 4, everyone within a group receives information regarding the delegate ID, the contributions and earnings of each member in her own group, and the total contributions of each of the three groups. To facilitate comparisons, we present subjects in Baseline with exactly the same information without any reference to delegation or elections. Communication is not allowed in either treatment. <ref type="bibr">7</ref> The experiment was conducted in the MELESSA laboratory at the University of Munich from February to April 2012. A total number of 126 undergraduate and postgraduate (master) students with various academic backgrounds were recruited via <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="c1acaa810f-software-2">ORSEE</rs> <ref type="bibr">(Greiner 2015)</ref>. Subjects remained anonymous throughout the experiment and cash payments were made privately. The experiment was programmed and conducted with the software <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="c1acaa810f-software-3">z-Tree</rs> <ref type="bibr">(Fischbacher 2007</ref>). An experimental session lasted for a bit less than 2 hours. Subjects earned an average of €22.05 (including a €4 show-up fee).</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="a9655cffa3">
            <titleStmt>
               <title>REPUTATION TRANSMISSION WITHOUT BENEFIT TO THE REPORTER: A BEHAVIORAL UNDERPINNING OF MARKETS IN EXPERIMENTAL FOCUS</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/ecin.12477</idno>
                  <idno type="origin">10.1111%2Fecin.12477</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>A total of 172 students (152 in the four main reporting treatments and 20 in the strategy method treatment) participated in ten experiment sessions in 2013 and 2014 at the University of Michigan. 17 Among them, 58.7% of subjects (101) were female. No subject participated in more than one session, and the sessions lasted about an hour on average, about half of this time being spent on reading of instructions and answering comprehension questions. The experiment was programmed in <rs cert="0.8" resp="#annotator20" subtype="used" type="software" xml:id="a9655cffa3-software-1">z-tree</rs> <ref type="bibr">(Fischbacher 2007)</ref>. All instructions (for the example of the LC treatment, see Supporting Information) were neutrally framed, avoiding terms such as "cooperate," "trust," and so on. Subjects had to answer a number of control questions (see the instructions) to confirm their understanding of the experiment. Communication between subjects was not permitted. Average earnings were $20.84, including a $5 participation fee, with a standard deviation of $4.16. <ref type="figure">Figure 2</ref> summarizes key subject behaviors and beliefs, with panels (A) (at left), (B) (upper right), and (C) lower right displaying cooperation decisions, decisions to report defectors, and decisions to report cooperators, respectively. We begin with our main focus, costly reporting. A glance at panel (B) makes clear that there was much costly reporting, but overwhelmingly reporting of defectors by cooperators, as predicted. Focusing on the left panel of 17. Sessions were conducted by Kamei while he was Assistant Professor at Bowling Green State University. All subjects were recruited from the University of Michigan experimental lab's subject pool using solicitation messages via <rs resp="#curator" type="software" xml:id="a9655cffa3-software-simple-1">ORSEE (Online Recruitment System for Economic Experiments)</rs>. We aimed to recruit 20 participants to each session, but actual numbers varied somewhat due to differing show-up rates, averaging 17.2. <ref type="figure">Figure 2</ref>B, we see that some 45%-65% of cooperators who encountered a defector chose to report when costly, and that reporting occurred considerably more often (almost 90% report) at cost zero (NC treatment) than at positive costs (an overall average of 58.6% report in LC, MC, and HC). The remainder of this panel and the panel below it, in <ref type="figure">Figure 2</ref>C, show that there is also some, but considerably less, costly reporting of cooperators by cooperators and of defectors by defectors, and no costly reporting of cooperators by defectors. Overall, an average of 8.0% of subjects in the cooperator-cooperator, defectorcooperator, and defector-defector situations choose to report.</p>
         </body>
      </text>
   </TEI>
   <TEI subtype="econ" type="article">
      <teiHeader>
         <fileDesc xml:id="d9952c080a">
            <titleStmt>
               <title>PRICES, INFLATION, AND SMOKING ONSET: THE CASE OF ARGENTINA</title>
            </titleStmt>
            <sourceDesc>
               <bibl>
                  <idno type="DOI">10.1111/ecin.12490</idno>
                  <idno type="origin">10.1111%2Fecin.12490</idno>
               </bibl>
            </sourceDesc>
         </fileDesc>
         <encodingDesc/>
         <profileDesc>
            <textClass>
               <catRef target="#with_reconciliation_and_scripts"/>
               <catRef target="#unique_annotator"/>
            </textClass>
         </profileDesc>
      </teiHeader>
      <text xml:lang="en">
         <body>
            <p>Standard survival/duration models assume that the probability of eventual failure is greater than zero for all individuals <ref type="bibr">(Boag 1949; Forster and Jones 2001; Schmidt and Witte 1989)</ref>. Given that a large proportion of individuals never start smoking, we also used discrete time split population models. 4 All models were estimated using <rs cert="1.0" resp="#annotator14" subtype="used" type="software" xml:id="d9952c080a-software-0">Stata/MP</rs> <rs corresp="#d9952c080a-software-0" resp="#annotator14" type="version">14.1</rs> with sampling weights. Split population models were estimated using spsurv developed by Jenkins without sampling weights. <ref type="bibr">5</ref> Finally, we conducted a number of sensitivity checks to ensure that our main results were not sensitive to alternative specifications. 4. A number of studies have used a split population approach to examine the effect of prices on smoking onset. See for example, <ref type="bibr">Douglas and Hariharan (1994)</ref>, <ref type="bibr">Douglas (1998)</ref>, <ref type="bibr">Forster and Jones (2001)</ref>, <ref type="bibr">López Nicolás (2002)</ref>, <ref type="bibr">Kidd and Hopkins (2004)</ref>, <ref type="bibr">Madden (2007)</ref>, and <ref type="bibr">Guindon (2014a)</ref>.</p>
         </body>
      </text>
   </TEI>
</teiCorpus>
